We would like to thank all the reviewers for their feedback and in-depth comments.

Reviewer 1:

The heuristics proposed in the paper were not developed with the main purpose of beating the baseline, but to make the algorithm work (learn) in a stable way. The intrinsic difficulties of adapting SVRG to the RL framework have, as a consequence, a certain sensitivity to the meta-parameters. Our contribution should be intended as two-fold: a theoretical part supporting the usage of SVRG in RL, and a more empirical part highlighting the additional challenges of variance reduction in RL, where we also show how these difficulties can be overcome with proper heuristics. We think the community would benefit from a picture of both the opportunities and the risks of applying SVRG to policy gradient learning.

The proof to Lemma 3.1 was meant to be in the appendix and is missing by mistake. We will definitively include it. The variance property is true at convergence because the O(1/B) terms cancel out as both the parameter and the snapshot parameter approach the optimum. More formally: "As \theta -> \theta^*, also \tilde\theta -> \theta^*; hence, by continuity of J: Var(SVRG) -> Var(\hat\nablaJ) + 1/B*Var[g(\theta^*) - 1*g(\theta^*)] = Var(\hat\nabla)". The importance weight is one because the two parameters are the same. See also a similar proof in Section 2 of the original SVRG paper.

The performance of GPOMDP on Swimmer is lower than the one reported in Duan et al. because we do not employ the linear critic used in that paper. Since critic and SVRG are both ways to reduce the variance of the gradient estimate, and a combination of the two is non-trivial (how do you critique snapshot policies?) we turned the critic off in both GPOMDP and SVRPG to measure the net advantage of SVRG. Later, in the Half-Cheetah experiment, we include the critic in both algorithms. 

We reported Duan et al. values only for the Half-Cheetah experiment because it is the only case in which we used the critic. This represent a further step: evaluating the combination of SVRG and critic as variance reduction techniques. We recognize that the current presentation is confusing. In fact, we now have reproduced the GPOMDP/Half-Cheetah result and we will simply report our results in the final version. We will mention that experiments on Half-Cheetah are coherent with Duan et al., while the ones on Cart-Pole and Swimmer cannot be compared since we did not use the critic.

The version of Cart-Pole we use is the same of Duan et al, and this should be mentioned, although the task is thoroughly described in Appendix E. As for Swimmer, the lower scores are due to the fact that we do not include a critic in either GPOMDP or SVRPG.

As a first attempt to bring the SVRG technique to the policy optimization framework, we do not claim SVRPG has the maturity of algorithms like TRPO. Recent policy optimization algorithms employ a series of tools, e.g. natural gradients and exploration bonuses, that are outside the scope of this paper but represent promising future-work material.

We thank the reviewer for pointing out typos.


Reviewer 2:
We see that meta-parameters are a delicate aspect of our algorithm. The pragmatic role of Theorem 4 is to show the interplay of these parameters more than prescript specific values. In our work, we both provide general theoretical guarantees and propose a practical way to select meta-parameters. The statement of Theorem 4 (and some subtleties of its proof) heavily guided the choice of this heuristics, and we will make this more clear in the final version. More specifically:

1) Variance reduction algorithms for supervised learning often provide explicit guidelines on how to set the epoch size, e.g. O(N). In RL, this meta-parameter is more delicate since it is linked to the variance introduced by importance sampling. Theory would prescribe a large epoch size to avoid instabilities, but this would hinder the efficiency of the algorithm. The value optimizing this tradeoff is difficult to find and changes over time. It is also problem dependent since it also depends on the geometry of policy space.is reasons, we designed an adaptive algorithm, which is heuristic but based on insights from the theory. In the final version we explain this process better, and show examples of manual selection of m for sensitivity analysis. Future work will try to bridge the gap between theory and practice, e.g. by employing natural gradient or more sophisticated importance weighting techniques to reduce the sensitivity of SVRPG to this meta-parameter. 
2) In nonconvex optimization, it is often the case that the step sizes prescribed by theory are prohibitively conservative to be used in applications. For instance, in Reddi et al. the authors understand from their theoretical analysis that a constant step-size should be used, but they empirically search for the specific value that yields the best practical performance. Even in the original SVRG paper, common annealing schemas are employed in the experiments.
3) In Theorem 4.1, \theta^* denotes a global optimizer, although only convergence to a local optimum is guaranteed. We will make this more clear.
4) \theta_m^0, \tilde\theta^0 and \theta_0^1 are all the same. Superscripts denote epochs and subscripts inner iterations, as should be clear from the pseudocode. We will check for notation inconsistencies in the text.
5) Also under these assumptions, plain PG is an instance of Stochastic Gradient Ascent, which has a 1/\sqrt{T} rate.
6) We consider the horizon H to be part of the problem, but we should mention that problems with long horizons can be more difficult since we employ importance weighting. In the final version we will show an example where we vary the horizon of a task for sensitivity analysis.

On experiments:
1) The comparison is fair, figures have the number of trajectories on the horizontal axis, and the extra trajectories collected by  SVRPG at the snapshot are indeed counted. Our algorithm updates the policy less often, but manages to be faster anyway because the quality of the updates is higher.
2) Using fixed batch sizes is indeed suboptimal, but we leave this aspect for future work since adapting batch sizes in the policy gradient framework is a  difficult problem that have been only slightly explored in the RL literature so far (cfr. Papini et al., "Adaptive batch size for safe policy gradients." NIPS 2017).
3) This is because we used the rllab variant of Cart-Pole (Duan et al., 2016), where the agent collects a reward of ~10 at each time-step, as described in Appendix E.
4) For SVRPG, \alpha is selected with the Adam variant described in Section 5.2. For GPOMDP, we use Adam. All meta-parameters are reported in Appendix E and have been chosen to make the comparison fair. 


Reviewer 3:

We thank the reviewer for the positive feedback and the suggestions on related works, which will be useful for future developments of our method. We will mention the suggested papers/directions in the final version, especially [Bietti et al. 2016] seems particularly relevant.
