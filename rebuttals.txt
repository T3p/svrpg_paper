We would like to thank all the reviewers for their feedback and in-depth comments.

Reviewer 1:

The heuristics proposed in the paper were not developed with the main purpose of beating the baseline, but to make the algorithm work (learn) in a stable way. The intrinsic difficulties of adapting SVRG to the RL framework have as a consequence a certain sensitivity to meta-parameters. Our contribution should be intended as two-fold: a theoretical part supporting the usage of SVRG in RL, and a more empirical part highlighting the practical difficulties and proposing a way to tackle them. We think the community would benefit from a picture of both the opportunities and the risks of this technique. In the final version, we will make this more clear.

The proof to Lemma 3.1. was meant to be in the appendix and is missing by mistake. We will definitively include it. The variance property is true at convergence because the O(1/B) terms cancel out as both the parameter and the snapshot parameter approach the optimum. This is a nice property of SVRG mentioned also in Section 2 of the original SVRG paper.

The performance of GPOMDP on Swimmer is lower than the one reported in Duan et al. because we do not employ the linear critic used in that paper. Since critic and SVRG are both ways to reduce the variance of the gradient estimate, and a combination of the two is non-trivial (how do you critique snapshot policies?) we turned the actor off to measure the net advantage of SVRG. Later, in the Half-Cheetah experiment, we include the actor in both GPOMDP and SVRPG. 

We reported Duan et al. values only for the Half-Cheetah experiment because it is the only case in which we used the actor. We recognize that this is confusing. In fact, we now have reproduced the GPOMDP/Half-Cheetah result and we will simply report our results, mentioning that the ones on Half-Cheetah are coherent with what is reported in Duan et al., while the ones on Cart-Pole and Swimmer cannot be compared since we did not use the critic.

The version of Cart-Pole we use is the same of Duan et al, and this should be mentioned, although the task is already described in Appendix E. The lower scores are due to the fact that we do not include a critic in GPOMDP.

As a first attempt to bring the SVRG technique to the policy optimization framework, we do not claim SVRPG has the maturity of algorithms like TRPO. Recent policy optimization algorithms employ a series of tools, e.g. natural gradients and exploration bonuses, that are outside the scope of this paper but represent promising future-work material.

We thank the reviewer for pointing out typos.


Reviewer 2:

1), 2) The impact of Theorem 4, besides the theoretical guarantees, is to show the interplay of the meta-parameters more than prescript specific values. In nonconvex optimization, it is often the case that the meta-parameters prescribed by the theory are prohibitively conservative to be used in applications. For instance, in Reddi et al. the authors understand from their theoretical analysis that a constant step-size should be used, but empirically search for the specific value that yield the best practical performance. Even in the original SVRG paper, common learning rate scheduling policies are employed in the experiments. In RL, an example is the learning rate suggested in [Kakade, Sham, and John Langford. "Approximately optimal approximate reinforcement learning." ICML. Vol. 2. 2002.], which guarantees monotonic improvement but is prohibitively small for practical usage. However, the ideas embedded in that theoretical analysis ultimately lead to TRPO, which achieves great performance by relaxing several theoretical constraints. In our work, we both provide theoretical guarantees and propose a practical way to adapt meta-parameters. The statement of Theorem 4 (and some subtelties of its proof) heavily guided the choice of this heuristics, and we will make this more clear in the final version.
3) \theta^* is a global optimizer and we should state it explicitly.
4) \theta_m^0, \tilde\theta^0 and \theta_0^1 are all the same. Superscripts denote epochs and subscripts inner iterations, as should be clear from the pseudocode. We will check for notation inconsistencies in the text.
5) Plain PG is an instance of Stochastic Gradient Ascent, which indeed has a 1/\sqrt{T} rate.
6) We consider the horizon H to be part of the problem, but we should mention that problems with long horizons can be more difficult since we employ importance weighting.

On experiments:
1) The comparison is fair, figures have the number of trajectories on the horizontal axis, and the extra trajectories collected by  SVRPG at the snapshot are indeed counted. Our algorithm manages to be faster anyway because the quality of the updates is higher.
2) Using fixed batch sizes is indeed suboptimal, but we leave this aspect for future work since adapting batch sizes in the policy gradient framework is a  difficult problem that have been only slightly explored in the RL literature so far (cfr. Papini et al., "Adaptive batch size for safe policy gradients." NIPS 2017).
3) This is because we used the rllab variant of Cart-Pole (Duan et al., 2016), where the agent collects a reward of ~10 at each time-step, as described in Appendix E.
4) For SVRPG, \alpha is selected with the Adam variant described in Section 5.2. For GPOMDP, we use Adam. All meta-parameters are reported in Appendix E and have be chosen to make the comparison fair. 


Reviewer 3:

We thank the reviewer for the positive feedback and the suggestions on related works, which will be useful for future developments of our method. [Bietti et al. 2016] seems particularly relevant and we should mention it in the final version. 
