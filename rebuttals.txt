We would like to thank the reviewers for their feedback and in-depth comments.

AR1
The heuristics proposed in the paper were not developed with the main purpose of beating the baseline, but to make the algorithm work in a stable way. Our contribution should be intended as two-fold: a theoretical part supporting the use of SVRG in RL, and an empirical part highlighting the additional challenges of variance reduction in RL (where we also show how these difficulties can be overcome with proper heuristics). We think the community would benefit from a picture of both the opportunities and the risks of applying SVRG to PG.

The proof to Lem 3.1 was meant to be in the appendix and is missing by mistake. The variance property is true at convergence because the O(1/B) terms cancel out as both the parameter and the snapshot parameter approach the optimum. More formally: "As \theta -> \theta^*, also \tilde\theta -> \theta^*; hence, by continuity of J: Var(SVRG) -> Var(\hat\nablaJ) + 1/B*Var[g(\theta^*) - 1*g(\theta^*)] = Var(\hat\nabla)". The importance weight is one because the two parameters are the same (see also Sec 2 of the original SVRG paper).

The performance of GPOMDP on Swimmer is lower than the one reported in Duan et al. because we do not employ the linear critic used in that paper. Since critic and SVRG are both ways to reduce the variance of the gradient estimate, and a combination of the two is non-trivial we turned the critic off in both GPOMDP and SVRPG to measure the net advantage of SVRG.
We reported Duan et al. values only for the Half-Cheetah experiment because it is the only case in which we used the critic. This represents a further step: evaluating the combination of SVRG and critic as variance reduction techniques. We see that the current presentation is confusing. In fact, we now have reproduced the GPOMDP/Cheetah result and we will simply report our results in the final version. We will mention that experiments on Half-Cheetah are coherent with Duan et al., while the ones on Cart-Pole and Swimmer cannot be compared since we did not use the critic.
Cart-Pole we used is the same of Duan et al, and this should be mentioned, although the task is thoroughly described in App E. As for Swimmer, the lower scores are due to the fact that we do not include a critic in either GPOMDP or SVRPG.

As a first attempt to bring the SVRG technique to the policy optimization framework, we do not claim SVRPG has the maturity of algorithms like TRPO. Recent policy optimization algorithms employ a series of tools, e.g. natural gradient and exploration bonus, that are outside the scope of this paper but represent promising future-work material.

We thank the reviewer for pointing out typos.

AR2
We see that meta-parameters are a delicate aspect of our algorithm. The pragmatic role of Thm 4 is to show the interplay of these parameters more than prescript specific values. We both provide general theoretical guarantees and propose a practical way to select meta-parameters. The statement of Thm 4 (and some subtleties of its proof) heavily guided the choice of this heuristics, and we will make this more clear in the final version.

1) Variance reduction algorithms for SL often provide explicit guidelines on how to set the epoch size, e.g. O(N). In RL, this meta-parameter is more delicate since it is linked to the variance introduced by importance sampling. The theory would prescribe a large epoch size to avoid instabilities, but this would hinder the efficiency of the algorithm. The optimal value is usually time and problem dependent (eg the geometry of the policy space). For these reasons, we designed an adaptive algorithm, which is heuristic but based on insights from the theory. In the final version, we will comment on this and we will do a sensitivity analysis for m. Future work will try to bridge the gap between theory and practice.
2) In nonconvex opt., it is often the case that the step sizes prescribed by the theory are prohibitively conservative to be used in applications. For example, Reddi et al. performed empirical search for the step size in the experiments rather then use the theoretical one. Even in the original SVRG paper, common annealing schemas are employed.
3) In Thm 4.1, \theta^* denotes a global optimizer, although only convergence to a local optimum is guaranteed. We will make this more clear.
4) \theta_m^0, \tilde\theta^0 and \theta_0^1 are all the same. Superscripts denote epochs and subscripts inner iterations (see pseudocode).
5) Also under these assumptions, plain PG is an instance of Stochastic Gradient, which has a 1/\sqrt{T} rate.
6) We consider the horizon H to be part of the problem. Long horizons can be more difficult since we employ importance weighting. We will mention this in the final version and we will perform a sensitivity analysis.

On experiments:
1) The comparison is fair, figures have the number of trajectories on the horizontal axis, and the extra trajectories collected by SVRPG at the snapshot are indeed counted. Our algorithm updates the policy less often but manages to be faster anyway because the quality of the updates is higher.
2) Using fixed batch sizes is indeed suboptimal, but we leave this aspect for future work. This topic has been faced, for example, in (Papini et al. 2017) for PG.
3) This is because we used the rllab variant of Cart-Pole (Duan et al., 2016), where the agent collects a reward of ~10 at each time-step, as described in App E.
4) For SVRPG, \alpha is selected with the Adam variant described in Sec 5.2. For GPOMDP, we use Adam. All meta-parameters are reported in App E and have been chosen to make the comparison fair.


AR3
We thank the reviewer for the positive feedback and the suggestions on related works, which will be useful for future developments of our method. We will mention the suggested papers/directions in the final version, especially [Bietti et al. 2016] seems particularly relevant.
