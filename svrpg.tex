\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

%%%USEFUL PACKAGES%%%
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{amsmath,amssymb}
\usepackage{mathtools}
\usepackage{pifont}
\usepackage[makeroom]{cancel}

%%Theorems
\usepackage{amsthm}
\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}
\newtheorem{case}{Case}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{conj}{Conjecture}
\newtheorem{example}{Example}
%Restatable
\usepackage{thmtools, thm-restate}
\declaretheorem[numberwithin=section]{thm}
\declaretheorem[sibling=thm]{lemma}
\declaretheorem[sibling=thm]{corollary}
\declaretheorem[sibling=thm]{assumption}
\declaretheorem[sibling=thm]{theorem}
%e.g. ...
\usepackage{xspace}
\DeclareRobustCommand{\eg}{e.g.,\@\xspace}
\DeclareRobustCommand{\ie}{i.e.,\@\xspace}
\DeclareRobustCommand{\wrt}{w.r.t.\@\xspace}
% graphics
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tikz,pgfplots}
\pgfplotsset{compat=newest}
%% TODOs
% \setlength{\marginparwidth}{2cm}
\usepackage[colorinlistoftodos, textwidth=20mm]{todonotes}
\definecolor{citrine}{rgb}{0.89, 0.82, 0.04}
\definecolor{blued}{RGB}{70,197,221}
%%todo by Marcello
\newcommand{\todomarc}[1]{\todo[color=green, inline]{\small #1}}
\newcommand{\todomarcout}[1]{\todo[color=green]{\scriptsize #1}}
%%todo by Matteo Papini
\newcommand{\todomat}[1]{\todo[color=citrine, inline]{\small #1}}
\newcommand{\todomatout}[1]{\todo[color=citrine]{\scriptsize #1}}
%%todo by Matteo Pirotta
\newcommand{\todopir}[1]{\todo[color=blued, inline]{\small #1}}
\newcommand{\todopirout}[1]{\todo[color=blued]{\scriptsize #1}}
%%todo by Damiano
\newcommand{\tododam}[1]{\todo[color=orange, inline]{\small #1}}
\newcommand{\tododamout}[1]{\todo[color=orange]{\scriptsize #1}}
%%todo by Giuseppe
\newcommand{\todobep}[1]{\todo[color=purple, inline]{\small #1}}
\newcommand{\todobepout}[1]{\todo[color=purple]{\scriptsize #1}}
%%%%%%

%%%CUSTOM COMMANDS%%%
%Math
\newcommand{\realspace}{\mathbb R}      % realspace
\newcommand{\transpose}[1]{{#1}^\texttt{T}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\EV}{\mathbb{E}}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator*{\Cov}{\mathbb{C}ov}
\DeclareMathOperator*{\Var}{\mathbb{V}ar}
\newcommand{\EVV}[2][\ppvect \in \ppspace]{\EV_{#1}\left[{#2}\right]}
\newcommand{\norm}[2][\infty]{\left\|#2\right\|_{#1}}
\newcommand{\Dij}[2]{\frac{\partial^{2}{#1}}{\partial{#2}_i\partial{#2}_j}}
\newcommand{\de}{\,\mathrm{d}}
\newcommand{\dotprod}[2]{\left\langle#1,#2\right\rangle}
\newcommand{\dnabla}{\nabla\!\!\!\!\nabla}
%RL
\newcommand{\vtheta}{\boldsymbol{\theta}}
\newcommand{\Aspace}{\mathcal{A}}
\newcommand{\Sspace}{\mathcal{S}}
\newcommand{\Tspace}{\mathcal{T}}
\newcommand{\Transition}{\mathcal{P}}
\newcommand{\Reward}{\mathcal{R}}
\newcommand{\stationary}{d_{\rho}^{\pi_{\vtheta}}(s)}
\newcommand{\policy}{\pi_{\vtheta}(a \vert s)}
\newcommand{\pol}{\pi_{\vtheta}}
\newcommand{\trajdistr}{\pi_{\vtheta}(\tau)}
\newcommand{\score}[2]{\nabla\log\pi_{#1}(#2)}
\newcommand{\Qfun}{Q^{\pi_{\vtheta}}(s,a)}
\newcommand{\Vfun}{V^{\pi_{\vtheta}}(s)}
\newcommand{\vTheta}{\boldsymbol{\Theta}}
\newcommand{\vphi}{\boldsymbol{\phi}}
\newcommand{\gradJ}[1]{\nabla J(#1)}
\newcommand{\gradApp}[2]{\widehat{\nabla}_{#2}J(#1)}
\newcommand{\eqdef}{\mathrel{\mathop:}=}
\newcommand{\Dataset}{\mathcal{D}}
%Specific
\newcommand{\Ets}[2][t]{\mathbb{E}_{#1\vert s}\left[#2\right]}
\newcommand{\Covts}[3][t]{{\mathbb{C}\text{ov}}_{#1\vert s}\left(#2,#3\right)}
\newcommand{\Varts}[2][t]{{\mathbb{V}\text{ar}}_{#1\vert s}\left[#2\right]}
\newcommand{\gradBlack}[1]{\blacktriangledown J(#1)}
\newcommand{\gradIdeal}[1]{\dnabla J(#1)}
\newcommand{\VARRF}{V}
\newcommand{\GRADLOG}{G}
\newcommand{\VARIS}{W}
\newcommand{\HESSLOG}{F}
% short forms 
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\wo}[1]{\overline{#1}}
\newcommand{\wb}[1]{\overline{#1}}
%%%%%%

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Stochastic Variance-Reduced Policy Gradient}

\begin{document}

\twocolumn[
\icmltitle{Stochastic Variance-Reduced Policy Gradient}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2018
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Damiano Binaghi}{equal,polimi}
\icmlauthor{Giuseppe Canonaco}{equal,polimi}
\icmlauthor{Matteo Papini}{polimi}
\icmlauthor{Matteo Pirotta}{inria}
\icmlauthor{Marcello Restelli}{polimi}
\end{icmlauthorlist}

\icmlaffiliation{polimi}{Politecnico di Milano, Milano, Italy}
\icmlaffiliation{inria}{Inria, Lille, France}

\icmlcorrespondingauthor{Matteo Papini}{matteo.papini@polimi.it}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}



\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
        We present and analyse stochastic variance-reduced gradient (SVRG) in the context of Markov Decision Processes (MDPs).
        We first show how to account for I) non-concativity of the objective function; II) approximations in the full gradient computation; and III) non-stationarity of the sampling process.
        These characteristics make the application of SVRG to MDPs challenging.
        We propose SVRPG, a stochastic variance-reduced policy gradient algorithm that leverages on importance weight to preserve the unbiasedness of the gradient estimate.
        We further analyse its convergence guarantees under common assumptions on the MDP.
        Finally, we investigate practical variants of SVRPG and we empirically evaluate them on continuous MDPs.
\end{abstract}

\section{Introduction}
%Similarities between RL and SL
On a very general level, machine learning addresses the problem of an agent that must select the right actions to solve a task. The approach of Reinforcement Learning~\citep{sutton1998reinforcement} is to learn the best actions by direct interaction with the environment and evaluation of the performance in the form of a reward signal. This makes RL fundamentally different from Supervised Learning (SL), where correct actions are explicitly prescribed by a human teacher (\eg for classification, in the form of class labels). However, the two approaches share many challenges and tools. The problem of estimating a model from samples, which is at the core of SL, is equally fundamental in RL, whether we choose to model the environment, a value function, or a policy of behaviour directly. Moreover, when the tasks are characterized by large or continuous state-action spaces, RL needs to borrow the powerful function approximators that are the main subject of study of SL, such as neural networks.
% SL needs SVRG so RL
In a typical SL setting, a performance function $J(\vtheta)$ has to be optimized \wrt to model parameters $\vtheta$. The set of data that are available for training is often a subset of all the cases of interest, which may even be infinite, leading to optimization of finite sums to approximate expected performance over an unknown data distribution. When generalization to the complete dataset is not taken into consideration we talk about Empirical Risk Minization (ERM). Even in this case, stochastic optimization is often used for reasons of efficiency. The idea of stochastic gradient (SG) ascent \cite{nesterov2013introductory} is to iteratively focus on a random subset of the available data to obtain an approximate improvement direction. At the level of the single iteration, this can be much less expensive than taking into account all the data. However, the sub-sampling of data is a source of variance that can potentially compromise convergence, so that per-iteration efficiency and convergence rate must be traded off with proper handling of meta-parameters.
Variance-reduced gradient algorithms such as SAG \cite{roux2012stochastic}, SVRG \cite{johnson2013accelerating} and SAGA \cite{defazio2014saga} offer better ways of addressing this trade-off, with significant results both in theory and practice. Although designed explicitly for ERM, these algorithms address a problem that affects more general machine learning problems. 

In RL, stochastic optimization is rarely a matter of choice, since data must be actively sampled by interacting with an initially unknown environment. In this scenario, containing the variance of the estimates is a problem that cannot be avoided, which makes variance-reduced algorithms very interesting.
% Potential advantages of SVRG
Among RL approaches, policy gradient \cite{sutton2000policy} is the one that bears the closest similarity to SL solutions. The fundamental principle of this methods is to optimize a parametric policy by means of stochastic gradient ascent. Compared to other applications of SG, the cost of collecting samples can be very high since it requires to interact with the environment. This makes SVRG-like methods potentially much more efficient than, \eg batch learning.
% Difficulties in RL \wrt SL
Unfortunately, RL has a series of difficulties that are not present in ERM. First, in SL the objective can often be designed to be strongly concave (we aim to maximize). This is not possible in RL, so we have to deal with non-concavities. Then, as mentioned, the dataset is not initially available and may even be infinite, which  makes approximations unavoidable. This also rules out SAG and SAGA because of their storage requirements, which leaves SVRG as the most promising choice. Finally, the distribution used to sample data is not under direct control of the algorithm designer, but is a function of policy parameters that changes over time as the policy is optimized, which is a form of non-stationarity.
% Related work?
SVRG has been used in RL as a efficient technique for optimizing the per-iteration problem in Trust-Region Policy Optimization~\citep{xu2017svrgtrpo} or for policy evaluation~\citep{du2017svrgpe}.
In both the cases, the optimization problems faced resemble the SL scenario and are not characterized by all the previously mentioned issues.

%Paper structure
After providing a background on policy gradient and SVRG in Section \ref{sec:pre}, in Section \ref{sec:alg} we address the aforementioned difficulties to design SVRPG, a variant of SVRG for the policy gradient framework. In Section \ref{sec:conv} we provide convergence guarantees for our algorithm which also offers insights on the challenges of variance reduction in RL. In Section \ref{sec:stopping} we suggests how to set the meta-parameters of SVRPG, while in Section \ref{sec:prac} we discuss some practical variants of the algorithm. Finally, in Section \ref{sec:exp} we empirically evaluate the performance of our method on popular continuous RL tasks.

\section{Preliminaries}\label{sec:pre}
We start providing the essential background on policy gradient methods and stochastic variance-reduced gradient methods for finite-sum optimization.
%In the next section we will show how these methods can be combined to solve Reinforcement Learning tasks.

\subsection{Policy Gradient}\label{subsec:PolicyGradient}
A Reinforcement Learning task~\citep{sutton1998reinforcement} can be modelled with a discrete-time continuous Markov Decision Process (MDP) $M = \{\Sspace,\Aspace,\Transition,\Reward,\gamma,\rho\}$, where $\Sspace$ is a continuous state space; $\Aspace$ is a continuous action space; $\Transition$ is a Markovian transition model, where $\Transition(s'|s,a)$ defines the transition density from state $s$ to $s'$ under action $a$; $\Reward$ is the reward function, where $\Reward(s,a) \in [-R,R]$ is the expected reward for state-action pair $(s,a)$;
% and $R$ is the maximum absolute-value reward;
$\gamma\in[0,1)$ is the discount factor; and $\rho$ is the initial state distribution.
The agent's behaviour is modelled as a policy $\pi$, where $\pi(\cdot|s)$ is the density distribution over $\Aspace$ in state $s$.
% We consider episodic tasks, \ie tasks composed of episodes of length $H$, also called time horizon.
% In this setting, a trajectory $\tau$ is the sequence of states and actions $(s_0,a_0,s_1,a_1,\dots,s_{H-1},a_{H-1})$ observed by following some stationary policy in a given episode, where $s_0$ is always sampled from the initial state distribution $\rho$.
We consider episodic MDPs with effective horizon $H$.\footnote{The episode duration is a random variable but the optimal policy is able to reach the target state (\ie absorbing state) in at most $H$ steps. This has not to be confused with a finite horizon problem where the optimal policy is non-stationary.} In this setting, we can limit our attention to trajectories of length $H$. A trajectory $\tau$ is a sequence of states and actions $(s_0,a_0,s_1,a_1,\dots,s_{H-1},a_{H-1})$ observed by following a stationary policy, where $s_0 \sim \rho$.
With abuse of notation, we denote with $\pi(\tau)$ the density distribution induced by policy $\pi$ on the set $\Tspace$ of all possible trajectories, and with $\Reward(\tau)$ the total discounted reward provided by trajectory $\tau$:
%
$\Reward(\tau) = \sum_{t=0}^{H-1}\gamma^t\Reward(s_t,a_t).$
%
Policies can be ranked based on their expected total reward: $J(\pi) = \EVV[\tau \sim \pi]{\Reward(\tau)|M}$.
Solving an MDP $M$ means finding $\pi^* \in \argmax_{\pi} \{J(\pi)\}$.

Policy gradient methods restrict the search for the performance-maximizing policy over a class of parametrized policies $\Pi_{\vtheta}=\{\pol: \vtheta \in \realspace^d\}$, with the only constraint that $\pol$ is differentiable \wrt $\vtheta$. We denote the performance of a parametric policy with $J(\vtheta)$ for brevity, while in some occasions we replace $\pol(\tau)$ with $\pi(\tau|\vtheta)$ for the sake of readability.
The search for a locally optimal policy is performed by means of gradient ascent, where the policy gradient
%of $J(\vtheta)$ \wrt the policy parameters 
is \cite{sutton2000policy, Peters2008reinf}:
\begin{align} \label{E:policygradient}
        \gradJ{\vtheta} = \EVV[\tau \sim \pol]{\score{\vtheta}{\tau}\Reward(\tau)}
        %\int_{\Tspace}\pol(\tau)\score{\vtheta}{\tau}\Reward(\tau)\de \tau.
\end{align}
Notice that the distribution defining the gradient is induced by the current policy. This aspect introduces a nonstationarity in the sampling process. Since the underlying distribution changes over time, it is necessary to resample at each update or use weighting techniques such as importance sampling.
Here, we consider the \emph{online learning scenario}, where trajectories are sampled by interacting with the environment at each policy change. 
In this setting, stochastic gradient ascent is typically employed.
At each iteration $k >0$, a batch $\mathcal{D}_N^k = \{\tau_i\}_{i=0}^N$ of $N>0$ trajectories is collected using policy $\pi_{\vtheta_k}$.
The policy is then updated as $\vtheta_{k+1}  = \vtheta_k + \alpha\gradApp{\vtheta_k}{N}$, where $\alpha$ is a step size and $\gradApp{\vtheta}{N}$ is an estimate of~\eqref{E:policygradient} using $\mathcal{D}_N^k$, \ie
\begin{align} \label{E:policygradient.estimate}
        \gradApp{\vtheta}{N} = \frac{1}{N}\sum_{n=1}^{N} g(\tau_i|\vtheta), \quad \tau_i \in \mathcal{D}_N^k.
\end{align}
The most widespread gradient estimators are REINFORCE~\citep{williams1992simple} and G(PO)MDP~\citep{baxter2001infinite}.
Although REINFORCE definition is simpler, G(PO)MDP is often preferred due to its lower variance.
We refer the reader to App.~\ref{A:gradient_estimators} for details.

The main limitation of plain policy gradient has been the high variance of these estimators.
The naive approach of increasing the batch size is not an option in RL due to the high cost of collecting samples, \ie by interacting with the environment.
For this reason, literature has focused on the introduction of baselines aiming to reduce the variance~\citep[\eg][]{Peters2008reinf,Thomas2017actionbaseline,wu2018variance}.\footnote{A baseline is a function $f(s,a)$ such that $\EVV[a \sim \pol]{f(s,a)} = 0$, for any $s\in\Sspace$.}
There has been a surge of recent interest in variance reduction techniques for gradient optimization in supervised learning.
Although these techniques has been mainly derived for finite-sum problems, we will show in Sec.~\ref{sec:alg} how they can be used in RL.
The next section has the aim to describe variance reduction techniques for finite-sum problems. In particular, we will present the SVRG algorithm that is at the core of this work.


\subsection{Stochastic Variance-Reduced Gradient}
Finite-sum optimization is the problem of maximizing an objective function $f(\vtheta)$ which can be decomposed into the sum or average of a finite number of functions $z_i(\cdot|\vtheta)$:
\begin{align*}
        \max_{\vtheta} \left\{ f(\vtheta) = \frac{1}{N}\sum_{i=1}^{N}z_i(\vtheta)\right\}.
\end{align*}
This kind of optimization is very common in machine learning, where each $z_i$ may correspond to a data sample $x_i$ from a dataset $\mathcal{D}_N$ of size $N$ (\ie $z_i(\vtheta) = z(x_i|\vtheta)$). 
%% In this case, we adopt the following, more meaningful notation:
%% \begin{align*}
%% \max_{\vtheta} f(\vtheta) = \frac{1}{N}\sum_{i=1}^{N}g(x_i \vert \vtheta).
%% \end{align*}
A common requirement is that $z$ must be smooth and {\color{red} concave} in $\vtheta$.\footnote{Note that we are considering a maximization problem instead of the classical minimization one.} 
Under this hypothesis, full gradient (FG) ascent~\citep{cauchy1847methode} with a constant step size achieves an Incremental First-order Oracle (IFO) complexity~\citep{agarwal2014lower} of $\mathcal{O}(\nicefrac{N}{\epsilon})$, \ie it requires $k = \mathcal{O}(\nicefrac{N}{\epsilon})$ gradient computations to achieve $\norm[2]{\nabla_{\vtheta}f(\vtheta_k)}^2\leq\epsilon$. % \footnote{In the following we will omit the subscript for the $\ell_2$-norm.} 
This can be too expensive for large values of $N$. Stochastic Gradient (SG) ascent~\citep[\eg][]{robbins1951stochastic,bottou2004large} overcomes this problem by sampling a single sample $x_i$ per iteration, but a vanishing step size is required to control the variance introduced by sampling. The resulting IFO complexity is $\mathcal{O}(\nicefrac{1}{\epsilon^2})$ in expectation, which does not depend on $N$, but has a worse dependency on $\epsilon$~\cite{nemirovskii1983problem,reddi2016stochastic}.
Starting from SAG, a series of variations to SG have been proposed to achieve a better trade-off between convergence speed and cost per iteration: \eg SAG~\citep{roux2012stochastic}, SVRG~\cite{johnson2013accelerating}, SAGA~\cite{defazio2014saga} and Finito~\cite{defazio2014finito}, 
The common idea is to reuse past gradient computations to reduce the variance of the current estimate.
In particular, Stochastic Variance-Reduced Gradient (SVRG) is often preferred to other similar methods for its limited storage requirements, which can become a problem when deep and/or wide neural networks are employed.  

The idea of SVRG is to alternate full and stochastic gradient updates. 
Each $m = \mathcal{O}(N)$ iterations, a snapshot $\widetilde{\vtheta}$ of the current parameter is saved together with its full gradient $\nabla f(\widetilde{\vtheta}) = \frac{1}{N} \sum_i z(x_i|\widetilde{\vtheta})$.
Between snapshots, the parameter is updated with a gradient estimate corrected using stochastic gradient, for any $t \in \{0,\ldots,m-1\}$
\begin{equation}\label{E:svrg.gradient.correction}
        \blacktriangledown f(\vtheta_{t}) = \nabla f(\tilde{\vtheta}) + \nabla z(x_i | \vtheta_t) - \nabla z(x_i | \tilde{\vtheta}),
\end{equation} 
where $x_i$ is sampled uniformly at random from $\mathcal{D}_N$ (\ie $x_i \sim \mathcal{U}(\mathcal{D}_N)$).
Note that $t=0$ corresponds to a FG step (\ie $\blacktriangledown f(\vtheta_0) = \nabla f(\wt{\vtheta})$) since $\vtheta_0 := \wt{\vtheta}$.
The corrected gradient $\blacktriangledown f(\vtheta)$ is an unbiased estimate of $\nabla f(\vtheta)$, and it is able to control the variance introduced by sampling even with a fixed step size, without resorting to plain full gradient. The resulting IFO complexity is $\mathcal{O}(N+\nicefrac{\sqrt{N}}{\epsilon})$ \cite{reddi2016stochastic}.

More recently, some extensions of variance reduction algorithms to the non-convex objectives have been proposed~\citep[\eg][]{reddi2016stochastic,allen2016variance,reddi2016stochastic,reddi2016fast}. In this scenario, $f$ is typically required to be L-smooth, \ie $\norm[2]{\nabla f(\vtheta') - \nabla f(\vtheta)} \leq L\norm[2]{\vtheta'-\vtheta}$ for each $\vtheta,\vtheta'\in\realspace^n$ and for some Lipschitz constant $L$. Under this hypothesis, the IFO complexity of full gradient  and of stochastic gradient ascent are the same as in the \textcolor{red}{concave} case~\citep{nesterov2013introductory,ghadimi2013stochastic}. Also in this case, SVRG yields an advantageous trade-off, with an IFO complexity of $\mathcal{O}(N + \nicefrac{N^{\nicefrac{2}{3}}}{N})$~\citep{reddi2016stochastic}. The only additional requirement is to select $\vtheta^*$ uniformly at random among all the $\vtheta_k$ instead of simply setting it to the final value. Pseudocode of SVRG for both the convex and the non-convex case is provided in Alg.~\ref{alg:svrg}.

\begin{algorithm}[tb]
	\caption{SVRG}
	\label{alg:svrg}
	\begin{algorithmic}
            \STATE {\bfseries Input:} a dataset $\mathcal{D}_N$, number of epochs $S$, epoch size $m$, step size $\alpha$, initial parameter $\vtheta_{0}^0:=\wt{\vtheta}^0$
		\FOR{$s=0$ {\bfseries to} $S-1$}
        \STATE $\wt{\mu} = f(\wt{\vtheta}^s)$
		\FOR{$t=0$ {\bfseries to} $m-1$}
%		\STATE Pick $i$ uniformly at random from $[1,N]$
        \STATE $x_i \sim \mathcal{U}\left(\mathcal{D}_N\right)$
		\STATE $v^{s}_t = 
%			\nabla f(\wt{\vtheta}^s) + 
			\wt{\mu} + 
			\nabla g(x_i|\vtheta_t^{s}) -
			\nabla g(x_i|\wt{\vtheta}^{s})
		$
        \STATE $\vtheta_{t+1}^{s} = \vtheta_t^{s} + \alpha v^{s}_t$
		\ENDFOR
        \STATE $\vtheta_0^{s+1} := \wt{\vtheta}^{s+1} = \vtheta_{m}^s$
		\ENDFOR
        \STATE \underline{Concave case:} \textbf{return} $\vtheta_{0}^S$
        \STATE \underline{Non-Concave case:} \textbf{return} $\vtheta_t^s$ with $(s,t)$ picked uniformly at random from $\{[0,S[\times[0,m[\} \cup \{(S,0)\}$
	\end{algorithmic}
\end{algorithm}


\section{SVRG in Reinforcement Learning}\label{sec:alg}
In online RL problems the usual approach is to tune the batch size of SG in order to find the optimal trade-off between variance and speed.
Recall that compared to SL, the samples are not fixed in advance but need to be collected at each policy change.
Since this operation may be costly, we would like to minimize the number of interactions with the environment.
Due to this issues, we would like to apply SVRG to RL problems in order to limit the variance introduced by sampling trajectories, which would ultimately lead to faster convergence.
However, a direct application of SVRG to RL is not possible due to the following properties:
% In on-line policy gradient problems, SG is not really a choice, but is dictated by the necessity of interacting with an unknown environment.\todopir{I do not understand the meaning of this sentence. I would say that SG is not a choice by referring to papers that uses batches}
% We would like to apply SVRG to the policy gradient framework in order to limit the variance introduced by sampling trajectories, which would ultimately lead to faster convergence. In this scenario, the function $f(\vtheta)$ to optimize is $J(\vtheta)$ and the (implicit) dataset $\mathcal{D}$ is the set of all possible trajectories $\mathcal{T}$. Compared to the typical finite sum optimization scenario, we have the following additional challenges:
\begin{description}
        \item[Non-concavity:] the objective function $J(\vtheta)$ is typically non-concave.
        \item[Infinite dataset:] the RL optimization cannot be expressed as a finite-sum problem. The actual dataset is infinite and coincide with $\pi_{\vtheta}(\tau)$.
%    \item[Approximation:] the dataset $\mathcal{T}$ can be infinite, but we can only sample a finite number of trajectories;
        \item[Non-stationarity:] the distribution of the samples changes over time. In particular, the value of the parameter $\vtheta$ influences the sampling process.
\end{description}
To deal with non-concavity, we require $J(\vtheta)$ to be $L$-smooth, which is a reasonable assumption for common policy classes such as truncated Gaussian and softmax~\citep[\eg][]{Furmston2012unifying,pirotta2015lipschitz}, see App.~\ref{app:gauss} for the derivation.
Because of the infinite dataset, we can only rely on an estimate of the full gradient.
\citet{harikandeh2015stopwasting} analysed this scenario under the assumptions of $z$ being \textcolor{red}{concave}, showing that SVRG is robust to inexact computation of the full gradient. In particular, it is still possible to recover the original convergence rate if the error decreases at an appropriate rate. {\color{red}Later on we will show how the estimation accuracy impacts on the convergence results with non-concave objective.} 
Finally, the unbiasedness of Eq.~\ref{E:svrg.gradient.correction} is broken by the non-stationarity of the optimization problem.
To overcome this limitation we employ importance weighting~\citep[\eg][]{rubinstein1981simulation,precup2000eligibility} to correct the distribution shift.
%Finally, we employ importance weighting~\citep[\eg]{rubinstein1981simulation,precup2000eligibility} to guarantee proper sampling of trajectories. 

We can now introduce Stochastic Variance-Reduced Policy Gradient (SVRPG) for a generic policy gradient estimator $g$ (see Eq.~\ref{E:policygradient.estimate}).
% For simplicity, we focus on the REINFORCE estimator but the extension to G(PO)MDP is straightforward.
Between snapshots, the SVRPG estimate is corrected using importance weighting, for any $t \in \{0,\ldots,m-1\}$
\begin{align*}
        \blacktriangledown J(\vtheta_{t}) &= \wh{\nabla}_N J(\wt{\vtheta}) + g(\tau_i|\vtheta_t) - \omega(\tau_i|\vtheta_t, \wt{\vtheta}) g(\tau_i|\wt{\vtheta})
\end{align*}
% \begin{align*}
% 	\blacktriangledown J(\vtheta) &= \frac{1}{N}\sum_{j=0}^{N-1}\nabla\log\pi(\tau_j \vert \tilde{\vtheta})\Reward(\tau_j) \\
% 		&+ \nabla\log\pi(\tau_i \vert \vtheta)\Reward(\tau_i) 
% 		- \omega(\tau_i)\nabla\log\pi(\tau_i \vert \tilde{\vtheta})\Reward(\tau_i),
% \end{align*}
where $\wh{\nabla}_N J(\wt{\vtheta})$ is as in~\eqref{E:policygradient.estimate} where $\mathcal{D}_N$ is sampled using the snapshot policy $\pi_{\wt{\vtheta}}$, $\tau_i$ is sampled from the current policy $\pi_{\vtheta_t}$, and $\omega(\tau|\vtheta_t, \wt{\vtheta}) = \frac{\pi(\tau|\wt{\vtheta})}{\pi(\tau|\vtheta_t)}$ is an importance weight from $\pi_{\vtheta_t}$ to the snapshot policy $\pi_{\wt{\vtheta}}$. 
Similarly to SVRG, we have that $\vtheta_0 := \wt{\vtheta}$ and the update is a FG step.
Our update is still fundamentally on-policy, since the weighting concerns only the correction term. However, this partial ``off-policyness'' is enough to introduce variance. This is a well-known issue of importance sampling~\citep[\eg][]{thomas2015high}. To mitigate it, we use mini-batches of trajectories of size $B \ll N$ to average the correction, \ie
\begin{align}\label{E:svrpg.estimate.batch}
        \blacktriangledown J(\vtheta_{t}) &:= v_t= \wh{\nabla}_N J(\wt{\vtheta})\\ \notag
                                            & \quad{} + \frac{1}{B} \sum_{i=0}^{B-1} \left[
        g(\tau_i|\vtheta_t) - \omega(\tau_i|\vtheta_t, \wt{\vtheta}) g(\tau_i|\wt{\vtheta})
        \right].
\end{align}
We want just to stress that $\EVV[\tau_i \sim \pi_{\vtheta_t}]{\frac{1}{B} \sum_{i=0}^{B-1} \omega(\tau_i|\vtheta_t, \wt{\vtheta}) g(\tau_i|\wt{\vtheta})} = \nabla J(\wt{\vtheta})$.\footnote{The reader can refer to App.~\ref{} for off-policy gradients and variants of REINFORCE and G(PO)MDP.}
This property will be used to prove Lem.~\ref{L:svrpg.properties}.
%\begin{align*}
%&\blacktriangledown J(\vtheta) = \frac{1}{N}\sum_{j=0}^{N-1}\nabla\log\pi(\tau_j \vert \tilde{\vtheta})\Reward(\tau_j) +\\
%&\quad\frac{1}{B}\left[\sum_{i=0}^{B-1}
%\nabla\log\pi(\tau_i \vert \vtheta)\Reward(\tau_i) 
%- \omega(\tau_i)\nabla\log\pi(\tau_i \vert \tilde{\vtheta})\Reward(\tau_i)\right].
%\end{align*}
The use of mini-batches is a common practice in SVRG since it can yield a performance improvement even in the supervised case~\citep{harikandeh2015stopwasting,konevcny2016mini}. It is easy to show that the SVRPG estimator has the following, desirable properties (see App.~\ref{app:proofs}):
\begin{lemma}\label{L:svrpg.properties}
Denote by $\wh{\nabla}_N^{\textsc{RF}} J(\vtheta)$ the REINFORCE estimator in~\eqref{E:policygradient.estimate} and let $\vtheta^* \in \argmin_{\vtheta} \{J(\vtheta)\}$. Then, the SVRG estimate in~\eqref{E:svrpg.estimate.batch} is \emph{unbiased}
\[
\mathop{\mathbb{E}}
\left[\blacktriangledown J(\vtheta)\right] = \gradJ{\vtheta}.
\]
and regardless of the mini-batch size $B$:\footnote{
For any vector $\mathbf{x}$, we use $\Var[\mathbf{x}]$ to denote the trace of the covariance matrix, \ie $\Tr\EVV[]{(\mathbf{x}-\EVV[]{\mathbf{x}})(\mathbf{x}-\EVV[]{\mathbf{x}})^T}$.}
\[
	\Var\left[\gradBlack{\vtheta^*}\right] = 
    \Var\left[\wh{\nabla}_N^{\textsc{RF}}J(\vtheta^*)\right].
\]
\end{lemma}

%\begin{restatable}{lemma}{unbias}\label{lemma:unbias}
%The SVRPG estimator is unbiased:
%\[
%\mathop{\mathbb{E}}
%\left[\blacktriangledown J(\vtheta)\right] = \gradJ{\vtheta}.
%\]
%\end{restatable}
%\begin{restatable}{lemma}{zerovar}\label{lemma:zerovar}
%At convergence, the variance of the SVRPG estimator is equal to the variance of the REINFORCE estimator, regardless of the mini-batch size $B$:
%\[
%	\mathbb{V}ar\left[\gradBlack{\vtheta^*}\right] = 
%	\mathbb{V}ar\left[\gradApp{\vtheta^*}{N}\right].
%\]
%\end{restatable}
In particular, the latter result suggests that an SVRG-like algorithm using $\gradBlack{\vtheta}$ can actually achieve faster convergence, by performing much more parameter updates with the same data without introducing additional variance (at least asymptotically).
The resulting SVRPG policy optimization method is detailed in Alg.~\ref{alg:svrpg}.\footnote{Note that when $t=0$ we can avoid to sample trajectories and instead perform directly the policy update using the FG. For sake of clarity, we have not explicitly differentiate this step from the standard update rule in Alg.~\ref{alg:svrpg}.} 

To obtain a method that can be used in practice, a number of additional details must be specified, namely the choice of epoch size $m$, the number $N$ of trajectories used to estimate the full gradient, the mini-batch size $B$, and the step size $\alpha$.
Adaptive selection of these meta-parameters is also possible, and is explored in Sec.~\ref{sec:stopping}.
However, the next section will provide general converge guarantees for SVRPG \textcolor{red}{with REINFORCE estimator}.

\begin{algorithm}[tb]
        \caption{SVRPG}
	\label{alg:svrpg}
	\begin{algorithmic}
        \STATE {\bfseries Input:} number of epochs $S$, epoch size $m$, step size $\alpha$, batch size $N$, mini-batch size $B$, gradient estimator $g$, initial parameter $\vtheta_{0}^0 := \wt{\vtheta}^0$
		\FOR{$s=0$ {\bfseries to} $S-1$}
        \STATE Sample $N$ trajectories $\{\tau_j\}$ from $\pi(\cdot\vert\tilde{\vtheta}^{s})$
        \STATE $ \wt{\mu} = \gradApp{\wt{\vtheta}^{s}}{N}$ (see Eq.~\ref{E:policygradient.estimate})% = \frac{1}{N}\sum_{j=0}^{N-1}g(\tau_j | \wt{\vtheta}^s)$
%        \STATE $ \vtheta^s_1 = \wt{\vtheta}^s + \alpha \wt{\mu}$
		\FOR{$t=0$ {\bfseries to} $m-1$}
        \STATE Sample $B$ trajectories $\{\tau_i\}$ from $\pi(\cdot\vert\vtheta_t^{s+1})$
        \STATE $c^s_t = \frac{1}{B} \sum\limits_{i=0}^{B-1} \left( g(\tau_i|\vtheta_t^s) - \omega(\tau_i|\vtheta^s_t, \wt{\vtheta}^s) g(\tau_i| \wt{\vtheta})^s\right)$
%        \STATE $c^{s+1}_t = \frac{1}{B} \sum\limits_{i=0}^{B-1}
%        \begin{aligned}[t]
%                & \Reward(\tau_i) \Big[ \score{}{\tau_i|\vtheta_t^{s+1}} \\
%                &- \omega(\tau|\vtheta^{s+1}_t, \wt{\vtheta}^s)\score{}{\tau_i|\wt{\vtheta}^{s}} \Big]
%        \end{aligned}$
        \STATE $v^{s}_t = \wt{\mu} + c^s_t$ % \gradApp{\wt{\vtheta}^{s}}{N} + c^{s}_t$
%%		\begin{align*}
%%		\blacktriangledown J(\vtheta_t^{s+1}) = 
%%		&\gradApp{\tilde{\vtheta}^s}{N} \\
%%		&+\frac{1}{B}\sum_{i=0}^{B-1}\left[ 
%%		\score{}{\tau_i\vert\vtheta_t^{s+1}}\Reward(\tau_i)\right. \\
%%		&\left. - \omega(\tau_i)\score{}{\tau_i \vert \tilde{\vtheta}^{s}}\Reward(\tau_i)\right]
%%		\end{align*}
        \STATE $\vtheta_{t+1}^{s} = \vtheta_t^{s} + \alpha v^{s}_t$
		\ENDFOR
        \STATE $\vtheta_0^{s+1} := \tilde{\vtheta}^{s+1} = \vtheta_{m}^s$
		\ENDFOR
        \STATE {\bfseries return} $\vtheta_t^s$ with $(s,t)$ picked uniformly at random from $\{[0,S[\times[0,m[\} \cup \{(S,0)\}$
	\end{algorithmic}
\end{algorithm}

\section{Convergence Guarantees of SVRPG}\label{sec:conv}
In this section, we state the convergence guarantees for SVRPG with REINFORCE estimator.\footnote{Similar results can be derived for G(PO)MDP, see App.~\ref{app:}.}
We mainly leverage on the recent analysis of \textcolor{red}{non-concave} SVRG~\cite{reddi2016stochastic,allen2016variance}.
Each of the three challenges presented at the beginning of Sec.~\ref{sec:alg} can potentially prevent convergence, so we need additional assumptions.
In App.~\ref{app:gauss} we show how the widespread Gaussian policy satisfies these assumptions.

\textit{1) Non-concavity.} A common assumption in this case it to assume the objective function to be $L$-smooth.
However, in RL we can consider the following assumption which is sufficient for the $L$-smoothness of the objective (see Lem.~\ref{lemma:lsmooth}).
	\begin{restatable}[On policy derivatives]{assumption}{boundedscore}\label{ass:bounded_score}
		For each trajectory $\tau$, value of $\vtheta$ and dimensions $i,j$ there are positive constants $G,F<\infty$ such that:
\[
		\left|\nabla_{\theta_i}\log\pi_{\vtheta}(\tau)\right| \leq \GRADLOG, \qquad
        \left|\frac{\partial^2}{\partial\theta_i\partial\theta_j}\log\pi_{\vtheta}(\tau)\right| \leq \HESSLOG.
\]
	\end{restatable}

\textit{2) FG Approximation.}
Since we cannot compute an exact full gradient, we require the variance of the estimator to be bounded.
This assumption is in spirit similar to the one in~\citep{harikandeh2015stopwasting}.
	\begin{restatable}[On the variance of the gradient estimator]{assumption}{varreinforce}\label{ass:REINFORCE}
		There is a constant $V<\infty$ such that, for any policy $\pol$:
		\[
			\Var\left[g(\cdot\vert\vtheta)\right] \leq \VARRF.
		\]
	\end{restatable}

\textit{3) Non-stationarity.} 
Similarly to what done in SL~\citep{cortes2010learning}, we require the variance of the importance weight to be bounded.
	\begin{restatable}[On the variance of importance weights]{assumption}{varweights}\label{ass:M2}
		There is a constant $W<\infty$ such that, for each pair of policies encountered in Alg.~\ref{alg:svrpg} and for each possible trajectory,
		\[
                \mathbb{V}ar\left[\omega(\tau| \vtheta_1, \vtheta_2)\right] \leq \VARIS, \quad \forall \vtheta_1,\vtheta_2 \in \realspace^d , \tau \sim \pi_{\vtheta_1}.
		\]
	\end{restatable}
Differently from Ass.~\ref{ass:bounded_score} and ~\ref{ass:REINFORCE}, Ass.~\ref{ass:M2} can be enforced by a proper handling of the epoch size $m$.

We can now state the convergence guarantees for SVRPG.
\begin{restatable}[Convergence of the REINFORCE-SVRPG algorithm]{theorem}{convergence}\label{theo:convergence}
Consider the REINFORCE estimator in Eq.~\ref{E:policygradient.estimate}.
Under Ass.~\ref{ass:bounded_score}, \ref{ass:REINFORCE} and \ref{ass:M2}, the parameter vector $\vtheta_A$ returned by Alg.~\ref{alg:svrpg} after $T=m\times S$ iterations has, for some positive constants $\psi,\zeta, \xi$ and for proper choice of the step size $\alpha$ and the epoch size $m$, the following property:
\begin{align*}
	&\EVV[]
	{\norm[2]{\nabla J(\vtheta_A)}^2} 
		\leq
		\frac{J(\vtheta^*)-J(\vtheta_0)}{\psi T} +
		\frac{\zeta}{N}
		+\frac{\xi}{B}
\end{align*}
\end{restatable}
% Specific constants and meta-parameter constraints are provided in the proof of the theorem in Appendix \ref{app:proofs}.
Refer to App.~\ref{app:proofs} for a detailed proof involving definition of the constants and the meta-parameter constraints.
%The theoretical settings may be too conservative.
In Sec.~\ref{sec:stopping} we propose a joint selection of step size $\alpha$ and epoch size $m$ which is more suited for applications. 
By analysing the upper-bound in Thm.~\ref{theo:convergence} we observe that: I) the $\mathcal{O}(\nicefrac{1}{T})$ term is coherent with results on \textcolor{red}{non-concave} SVRG~\citep[\eg][]{reddi2016stochastic}; II) the $\mathcal{O}(\nicefrac{1}{N})$ term is due to the FG approximation and is analogous to the one in~\citep{harikandeh2015stopwasting}; III) the $\mathcal{O}(\nicefrac{1}{B})$ term is due to importance weighting.
To achieve true convergence, both the batch size $N$ and the mini-batch size $B$ should increase over time. 
In practice, it is enough to choose $N$ and $B$ large enough to make the second and the third term negligible, \ie to mitigate the variance introduced by FG approximation and importance sampling, respectively.
Since the optimal configuration may be task-dependent, we leave this choice to the empirical analysis of Sec.~\ref{sec:exp}. 

\textbf{G(PO)MDP.} A similar result to the one stated in Thm~\ref{theo:convergence} can be derived for the G(PO)MDP estimator. A sketch of the proof is reported in~\ref{app:gpomdpproof}.
In practice, due to the lower variance G(PO)MDP is preferred to REINFORCE.
For this reason we will use G(PO)MDP for the experiments (see Sec.~\ref{sec:exp}).
\todopir{sketch of proof for GPOMDP in appendix, probably can be unified with the one of REINFORCE}

\section{Remarks on SVRPG}
The convergence guarantees presented in the previous section come with requirements on the meta-parameters (\ie $\alpha$ and $m$) that may be too conservative for practical applications.
Here we provide a practical and automatic way to choose the step size $\alpha$ and the number of sub-iterations $m$ performed between snapshots.
Additionally, we provide variants of SVRPG exploiting variance-reduction techniques for importance weights. Despite lacking of theoretical guarantees, we will show that these methods are able to outperform the baseline SVRPG (Alg.~\ref{alg:svrpg}).

\subsection{Meta-Parameters Selection}\label{sec:stopping}
% In section \ref{sec:conv}, we introduced the practical need to properly choose $\alpha$ and $m$, which are, respectively, the step size and the number of sub-iterations to perform after a snapshot.
% At this point, the open question is how to properly select the step size $\alpha$ and the sub-iteration number $m$ in practical applications.
The step size $\alpha$ is crucial to balance the variance due to the sub-iterations and the variance associated to the estimator at the snapshot.
% The first parameter is crucial to balance the variance introduced by estimations within sub-iterations with respect to the variance introduced by estimations associated to the snapshots.
The number $m$ of sub-iterations controls the variance introduced by the importance weights. Low $m$ values are associated to low variance but induce lots of snapshot points (\ie lots of FG computations). High $m$ values allow to move policy $\pi_{\vtheta_t}$ away from the snapshot policy $\pi_{\wt{\vtheta}}$, causing high variance of the importance weights.
% the farther the distance between the snapshot policy and the current one, the higher the variance of the importance weights.

\textbf{Adaptive step size.}
A common way to deal with noisy gradients is to use adaptive strategies to compute the step size.
ADAptive Moment estimation (ADAM)~\citep{kingma2014adam} stabilizes the parameter update by computing learning rates for each parameter based on an incremental estimate of the gradient variance.
Due to this feature we would like to incorporate ADAM in the structure of the SVRPG update.
Recall that SVRPG is characterized by two updates: I) FG update in the snapshot; II) corrected gradient update in the sub-iterations.
Given this structure, we suggest to use two ADAM estimators:
\begin{align*}
        \vtheta^{s}_1 &= \wt{\vtheta}^s + \alpha^{\textsc{FG}}_s\wh{\nabla}_N J(\wt{\vtheta}^s)\\
        \vtheta^{s}_{t+1} &= \vtheta^{s}_t + \alpha^{\textsc{SI}}_{s,t} v_t^{s}
\end{align*}
where $\alpha^{\textsc{FG}}_{s}$ is associated to the snapshot and $\alpha^{\textsc{SI}}_{s,t}$ with the sub-iterations.
{\color{red} Note that the ADAM estimator used in the sub-iterations is reset at each snapshot with the FG estimate (see App.~\ref{app:adam} for details).}
\todopir{Add ADAM definition and explain in more details}
By doing so we decouple the contribution of the variance introduced by the approximate FG from the one introduced by the sub-iterations.
Note that these two terms have different orders of magnitude since are estimated with a different number of trajectories ($B \ll N$) and w/o importance weights.
The use of two ADAM estimators allows to capture and exploit this property.

\textbf{Adaptive Sub-Iterations.}
It is easy to imagine that a predefined schedule (\eg $m$ fixed in advance or changed with a policy independent process) may poorly perform due to the high variability of the updates.
In particular, fixed a value $m$ the variance of the updates in the sub-iterations depends on the snapshot policy and sampled trajectories.
This variability is in part captured by the ADAM estimate. Hence, we propose to take a snapshot (\ie interrupt the sub-iterations) when the step size $\alpha^{\textsc{SI}}$ proposed by ADAM for the sub-iterations is smaller than the one for the FG (\ie $\alpha^{\textsc{FG}}$).
If the latter condition is verified, it amounts to say that the noise in the corrected gradient has overcome the information of the FG.
Formally, the stopping condition is as follow
\[
        \text{If }        \frac{\alpha^{\textsc{FG}}}{N} > \frac{\alpha^{\textsc{SI}}}{B} \textbf{ then } \text{take snapshot}
\]
where we have introduced $N$ and $B$ to take into account the trajectory efficiency (\ie weighted advantage).
The less the number of trajectories used to update the policy, the better.
This weighted approach comes from the fact that, as mentioned above, there is no advantage in having $m$ too small.
Thus, we aim to facilitate sub-iterations rather than snapshots.

\subsection{Normalized Importance Sampling}\label{sec:prac}
\todopir{TODO: This section should be generic for normalized importance sampling trying to explain when and where are the advantages of doing this. Explicit formulation can be postponed in appendix.}
We introduce a term with importance weights to guarantee proper sampling of trajectories.
This term's variance is subjected to the variance of importance weights. 
There are some methods to reduce this variance.

As reported in Section \ref{subsec:PolicyGradient} we use G(PO)MDP gradient estimator~\cite{baxter2001infinite}, which gives us the same guarantees we have for the Reinforce Estimator.
One variant in off-policy G(PO)MDP is to use the weighted per-decision importance sampling estimator \cite{precup2000eligibility}, allowing a reduction of the gradient estimator variance. The off-policy gradient estimator becomes:
\begin{align*}
\gradApp{\vtheta}{N} = \frac{1}{\Omega}\sum_{n=1}^{N}\sum_{h=0}^{H-1}\left(\sum_{k=0}^{h}\score{\vtheta}{a_k^n\vert s_k^n}\right)\left(\gamma^h r_h^n \omega(z_{0:h})\right).
\end{align*} 
where:
\begin{align*}
\omega(z_{0:h}) = \prod_{k=0}^{h} \frac{\pi_{\vtheta}({a_k^n\vert s_k^n})
 }{\pi_{\vtheta^{'}}({a_k^n\vert s_k^n})}
\end{align*}
\begin{align*}
\Omega = \sum_{n=1}^{N}\sum_{h=0}^{H-1}\gamma^h\prod_{k=0}^{h} \frac{\pi_{\vtheta}({a_k^n\vert s_k^n})
}{\pi_{\vtheta^{'}}({a_k^n\vert s_k^n})}
\end{align*}
\todomat{why should we use it? What advantages do we expect? At what cost? Pointer to experiments}
\section{Related Work}
\todopir{I think that this section is worth here}

\section{Experiments}\label{sec:exp}
We have designed our experiments to investigate whether:
\begin{enumerate}
\item The algorithm reaches high value of the reward function faster than GPOMDP.
\item Averagely SVRPG keep the reward function more stable than GPOMDP.
\end{enumerate}

In order to pursue the above mentioned objectives, we evaluate the behavior of the reward function averaged on 10 runs for each experiment.

We decide to use the following models of Rllab: \emph{Cart-Pole Balancing}, \emph{Mujoco Swimmer}, \emph{Mujoco Half Cheetah}.


In all the experiments we compete with GPOMDP standard algorithm. GPOMDP uses a number of trajectories per batch that is equal to the number of trajectories used in SVRPG mini-batch size.
The neural network and the learning rate are the same for GPOMDP and SVRPG.
In SVRPG we used two different Adam parameters: the first one for Snapshot and the second one for sub-iterations.
We used the stopping condition defined in Section \ref{sec:stopping}.
We fixed the maximum number of trajectories that both the algorithms can use as shown in appendices. 

The neural networks is composed of one layer with 8 neurons for Cart-Pole environment and by two layers with 32x32 neurons for Swimmer and for Half Cheetah environments. In each experiment, the length of episodes is 500 steps.

\section{Conclusion}
In this paper we introduce SVRPG, a variant of SVRG specifically designed for RL problems.
RL has a series of difficulties non-common in SL.
Among them non-concavity and approximate estimate of the FG have been analysed independently in SL~\citep[\eg][]{allen2016variance,reddi2016stochastic,harikandeh2015stopwasting} but never combined.
Nevertheless, the main issue in RL is the non-stationarity of the sampling process since the distribution underlying the objective function is policy dependent.
We have shown that by exploiting importance weighting techniques it is possible to overcome this issue and preserve the unbiasedness of the corrected gradient.
SVRG has been previously used in RL for policy evaluation~\citep{du2017svrgpe} and as alternative optimization method for TRPO~\citep{xu2017svrgtrpo}.
The first case is much more similar to the SL case since the problem is stationary and may be made concave, removing several issues we have faced here.
In the second case it was used as a better optimization technique to solve the per-iteration problem imposed by TRPO.
Additional, as far as we know, we are the first to report convergence guarantees for SVRG in the RL scenario.
Empirically, we have shown that practical variants of the theoretical SVRPG are able to outperform classical actor-only approaches on several benchmarks.




\bibliography{svrpg}
\bibliographystyle{icml2018}

\newpage
\mbox{}
\newpage
\onecolumn
\appendix

\section{Policy Gradient Estimators} \label{A:gradient_estimators}


\subsection{On-policy}
The REINFORCE gradient estimator~\citep{williams1992simple} provides a simple, unbiased way of estimating the gradient:
\begin{align*}
\gradApp{\vtheta}{N} = \frac{1}{N}\sum_{n=1}^{N}\left(\sum_{h=0}^{H-1}\score{\vtheta}{a_h^n\vert s_h^n}\right)\left(\sum_{h=0}^{H-1}\gamma^h r_h^n - b\right),
\end{align*}
where subscripts denote the time step, superscripts denote the trajectory, $r_h$ is the reward actually collected at time $h$ and b can be any baseline provided $\EVV[a \sim \pol]{b(s,a)} = 0$, for any $s\in\Sspace$.
The G(PO)MDP gradient estimator~\cite{baxter2001infinite} is a refinement of REINFORCE which is subject to less variance \cite{zhao2011analysis} while preserving the unbiasedness:
\begin{align*}
\gradApp{\vtheta}{N} = \frac{1}{N}\sum_{n=1}^{N}\sum_{h=0}^{H-1}\left(\sum_{k=0}^{h}\score{\vtheta}{a_h^n\vert s_h^n}\right)\left(\gamma^h r_h^n - b\right).
\end{align*}

\subsection{Off-Policy}
\todopir{Make ref to Luca's thesis and describe in brief the derivations}



\section{Proofs}\label{app:proofs}
In this Appendix we prove all the claims made in the paper, with the primary objective of proving Theorem \ref{theo:convergence}. Our proof is adapted from the one of Theorem 2 from \cite{reddi2016stochastic} and has a very similar structure, but with all the additional challenges and assumptions described in Section \ref{sec:conv}.

\subsection*{Definitions}
We give some additional definitions which will be useful in the proofs.

\begin{definition}
For a random variable $X$:
\begin{align*}
	&\mathbb{E}_{t\vert s}\left[X\right] \coloneqq 
		\mathop{\mathbb{E}}_{\substack{\tau_j\sim\pi(\cdot\vert\tilde{\vtheta}^s)\forall j \in {\it {\bf N}} \\ \tau_{i,h}\sim\pi(\cdot\vert\vtheta^{s+1}_i) \forall h \in {\it {\bf B}}, \text{ for $i=0,\dots,t-1$}}}{\left[X \vert \tilde{\vtheta^s}\right]} \\
	&\coloneqq \EVV[\tau_j\sim\pi(\cdot\vert\tilde{\vtheta}^s)\forall j \in {\it {\bf N}}]{
			\EVV[\tau_{0,h}\sim\pi(\cdot\vert\vtheta_0^{s+1}) \forall h \in {\it {\bf B}}]
				{\dots
					\EVV[\tau_{t,h}\sim\pi(\cdot\vert\vtheta_t^{s+1})\forall h \in {\it {\bf B}}]
						{X\vert\vtheta_t^{s+1}}
				 \dots
			\vert\vtheta_0^{s+1}}
		\vert\tilde{\vtheta}^s},
\end{align*}
where the sequence $\tilde{\vtheta}^s,\vtheta_0^{s+1},\dots,\vtheta_t^{s+1}$ is defined in Algorithm \ref{alg:svrpg}, ${\it {\bf N}} = [0,\dots,N)$, and ${\it {\bf B}} = [0,\dots,B)$. 
\end{definition}
Intuitively, the $\Ets{\cdot}$ operator computes the expected value with respect to the sampling of trajectories from the snapshot $\tilde{\vtheta}^s$ up to the $t$-th iteration included. Note that the order in which expected values are taken is important since each $\vtheta_{t}^{s+1}$ is function of previously sampled trajectories and is used to sample new ones.

\begin{definition}
For random vectors X, Y:
\begin{align*}
	\Covts{X}{Y} &\coloneqq \Tr\left(\Ets{(X-\Ets{X})(Y-\Ets{Y})^T}\right), \\
	\Varts{X} &\coloneqq \Covts{X}{Y},
\end{align*}
where $\Tr(\cdot)$ denotes the trace of a matrix.
\end{definition}

\begin{definition}
The full gradient estimation error is:
\[
	e_s \coloneqq \gradApp{\tilde{\vtheta}^s}{N} - \gradJ{\tilde{\vtheta}^s} 
\]
\end{definition}

\begin{definition}\label{def:ideal}
The ideal SVRPG gradient estimate is:
\begin{align*}
	\gradIdeal{\vtheta_t^{s+1}} &\coloneqq 
	\gradJ{\tilde{\vtheta}^s}
	+ \nabla\log\pi(\tau_i \vert \vtheta_t^{s+1})\Reward(\tau_i) 
	- \omega(\tau_i)\nabla\log\pi(\tau_i \vert \tilde{\vtheta}^s)\Reward(\tau_i) \\
	&= \gradBlack{\vtheta_t^{s+1}} - \gradApp{\tilde{\vtheta}^s}{N} + \gradJ{\tilde{\vtheta}^s} \\
	&= \gradBlack{\vtheta_t^{s+1}} - e_s
\end{align*}
\end{definition}


\subsection*{Basic Lemmas}
We prove two basic properties of the SVRPG update.

%\unbias*
\begin{restatable}{lemma}{unbias}\label{lemma:unbias}
The SVRPG estimator is unbiased:
\[
\mathop{\mathbb{E}}
\left[\blacktriangledown J(\vtheta)\right] = \gradJ{\vtheta}.
\]
\end{restatable}
\begin{proof}
\begin{align*}
\EVV[]{\gradBlack{\vtheta}} &= \EVV[]{\gradApp{\tilde{\vtheta}}{N}}  + \EVV[]{\gradApp{\vtheta}{B}} - \EVV[]{\frac{1}{B}\sum_{i=0}^{B-1}\omega(\tau_i)\score{\tilde{\vtheta}}{\tau_i}R(\tau_i)} \\
&= \gradJ{\tilde{\vtheta}} + \gradJ{\vtheta} - \gradJ{\tilde{\vtheta}} = \gradJ{\vtheta}.
\end{align*}
Note that the importance weight is necessary to guarantee unbiasedness, since the $\tau_i$ are sampled from $\pi_{\vtheta}$.
\end{proof}

% \zerovar*
\begin{restatable}{lemma}{zerovar}\label{lemma:zerovar}
At convergence, the variance of the SVRPG estimator is equal to the variance of the REINFORCE estimator, regardless of the mini-batch size $B$:
\[
	\Var\left[\gradBlack{\vtheta^*}\right] = 
	\Var\left[\gradApp{\vtheta^*}{N}\right].
\]
\end{restatable}
\begin{proof}
As $\vtheta\to\vtheta^*$, also $\tilde{\vtheta}\to\vtheta^*$. Hence, by continuity of $J(\vtheta)$:
\begin{align*}
\Var\left[\gradBlack{\vtheta}\right] &\to \Var\left[\gradApp{\vtheta^*}{N}\right] + \frac{1}{B}\Var\left[\score{\vtheta^*}{\tau}R(\tau) - \cancel{\omega(\tau)}\score{\vtheta^*}{\tau}R(\tau)\right] \\
&= \Var\left[\gradApp{\vtheta^*}{N}\right].
\end{align*}
Note that it is important that the trajectories used in the second and the third term are the same for the variance to vanish.
\end{proof}

\subsection*{Ancillary Lemmas}
Before addressing the main convergence theorem, we prove some useful lemmas.

 
\begin{restatable}[]{lemma}{L-smoothness}\label{lemma:lsmooth}
	Under Assumption \ref{ass:bounded_score}, $J(\vtheta)$ is L-smooth for some positive Lipschitz constant $L$.
\end{restatable}
\begin{proof}
By definition of $J(\vtheta)$:
\begin{align}
\Dij{J(\vtheta)}{\theta} 
&= \int_{\Tspace}\Dij{}{\theta}\pol(\tau)\Reward(\tau)\de \tau
\nonumber\\ 
&= \int_{\Tspace}\pol(\tau)\score{\theta}{\tau}\score{\theta}{\tau}^T\Reward(\tau)\de \tau + \int_{\Tspace}\pol(\tau)\Dij{}{\theta}\log\pol(\tau)\Reward(\tau)\de \tau \nonumber\\
&\leq \sup_{\tau} \left\{\left|\Reward(\tau)\right|\right\} \left(\GRADLOG^2+\HESSLOG\right) \label{eq:0}\\
&= \frac{R}{1-\gamma}\left(\GRADLOG^2+\HESSLOG\right),\nonumber
\end{align}
where \ref{eq:0} is from Assumption \ref{ass:bounded_score}.
Since the Hessian is bounded, $J(\vtheta)$ is Lipschitz-smooth.
\end{proof}

%Lemma 2
\begin{restatable}[]{lemma}{auxtwo}\label{lemma:aux2}
Under Assumption \ref{ass:bounded_score}
, the expected squared norm of the SVRPG gradient can be bounded as follows:
\[
\Ets{\gradBlack{\vtheta_t^{s+1}}} \leq
\Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
+L^2\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
+\frac{1}{N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)}
\nonumber 
+\frac{G^2R^2W}{(1-\gamma)^2B}
\]
\end{restatable}
\begin{proof}
	\begin{align}
	\Ets{\gradBlack{\vtheta_t^{s+1}}} 
	&= \Ets{\norm[]{\gradApp{\tilde{\vtheta}^s}{N}
			+\frac{1}{B}\sum_{i=0}^{B-1} g(\tau_i\vert\vtheta_t^{s+1}) 
			-\frac{1}{B}\sum_{i=0}^{B-1}
			\omega(\tau_i)g(\tau_i\vert\tilde{\vtheta}^s)}^2} \nonumber\\
	%
	&= \mathbb{E}_{t\vert s}\left[\left\|\gradApp{\tilde{\vtheta}^s}{N}
			+\frac{1}{B}\sum_{i=0}^{B-1}\left( 
			g(\tau_i\vert\vtheta_t^{s+1}) -
			\omega(\tau_i)g(\tau_i\vert\tilde{\vtheta}^s)\right)
			\right.\right.\nonumber\\&\qquad\left.\left.
			-\gradJ{\vtheta_t^{s+1}} + \gradJ{\tilde{\vtheta}^s}
			+\gradJ{\vtheta_t^{s+1}} - \gradJ{\tilde{\vtheta}^s}\right\|^2\right] \nonumber\\
	%
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2}
	+\Ets[]{\norm[]{\gradApp{\tilde{\vtheta}^s}{N} - \Ets[]{\gradApp{\tilde{\vtheta}^s}{N}}}^2} \nonumber\\
	&\qquad+ 
	\mathbb{E}_{t\vert s}\left[\left\|
		\frac{1}{B}\sum_{i=0}^{B-1}\left(
		g(\tau_i\vert\vtheta_t^{s+1}) -
			\omega(\tau_i)g(\tau_i\vert\tilde{\vtheta}^s)\right)
		\right.\right.\nonumber\\&\qquad\left.\left.
		- \Ets{
			\frac{1}{B}\sum_{i=0}^{B-1}\left(
			g(\tau_i\vert\vtheta_t^{s+1}) -
				\omega(\tau_i)g(\tau_i\vert\tilde{\vtheta}^s)\right)}\right\|^2\right] 
	\nonumber\\
	%
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)}
	\nonumber\\
	&\qquad+ 
		\mathbb{E}_{t\vert s}\left[\left\|
		\frac{1}{B}\sum_{i=0}^{B-1}\left(
		g(\tau_i\vert\vtheta_t^{s+1}) -
		\omega(\tau_i)g(\tau_i\vert\tilde{\vtheta}^s)\right)
		\right.\right.\nonumber\\&\qquad\left.\left.
		- \Ets{
			\frac{1}{B}\sum_{i=0}^{B-1}\left(
			g(\tau_i\vert\vtheta_t^{s+1}) -
			\omega(\tau_i)g(\tau_i\vert\tilde{\vtheta}^s)\right)}\right\|^2\right] 
		\label{eq:1}\\%
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)} \nonumber\\
	&\qquad+\Ets{\norm[]{
			\frac{1}{B}\sum_{i=0}^{B-1}\left(
			g(\tau_i\vert\vtheta_t^{s+1}) -
			\omega(\tau_i)g(\tau_i\vert\tilde{\vtheta}^s)\right)}^2} \label{eq:2}\\
	%
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)} \nonumber\\
	&\qquad+
			\frac{1}{B^2}\sum_{i=0}^{B-1}
			\Ets{\norm[]{
			g(\tau_i\vert\vtheta_t^{s+1}) -
			\omega(\tau_i)g(\tau_i\vert\tilde{\vtheta}^s)}^2} \nonumber\\
	%
	&= \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)}
	\nonumber\\
	&\qquad+
			\frac{1}{B^2}\sum_{i=0}^{B-1}
			\Ets{\norm[]{g(\tau_i\vert\vtheta_t^{s+1})
			-g(\tau_i\vert\tilde{\vtheta}^s)
			+g(\tau_i\vert\tilde{\vtheta}^s) 
			-\omega(\tau_i)g(\tau_i\vert\tilde{\vtheta}^s)}^2} \nonumber\\
	%
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)}
	\nonumber\\
	&\qquad+
			\frac{1}{B^2}\sum_{i=0}^{B-1}
			\Ets{\norm[]{g(\tau_i\vert\vtheta_t^{s+1})
			-g(\tau_i\vert\tilde{\vtheta}^s)}^2} \nonumber\\
	&\qquad
			+\frac{1}{B^2}\sum_{i=0}^{B-1}
			\Ets{\norm[]{g(\tau_i\vert\tilde{\vtheta}^s) 
			-\omega(\tau_i)g(\tau_i\vert\tilde{\vtheta}^s)}^2} \nonumber\\
	%
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)}
	+\frac{L^2}{B}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}\nonumber\\
	&\qquad+
		\frac{1}{B^2}\sum_{i=0}^{B-1}
		\Ets{\norm[]{g(\tau_i\vert\tilde{\vtheta}^s) 
			-\omega(\tau_i)g(\tau_i\vert\tilde{\vtheta}^s)}^2} \label{eq:3}
% ...
\end{align}
\begin{align}
% ...
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)}
	\nonumber\\
	&\qquad+\frac{L^2}{B}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
	+\GRADLOG^2\frac{R^2}{(1-\gamma)^2}\frac{1}{B^2}\sum_{i=0}^{B-1}\Ets{(\omega(\tau_i)-1)^2} \label{eq:4}\\
	%
	&= \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)}
	\nonumber\\
	&\qquad+\frac{L^2}{B}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
	+\GRADLOG^2\frac{R^2}{(1-\gamma)^2b^2}\sum_{i=0}^{B-1}\Varts{\omega(\tau_i)} \nonumber\\
	%
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)}
	\nonumber\\
	&\qquad+\frac{L^2}{B}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
	+\GRADLOG^2\frac{R^2\VARRF}{(1-\gamma)^2B}, \label{eq:5}
\end{align}
where (\ref{eq:1}) is from the definition of $\gradApp{\vtheta}{N}$, (\ref{eq:2}) is from $\EVV[]{\norm[]{X-\EVV[]{X}}^2}\leq\EVV[]{\norm[]{X}^2}$ for any stochastic variable $X$, (\ref{eq:3}) is from L-smoothness, 
(\ref{eq:4}) is from Assumption \ref{ass:bounded_score} 
{\todomatout{We could generalize to GPOMDP by defining some more constants}\color{red}and the definition of REINFORCE}, and (\ref{eq:5}) is from Assumption \ref{ass:M2}.
\end{proof}

%Lemma 0
\begin{restatable}[]{lemma}{auxzero}\label{lemma:aux0}
Under Assumption \ref{ass:bounded_score}, for any function $f(\vtheta_t^{s+1})$ which is deterministic for a fixed $\vtheta_t^{s+1}$:
\begin{align*}
\left|\Ets[t]{\dotprod{\gradBlack{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}}
-\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}}
\right|
\leq
\frac{1}{2N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)} +\frac{1}{2}\Ets[t-1]{\norm[]{f(\vtheta_t^{s+1})}^2}
\end{align*}
\end{restatable}
\begin{proof}
\begin{align}
	\Ets{\dotprod{\gradBlack{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}}
	&=
	\Ets{\dotprod{\gradIdeal{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} +
	\Ets[t-1]{\dotprod{e_s}{f(\vtheta_t^{s+1})}} \label{eq:6}\\
	&=
	\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} +
	\Ets[t-1]{\dotprod{e_s}{f(\vtheta_t^{s+1})}} \label{eq:7}\\
	&=
	\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} \nonumber\\
	&\qquad+
	\dotprod{\Ets{e_s}}{\Ets{f(\vtheta_t^{s+1})}}
	+\Covts[t-1]{\gradApp{\tilde{\vtheta}^s}{N}}{f(\vtheta_t^{s+1})}  \nonumber\\
	&= 
	\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} \nonumber\\
	&\qquad+
	\Covts[t-1]{\gradApp{\tilde{\vtheta}^s}{N}}{f(\vtheta_t^{s+1})} \label{eq:8}
\end{align}
Hence:
\begin{align}
	\left|\Ets{\dotprod{\gradBlack{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}}
	\right.&-\left.\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}}\right| 
	=
	\left|\Covts[t-1]{\gradApp{\tilde{\vtheta}^s}{N}}{f(\vtheta_t^{s+1})}\right|  
	\nonumber\\
	&\leq
	\sqrt{\Varts[]{\gradApp{\tilde{\vtheta}^s}{N}}}\circ\sqrt{\Varts[t-1]{f(\vtheta_t^{s+1})}} \nonumber\\
	%
	&\leq	
	\frac{\alpha_t}{2}\Varts[]{\gradApp{\tilde{\vtheta}^s}{N}} +\frac{\alpha_t}{2}\Varts[t-1]{f(\vtheta_t^{s+1})}\label{eq:9}\\
	%
	&=
	\frac{1}{2N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)} +\frac{1}{2}\Varts[t-1]{f(\vtheta_t^{s+1})} \label{eq:10}\\
	%
	&\leq
	\frac{1}{2N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)} +\frac{1}{2}\Ets[t-1]{\norm[]{f(\vtheta_t^{s+1})}^2},
	\nonumber
\end{align}
where (\ref{eq:6}) is from Definition \ref{def:ideal}; (\ref{eq:7}) is from the fact that $\gradIdeal{\vtheta_t^{s+1}}$ is both unbiased and independent from $f(\vtheta_t^{s+1})$ \wrt the sampling at time $t$ alone, which is not true for $\gradBlack{\vtheta_t^{s+1}}$; (\ref{eq:8}) is from $\Ets{e_s}=0$; (\ref{eq:9}) is from Young's inequality; and (\ref{eq:10}) is from the definition of $\gradApp{\vtheta}{N}$.
\end{proof}

%Lemma 1
\begin{restatable}[]{lemma}{auxone}\label{lemma:aux1}
Under Assumptions \ref{ass:bounded_score} ans \ref{ass:M2}, the expected squared norm of the true gradient $\gradJ{\vtheta_t^{s+1}}$, for appropriate choices of $\alpha_t\geq0$ and $\beta_t>0$, can be bounded as follows:
\[
	\Ets{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} \leq
	\frac{R_{t+1}^{s+1} - R_t^{s+1}}{\Psi_t} + \frac{d_tV}{N\Psi_t}
	+\frac{f_tW}{b\Psi_t},
\]
	where
\begin{align*}
	&R_t^{s+1}\coloneqq \Ets{J(\vtheta_t^{s+1}) - c_t\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}, \\
	&c_{m-1} = 0, \\
	&c_t = c_{t+1}\left(\frac{3}{2}+\alpha_t\beta_t+\alpha_t^2\right) + \alpha_t\frac{L^2}{b}+\frac{\alpha_t^2L^3}{2}, \\
	&\Psi_t = \alpha_t\left(1-\frac{c_{t+1}}{\beta_t}-\frac{\alpha_tL}{2}-\alpha_tc_{t+1}\right), \\
	&d_t = \frac{\alpha_t}{2}\left(1+2c_{t+1}+\alpha_t^2L+2\alpha_t^2c_{t+1}\right), \\
	&f_t = \alpha_t^2\frac{G^2R^2(L+2c_{t+1})}{2(1-\gamma)^2}.
\end{align*}
In particular, the following constraints on $\alpha_t$ and $\beta_t$ are sufficient:
\begin{align*}
&0\leq\alpha_t < \frac{2(1-\nicefrac{c_{t+1}}{\beta_t})}{L+2c_{t+1}} \\
&\beta_t > c_{t+1}.
\end{align*}
\end{restatable}
\begin{proof}
	We have:
	\begin{align}
	\Ets{J(\vtheta_{t+1}^{s+1})} 
	&\geq \Ets{J(\vtheta_t^{s+1})+\dotprod{\gradJ{\vtheta_t^{s+1}}}{\vtheta_{t+1}^{s+1}-\vtheta_t^{s+1}} - \frac{L}{2}\norm[]{\vtheta_{t+1}^{s+1}-\vtheta_t^{s+1}}^2} \label{eq:11}\\
	&= \Ets{J(\vtheta_t^{s+1})+\alpha_t\dotprod{\gradJ{\vtheta_t^{s+1}}}{\gradBlack{\vtheta_t^{s+1}}} - \frac{\alpha_t^2L}{2}\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2} \label{eq:12}\\
	&\geq
	\Ets{J(\vtheta_t^{s+1})+\alpha_t\norm[]{\gradJ{\vtheta_t^{s+1}}}^2 - \frac{\alpha_t^2L}{2}\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2} \nonumber\\
	&\qquad-
	\frac{\alpha_t}{2N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)} -\frac{\alpha_tL^2}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1} - \tilde{\vtheta}^s}^2}, \label{eq:13}
\end{align}
where (\ref{eq:11}) is from the L-smoothness of $J(\vtheta)$, (\ref{eq:12}) is from the SVRPG update, and (\ref{eq:13}) is from Lemma \ref{lemma:aux0}.

Next we have:
\begin{align}
\Ets{\norm[]{\vtheta_{t+1}^{s+1}-\tilde{\vtheta}^s}^2} 
&= \Ets{\norm[]{\vtheta_{t+1}^{s+1}- \vtheta_t^{s+1} + \vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \nonumber\\
&=\Ets{\norm[]{\vtheta_{t+1}^{s+1}-\vtheta_{t}^{s+1}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2+2\dotprod{\vtheta_{t+1}^{s+1}-\vtheta_{t}^{s+1}}{\vtheta_t^{s+1}-\tilde{\vtheta}^s}} \nonumber \\
&= \Ets{\alpha_t^2\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2+2\alpha_t\dotprod{\gradBlack{\vtheta_t^{s+1}}}{\vtheta_t^{s+1}-\tilde{\vtheta}^s}} \label{eq:14}\\
&\leq \Ets{\alpha_t^2\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2+2\alpha_t\dotprod{\gradJ{\vtheta_t^{s+1}}}{\vtheta_t^{s+1}-\tilde{\vtheta}^s}} \nonumber\\ 
&\qquad+
\frac{\alpha_t}{N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)} +\frac{1}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \label{eq:15}\\
%
&\leq \Ets{\alpha_t^2\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
+2\alpha_t\Ets{\left|\dotprod{\gradJ{\vtheta_t^{s+1}}}{\vtheta_t^{s+1}-\tilde{\vtheta}^s}\right|} \nonumber\\ 
&\qquad+
\frac{\alpha_t}{N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)} +\frac{1}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \nonumber\\
%
&\leq \Ets{\alpha_t^2\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
+2\alpha_t\Ets{\norm[]{\gradJ{\vtheta_t^{s+1}}}\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}} \nonumber\\ 
&\qquad+
\frac{\alpha_t}{N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)} +\frac{1}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \nonumber\\
%
&\leq \Ets{\alpha_t^2\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
+2\alpha_t\Ets{\frac{1}{2\beta}\norm[]{\gradJ{\vtheta_t^{s+1}}}^2+\frac{\beta}{2}\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \nonumber\\ 
&\qquad
\frac{\alpha_t}{N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)} +\frac{1}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}, \label{eq:16}
\end{align}
where (\ref{eq:14}) is from the SVRPG update, (\ref{eq:15}) is from Lemma \ref{lemma:aux0}, and (\ref{eq:16}) is from Young's inequality in the 'Peter-Paul' variant.
Finally:
\begin{align}
	R_{t+1}^{s+1} &= \Ets{J(\vtheta_{t+1}^{s+1}) - c_{t+1}\norm[]{\vtheta_{t+1}^{s+1}-\tilde{\vtheta}^s}^2} \nonumber\\
	%
	&\geq	\Ets{J(\vtheta_t^{s+1})+\alpha_t\norm[]{\gradJ{\vtheta_t^{s+1}}}^2 - \frac{\alpha_t^2L}{2}\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2} \nonumber\\
	&\qquad-
	\frac{\alpha_t}{2N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)} -\frac{\alpha_tL^2}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1} - \tilde{\vtheta}^s}^2}
	-c_{t+1}\Ets{\norm[]{\vtheta_{t+1}^{s+1}-\tilde{\vtheta}^s}^2} \label{eq:17}\\
	%
	&\geq \Ets{J(\vtheta_t^{s+1})+\alpha_t\norm[]{\gradJ{\vtheta_t^{s+1}}}^2 - \frac{\alpha_t^2L}{2}\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2} 
	-\frac{\alpha_t}{2N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)} \nonumber\\
	&\qquad -\frac{\alpha_tL^2}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1} - \tilde{\vtheta}^s}^2}
	-c_{t+1}\Ets{\norm[]{\vtheta_{t+1}^{s+1}-\tilde{\vtheta}^s}^2} \nonumber\\
	%
	&\geq \Ets{J(\vtheta_t^{s+1})+\alpha_t\norm[]{\gradJ{\vtheta_t^{s+1}}}^2 - \frac{\alpha_t^2L}{2}\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2} 
	-\frac{\alpha_t}{2N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)} \nonumber\\
	&\qquad -\frac{\alpha_tL^2}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1} - \tilde{\vtheta}^s}^2} -c_{t+1}\Ets{\alpha_t^2\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
	\nonumber\\
	&\qquad-2c_{t+1}\alpha_t\Ets{\frac{1}{2\beta}\norm[]{\gradJ{\vtheta_t^{s+1}}}^2+\frac{\beta}{2}\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \nonumber\\ 
	&\qquad
	-c_{t+1}\frac{\alpha_t}{N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)} -c_{t+1}\frac{1}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \label{eq:18}\\
	%
	&= \Ets{J(\vtheta_t^{s+1})} - \left(c_{t+1}\left(\frac{3}{2}+\alpha_t\beta\right)+\alpha_tL^2\right)\Ets{\norm[]{\vtheta_{t}^{s+1}-\tilde{\vtheta}}^2} \nonumber\\
	&\qquad-\alpha_t^2\left(\frac{L}{2}+c_{t+1}\right)\Ets{\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2}
	+\alpha_t\left(1-\frac{c_{t+1}}{\beta}\right)\Ets{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} \nonumber\\
	&\qquad-\frac{\alpha_t}{2N}\left(1+2c_{t+1}\right)\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)} \nonumber\\
	%
	&\geq  \Ets{J(\vtheta_t^{s+1})} - \left(c_{t+1}\left(\frac{3}{2}+\alpha_t\beta\right)+\alpha_tL^2\right)\Ets{\norm[]{\vtheta_{t}^{s+1}-\tilde{\vtheta}}^2} \nonumber\\
	&\qquad
	-\alpha_t^2\left(\frac{L}{2}+c_{t+1}\right)\left(\Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)}
	\right.\nonumber\\
	&\left.\qquad+\frac{L^2}{b}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
	+G^2\frac{R^2}{(1-\gamma)^2b}W\right)
	+\alpha_t\left(1-\frac{c_{t+1}}{\beta}\right)\Ets{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} \nonumber\\
	&\qquad-\frac{\alpha_t}{2N}\left(1+2c_{t+1}\right)\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)} \label{eq:19}\\
	%
	& = \Ets{J(\vtheta_t^{s+1}) - \left(c_{t+1}\left(\frac{3}{2}+\alpha_t\beta+\alpha_t^2\right)+\alpha_t\frac{L^2}{b}+\frac{\alpha_t^2L^3}{2}\right)\norm[]{\vtheta_{t}^{s+1}-\tilde{\vtheta}}^2} \nonumber\\
	&\qquad
	+\alpha_t\left(1-\frac{c_{t+1}}{\beta}-\frac{\alpha_tL}{2}-\alpha_tc_{t+1}\right)\Ets{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} \nonumber\\
	&\qquad-\frac{\alpha_t}{2N}\left(1+2c_{t+1}+\alpha_t^2L+2\alpha_t^2c_{t+1}\right)\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)} \nonumber\\
	&\qquad-\alpha_t^2\frac{(L+2c_{t+1})G^2R^2\VARIS}{2(1-\gamma)^2b} \nonumber\\
	%
	&= R_t^{s+1}
	+\Psi_t\Ets{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2}
	-\frac{d_t}{N}\Varts[]{g(\cdot\vert\tilde{\vtheta}^s)}
	-\frac{f_t}{b}\VARIS,\nonumber\\
	&\geq R_t^{s+1}
	+\Psi_t\Ets{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2}
	-\frac{d_t}{N}\VARRF
	-\frac{f_t}{b}\VARIS, \label{eq:20}
\end{align}
where (\ref{eq:17}) is from (\ref{eq:13}), (\ref{eq:18}) is from (\ref{eq:16}), (\ref{eq:19}) is from Lemma \ref{lemma:aux0}, and (\ref{eq:20}) is from Assumption \ref{ass:REINFORCE}.
To complete the proof, besides rearranging terms, we have to ensure that $\Psi_t>0$ for each $t$. This gives the constraints on $\alpha_t$ and $\beta_t$.
\end{proof}


\subsection*{Main theorem}
We finally provide the proof of the convergence theorem:

\convergence*
\begin{proof}
We prove the theorem for the following values of the constants:
\begin{align*}
& \psi \coloneqq \min_t\{\Psi_t\}, \\
& \zeta \coloneqq \frac{\max_t\{d_t\}V}{\psi} \\
& \xi \coloneqq \frac{\max_t\{f_t\}W}{\psi}.
\end{align*}
The values of all other constants and the constraints on the step size are provided in Lemma \ref{lemma:aux1}.

Starting from Lemma \ref{lemma:aux1}, summing over iterations of an epoch $s$ we get:
\begin{align}
\sum_{t=0}^{m-1}\Ets{\norm[]{\gradJ{(\vtheta_t^{s+1})}^2}}&\leq
 \frac{\sum_{t=0}^{m-1}\left(R_{t+1}^{s+1} - R_t^{s+1}\right)}{\psi} + \frac{m\zeta}{N} + \frac{m\xi}{B} \nonumber\\
 %
 &= \frac{\sum_{t=0}^{m-1}\left(\Ets[t+1]{J(\vtheta_{t+1}^{s+1}) - c_{t+1}\norm[]{\vtheta_{t+1}^{s+1}-\tilde{\vtheta}^s}^2} - \Ets[t]{J(\vtheta_{t}^{s+1}) - c_{t}\norm[]{\vtheta_{t}^{s+1}-\tilde{\vtheta}^s}^2}\right)}{\psi} \nonumber\\
 &\qquad+ \frac{m\zeta}{N} + \frac{m\xi}{B} \nonumber\\
 %
 &= \frac{\Ets[m]{\sum_{t=0}^{m-1}\left(J(\vtheta_{t+1}^{s+1}) - c_{t+1}\norm[]{\vtheta_{t+1}^{s+1}-\tilde{\vtheta}^s}^2 - J(\vtheta_{t}^{s+1}) - c_{t}\norm[]{\vtheta_{t}^{s+1}-\tilde{\vtheta}^s}^2\right)}}{\psi} \nonumber\\
 &\qquad+ \frac{m\zeta}{N} + \frac{m\xi}{B} \nonumber\\
 %
 &= \frac{\Ets[m]{J(\vtheta_{m}^{s+1}) - c_{m}\norm[]{\vtheta_{m}^{s+1}-\tilde{\vtheta}^s}^2 - J(\vtheta_{0}^{s+1}) - c_{0}\norm[]{\vtheta_{0}^{s+1}-\tilde{\vtheta}^s}^2}}{\psi} \nonumber\\
 &\qquad+ \frac{m\zeta}{N} + \frac{m\xi}{B} \label{eq:21}\\
 %
 &= \frac{\Ets[m]{J(\tilde{\vtheta}^{s+1}) - J(\tilde{\vtheta}^{s})}}{\psi} + \frac{m\zeta}{N} + \frac{m\xi}{B}, \label{eq:22}
 \end{align}
where (\ref{eq:21}) is obtained by telescoping the sum and (\ref{eq:22}) from the fact that $c_m=0$ and $\vtheta_0^{s+1}=\tilde{\vtheta}^s$.
Next, summing over epochs:
\begin{align}
\sum_{s=0}^{S-1}\sum_{t=0}^{m-1}\Ets{\norm[]{\gradJ{(\vtheta_t^{s+1})}^2}}&\leq
\frac{\sum_{s=0}^{S-1}\Ets[m]{J(\tilde{\vtheta}^{s+1}) - J(\tilde{\vtheta}^{s})}}{\psi} + \frac{T\zeta}{N} + \frac{T\xi}{B} \nonumber\\
%
&\leq
\frac{\EVV[]{J(\tilde{\vtheta}^{S}) - J(\tilde{\vtheta}^{0})}}{\psi} + \frac{T\zeta}{N} + \frac{T\xi}{B}
 \label{eq:23}\\
%
&\leq
\frac{J(\vtheta^*) - J(\vtheta^0)}{\psi} + \frac{T\zeta}{N} + \frac{T\xi}{B}, \label{eq:24}
\end{align}
where the expectation in (\ref{eq:23}) is \wrt all the trajectories sampled in a run of Algorithm \ref{alg:svrpg} and (\ref{eq:24}) is from the definition of $\vtheta^*$.
 
Finally, we consider the expectation \wrt all sources of randomness, including the uniform sampling of the output parameter:
\begin{align}
\EVV[]{\norm[]{\gradJ{(\vtheta_t^{s+1})}}^2} 
&=\frac{1}{T}\sum_{s=0}^{S-1}\sum_{t=0}^{m-1}\Ets{\norm[]{\gradJ{(\vtheta_t^{s+1})}}^2} \nonumber\\
&\leq
\frac{J(\vtheta^*) - J(\vtheta^0)}{\psi T} + \frac{\zeta}{N} + \frac{\xi}{B} \nonumber.
\end{align}
\end{proof}


\section{Applicability to Gaussian Policies}\label{app:gauss}
We provide more details on the applicability of Theorem \ref{theo:convergence} on the case of Gaussian policies. We start from the case of one-dimensional bounded action space $\mathcal{A}\subset\mathbb{R}$, linear mean $\mu(s) = \vtheta^T\vphi(s)$ and fixed standard deviation $\sigma$:
\[
	\pi_{\vtheta}(a\vert s) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left\{
		-\frac{(\vtheta^T\vphi(s) - a)^2}{2\sigma^2}\right\},
\]
where $\vphi(s)\leq M_{\phi}$ is a bounded feature vector, and we see under which conditions the three assumptions of Section \ref{sec:conv} hold.

\boundedscore*
For the Gaussian policy defined above, it's easy to show that:
\begin{align*}
	&\nabla_{\theta_i}\log\pi_{\vtheta}(\tau) =  \phi_i(s)\frac{a-\vtheta^T\phi(s)}{\sigma^2},\\
	&\frac{\partial^2}{\partial\theta_i\partial\theta_j}\log\pi_{\vtheta}(\tau) = \frac{\phi_i(s)\phi_j(s)}{\sigma^2}.
\end{align*}
Hence, Assumption \ref{ass:bounded_score} is automatically satisfied \footnote{This relies on the fact that $\vtheta^T\phi(s)$ lies in bounded $\Aspace$. In practice, this is usually enforced by clipping the action selected by $\pi_{\vtheta}$. A more rigorous way would be to employ the truncated Gaussian distribution.} by taking $\GRADLOG = \frac{M_{\phi}|\Aspace|}{\sigma^2}$ and $\HESSLOG = \frac{M_{\phi}^2}{\sigma^2}$.
\par
\varreinforce*
As mentioned, \cite{pirotta2013adaptive} provides a bound on the variance of the REINFORCE estimator, adapted from \cite{zhao2011analysis}, which does not depend on $\vtheta$:
\begin{align*}
\Var\left[\gradApp{\theta_i}{N}\right] \leq \frac{R^2M_{\phi}^2H(1-\gamma^H)^2}{N\sigma^2(1-\gamma)^2}.
\end{align*}
The same authors provide a similar bound for G(PO)MDP.

\varweights*
It is noted in \cite{cortes2010learning} that, for any two Gaussian distributions $\mathcal{N}(\mu_1,\sigma_1)$ and $\mathcal{N}(\mu_2,\sigma_2)$, the variance of the importance weights from the latter to the former is bounded whenever $\sigma_2 > \frac{\sqrt{2}}{2}\sigma_1$. This is automatically satisfied by our fixed-variance Gaussian policies, since $\sigma_2=\sigma_1=\sigma$.

We now briefly examine some generalizations of the simple Gaussian policy defined above that can be found in applications:

\paragraph{Multi-dimensional actions}
When actions are multi-dimensional, factored Gaussian policies are typically employed, so the results extends trivially from the one-dimensional case. Actual multi-variate Gaussian distributions would require more calculations, but we do not expect substantially different results.

\paragraph{Non-linear mean}
In complex continuous tasks, $\mu(s)$ often represents a deep neural network, or multi-layer perceptron, where $\vtheta$ are the weights of the network. The analysis of first and second order log-derivatives in such a scenario is beyond the scope of this paper.

\paragraph{Adaptive variance}
It is a common practice to learn also the variance of the policy in order to adapt the degree of exploration. The variance (or diagonal covariance matrix in the multi-dimensional case) can be learned as a separate parameter or be state-dependent like the mean. In any case, adaptive variance must be carefully employed since it can clearly undermine all the three assumptions of Theorem \ref{theo:convergence}.

\section{Experimental Details}\label{app:exp}
Setting specifications for all the used environments:
\begin{enumerate}
	\item \emph{Cart-Pole Balancing} : 4-dimensional state space: cart position x, pole angle $\theta$, cart velocity $\dot{x}$ and pole velocity $\dot{\theta}$; 1-dimensional action space: the horizontal force applied to the cart body. Reward function  is defined as $r(s, a) := 10 - (1 - cos(\theta)) - 10^{-5}\norm[] a^2$. The episodes terminates when $|x|>2.4$ or $|\theta|>0.2$ or the number of time steps T is greater than 100.
	\item \emph{Mujoco Swimmer}: 12-dimensional state space: 3 links velocities ($v_x$ and $v_y$ of center of masses) and 2 actuated joints angles. 2-dimensional action space: the two momentums applied on actuated joints.  The reward function is defined as $r(s, a) := v_x - 10^{-4}\norm[2] a^2$. The episodes terminates when the number of time steps T is greater than 500.
	\item \emph{Mujoco Half Cheetah}: 20-dimensional state space: 9 links and 6 actuated joints angles. 6-dimensional action space: the 6 momentums applied on actuated joints.  The reward function is defined as $r(s, a) := v_x - 0.05\norm[2] a^2$. The episodes terminates when the number of time steps T is greater than 500.
\end{enumerate}

All the experiments' parameters are reported in the following table:

\centering
\begin{tabular}{| l | c  c  c |}
	\hline	
	& Cart-Pole & Swimmer & Half Cheetah \\
	\hline
	State space dimension  & 4 & 8 & 20 \\
	Control space dimension & 1 & 2 & 6 \\
	NN Hidden layers & 8 & 32x32 & 32x32 \\
	Learning rate & $10^{-2}$ & $10^{-3}$ & $10^{-3}$ \\
	Snapshot trajectories & 100 & 100 & 100 \\
	Sub-iterations trajectories/GPOMDP batch & 10 & 10 & 10 \\
	Max sub-iterations & 50 & 20 & 20 \\
	Trajectories lengths& 500 & 500 & 500 \\
	Discount $(\gamma)$& 0.99 & 0.995 & 0.99 \\
	Total trajectories& 10000 & 10000 & 10000 \\
	\hline  
\end{tabular}


\end{document}
