\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

%%%USEFUL PACKAGES%%%
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{amsmath,amssymb}
\usepackage{mathtools}
\usepackage{pifont}
%%Theorems
\usepackage{amsthm}
\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}
\newtheorem{case}{Case}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{conj}{Conjecture}
\newtheorem{example}{Example}
%Restatable
\usepackage{thmtools, thm-restate}
\declaretheorem[numberwithin=section]{thm}
\declaretheorem[sibling=thm]{lemma}
\declaretheorem[sibling=thm]{corollary}
\declaretheorem[sibling=thm]{assumption}
\declaretheorem[sibling=thm]{theorem}
%e.g. ...
\usepackage{xspace}
\DeclareRobustCommand{\eg}{e.g.,\@\xspace}
\DeclareRobustCommand{\ie}{i.e.,\@\xspace}
\DeclareRobustCommand{\wrt}{w.r.t.\@\xspace}
%%%%%%

%%%CUSTOM COMMANDS%%%
%Math
\newcommand{\realspace}{\mathbb R}      % realspace
\newcommand{\transpose}[1]{{#1}^\texttt{T}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\EV}{\mathbb{E}}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator*{\Cov}{\mathbb{C}ov}
\newcommand{\EVV}[2][\ppvect \in \ppspace]{\EV_{#1}\left[{#2}\right]}
\newcommand{\norm}[2][\infty]{\left\|#2\right\|_{#1}}
\newcommand{\Dij}[2]{\frac{\partial^{2}{#1}}{\partial{#2}_i\partial{#2}_j}}
\newcommand{\de}{\,\mathrm{d}}
\newcommand{\dotprod}[2]{\left\langle#1,#2\right\rangle}
%RL
\newcommand{\vtheta}{\boldsymbol{\theta}}
\newcommand{\Aspace}{\mathcal{A}}
\newcommand{\Sspace}{\mathcal{S}}
\newcommand{\Tspace}{\mathcal{T}}
\newcommand{\Transition}{\mathcal{P}}
\newcommand{\Reward}{\mathcal{R}}
\newcommand{\stationary}{d_{\rho}^{\pi_{\vtheta}}(s)}
\newcommand{\policy}{\pi_{\vtheta}(a \mid s)}
\newcommand{\pol}{\pi_{\vtheta}}
\newcommand{\trajdistr}{\pi_{\vtheta}(\tau)}
\newcommand{\score}[2]{\nabla\log\pi_{#1}(#2)}
\newcommand{\Qfun}{Q^{\pi_{\vtheta}}(s,a)}
\newcommand{\Vfun}{V^{\pi_{\vtheta}}(s)}
\newcommand{\vTheta}{\boldsymbol{\Theta}}
\newcommand{\vphi}{\boldsymbol{\phi}}
\newcommand{\gradJ}[1]{\nabla J(#1)}
\newcommand{\gradApp}[2]{\hat{\nabla}_{#2}J(#1)}
\newcommand{\eqdef}{\mathrel{\mathop:}=}
\newcommand{\Dataset}{\mathcal{D}}
%Specific
\newcommand{\Ets}[2][t]{\mathbb{E}_{#1\mid s}\left[#2\right]}
\newcommand{\Covts}[3][t]{{\mathbb{C}\text{ov}}_{#1\mid s}\left(#2,#3\right)}
\newcommand{\Varts}[2][t]{{\mathbb{V}\text{ar}}_{#1\mid s}\left[#2\right]}
\newcommand{\gradBlack}[1]{\blacktriangledown J(#1)}
\newcommand{\gradIdeal}[1]{\overline{\blacktriangledown} J(#1)}
%%%%%%

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{SVRPG}

\begin{document}

\twocolumn[
\icmltitle{Stochastic Variance Reduced Policy Gradient}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2018
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Damiano Binaghi}{equal,polimi}
\icmlauthor{Giuseppe Canonaco}{equal,polimi}
\icmlauthor{Matteo Papini}{polimi}
\icmlauthor{Matteo Pirotta}{inria}
\icmlauthor{Marcello Restelli}{polimi}
\end{icmlauthorlist}

\icmlaffiliation{polimi}{Politecnico di Milano, Milano, Italy}
\icmlaffiliation{inria}{Inria, Lille, France}

\icmlcorrespondingauthor{Matteo Papini}{matteo.papini@polimi.it}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

\end{abstract}

\section{Introduction}

\section{Preliminaries}
In this section we provide essential background on policy gradient methods for Reinforcement Learning and stochastic variance-reduced gradient methods for finite sum optimization. In the next section we will show how these methods can be combined to solve Reinforcement Learning tasks.

\subsection{Policy Gradient}
A Reinforcement Learning task \cite{sutton2011reinforcement} can be modeled with a discrete-time continuous Markov Decision Process (MDP) $\langle\Sspace,\Aspace,\Transition,\Reward,\gamma,\rho\rangle$, where $\Sspace$ is a continuous state space; $\Aspace$ is a continuous action space; $\Transition$ is a Markovian transition model, where $\Transition(s'\mid s,a)$ defines the transition density from state $s$ to $s'$ under action $a$; $\Reward$ is the reward function, where $\Reward(s,a) \in [-R,R]$ is the expected reward for state-action pair $(s,a)$ and $R$ is the maximum absolute-value reward; $\gamma\in[0,1)$ is the discount factor; and $\rho$ is the initial state distribution.
The agent's behavior is modeled as a policy $\pi$, where $\pi(a\mid s)$ is the density distribution over $\Aspace$ of the action to take in state $s$.
We consider episodic tasks, \ie, tasks composed of episodes of length $H$, also called time horizon. In this setting, a trajectory $\tau$ is the sequence of states and actions $(s_0,a_0,s_1,a_1,\dots,s_{H-1},a_{H-1})$ observed by following some stationary policy in a given episode, where $s_0$ is always sampled from the initial state distribution $\rho$. With abuse of notation, we denote with $\pi(\tau)$ the density distribution induced by policy $\pi$ on the set $\Tspace$ of all possible trajectories, and with $\Reward(\tau)$ the total discounted reward provided by trajectory $\tau$:
\begin{align*}
\Reward(\tau) = \sum_{t=0}^{H-1}\gamma^t\Reward(s_t,a_t).
\end{align*}
Performance $J$ allows to rank policies in terms of expected reward:
\begin{align*}
	J(\pi) = \int_{\Tspace}\pi(\tau)\Reward(\tau)\de \tau.
\end{align*}
Solving an MDP means finding a policy $\pi^*$ maximizing $J$.
\par
Policy gradient methods restrict the search for the performance-maximizing policy over a class of parametrized policies $\Pi_{\vtheta}=\{\pol: \vtheta \in \realspace^m\}$, with the only constraint that $\pol$ is differentiable in the $m$-dimensional parameter vector $\vtheta$. We denote the performance of a parametric policy with $J(\vtheta)$ for brevity, while in some occasions we denote $\pol(\tau)$ with $\pi(\tau\mid\vtheta)$ for the sake of readability. The search for a locally optimal policy is performed by means of gradient ascent, where the gradient of $J(\vtheta)$ \wrt the policy parameters is \cite{sutton2000policy}:
\begin{align*}
\gradJ{\vtheta} = \int_{\Tspace}\pol(\tau)\score{}{\tau}\Reward(\tau)\de \tau.
\end{align*}
We consider the on-line learning problem, where trajectories must be sampled directly by interacting with the environment. In this setting, stochastic gradient descent is typically employed: after sampling a trajectory $\tau$ or a batch of $N$ trajectories $\Dataset_{N}$,
policy parameters are updated as $\vtheta\gets\vtheta + \alpha\gradApp{\vtheta}{N}$, where $\alpha$ is a step size and $\gradApp{\vtheta}{N}$ is an estimate of the true gradient. The REINFORCE gradient estimator \cite{williams1992simple} provides a simple, unbiased way of estimating the gradient:
\begin{align*}
\gradApp{\vtheta}{N} = \frac{1}{N}\sum_{n=1}^{N}\left(\sum_{h=0}^{H-1}\score{}{a_h^n\mid s_h^n}\right)\left(\sum_{h=0}^{H-1}\gamma^h r_h^n - b\right),
\end{align*}
where subscripts denote the time step, superscripts denote the trajectory, $r_h$ is the reward actually collected at time $h$ and b can be any baseline, provided it is constant \wrt to actions.
The G(PO)MDP gradient estimator \cite{baxter2001infinite} is a refinement of REINFORCE which is subject to less variance while preserving the unbiasedness:
\begin{align*}
\gradApp{\vtheta}{N} = \frac{1}{N}\sum_{n=1}^{N}\sum_{h=0}^{H-1}\left(\sum_{k=0}^{h}\score{\vtheta}{a_h^n\mid s_h^n}\right)\left(\gamma^h r_h^n - b\right).
\end{align*}

\subsection{Stochastic Variance-Reduced Gradient}
Finite sum optimization is the problem of maximizing an objective function $f(\vtheta)$ which can be decomposed into the sum or average of a finite number of functions $g_i(\vtheta)$:
\begin{align*}
\max_{\vtheta} f(\vtheta) = \frac{1}{N}\sum_{i=1}^{N}g_i(\vtheta).
\end{align*}
This kind of optimization is very common in machine learning, where each $g_i$ may correspond to a data sample $x_i$ from a dataset $\mathcal{D}_N$ of size $N$. In this case, we adopt the following, more meaningful notation:
\begin{align*}
\max_{\vtheta} f(\vtheta) = \frac{1}{N}\sum_{i=1}^{N}g(x_i \mid \vtheta).
\end{align*}
A common requirement is that $g$ must be smooth and convex in $\vtheta$. Under this hypothesis, full gradient ascent \cite{cauchy1847methode} with a constant step size achieves an IFO complexity \cite{agarwal2014lower} of $O(\nicefrac{N}{\epsilon})$, \ie it requires $k = O(\nicefrac{N}{\epsilon})$ gradient computations to achieve $\norm[]{\nabla_{\vtheta}f(\vtheta_k)}^2\leq\epsilon$. This can be too expensive for large values of $N$. Stochastic gradient ascent \cite{robbins1951stochastic} \cite{bottou2004large} overcomes this problem by sampling a single $x_i$ per iteration, but a vanishing step size is required to control the variance introduced by sampling. The resulting IFO complexity is $O(\nicefrac{1}{\epsilon^2})$ in expectation, which does not depend on $N$, but has a worse dependency on $\epsilon$. 
Starting from SAG \cite{roux2012stochastic}, a series of variations to stochastic gradient ascent have been proposed to achieve a better trade-off between convergence speed and cost per iteration. This family of algorithms includes SAG, SVRG \cite{johnson2013accelerating}, SAGA \cite{defazio2014saga} and Finito \cite{defazio2014finito}, the common intuition being the reuse of previous gradient computations to reduce the variance of the current one. In particular, Stochastic Variance Reduced Gradient (SVRG) is often preferred to other similar methods for its limited storage requirements, which can become a problem when neural networks with several parameters are employed.  

The idea of SVRG is to alternate full and stochastic gradient updates. Each $m = O(N)$ iterations, a snapshot $\tilde{\vtheta}$ of the current parameter is saved together with its full gradient $\nabla f(\tilde{\vtheta})$.
Otherwise, the parameter is updated with a corrected stochastic gradient estimate of the form:
\begin{align*}
\blacktriangledown f(\vtheta) = 
	\nabla f(\tilde{\vtheta}) + 
	\nabla g(x_i\mid\vtheta) -
	\nabla g(x_i \mid \tilde{\vtheta}),
\end{align*} 
where $x_i$ is sampled uniformly at random from $\mathcal{D}_N$. The corrected gradient $\blacktriangledown f(\vtheta)$ is an unbiased estimate of $\nabla f(\vtheta)$, and it is able to control the variance introduced by sampling even with a fixed step size, without resorting to plain full gradient. The resulting IFO complexity is $O(N+\nicefrac{\sqrt{N}}{\epsilon})$ \cite{reddi2016stochastic}.

More recently, some extensions of variance reduction algorithms to the non-convex objectives have been proposed \cite{reddi2016stochastic} \cite{allen2016variance} \cite{reddi2016stochastic} \cite{reddi2016fast}. In this scenario, $f$ is typically required to be L-smooth, \ie $\norm[]{\nabla f(\vtheta') - \nabla f(\vtheta)} \leq L\norm[]{\vtheta'-\vtheta}$ for each $\vtheta,\vtheta'\in\realspace^n$ and for some Lipschitz constant $L$. Under this hypothesis, the IFO complexity of full gradient  and of stochastic gradient ascent are the same as in the convex case \cite{nesterov2013introductory} \cite{ghadimi2013stochastic}. Also in this case, SVRG yields an advantageous trade-off, with an IFO complexity of $O(N + \nicefrac{N^{\nicefrac{2}{3}}}{N})$ \cite{reddi2016stochastic}. The only additional requirement is to select $\vtheta^*$ uniformly at random among all the $\vtheta_k$ instead of simply setting it to the final value. Pseudocode of SVRG for both the convex and the non-convex case is provided in Algorithm \ref{alg:svrg}. 


\begin{algorithm}[tb]
	\caption{SVRG}
	\label{alg:svrg}
	\begin{algorithmic}
		\STATE {\bfseries Input:} number of epochs $S$, epoch size $m$, step size $\alpha$, initial parameter $\vtheta_{m-1}^0$
		\FOR{$s=0$ {\bfseries to} $S-1$}
		\STATE $\vtheta_0^{s+1} = \tilde{\vtheta}^s = \vtheta_{m-1}^s$
		\FOR{$t=0$ {\bfseries to} $m-1$}
		\STATE Pick $i$ uniformly at random from $[1,N]$
		\STATE \begin{align*}
			\blacktriangledown f(\vtheta_t^{s+1}) = 
			&\nabla f(\tilde{\vtheta}^s) \\ + 
			&\nabla g(x_i\mid\vtheta_t^{s+1}) -
			\nabla g(x_i \mid \tilde{\vtheta}^{s+1})
		\end{align*}
		\STATE $\vtheta_{t+1}^{s+1} = \vtheta_t^{s+1} + \alpha\blacktriangledown f(\vtheta_t^{s+1})$
		\ENDFOR
		\ENDFOR
		\STATE {\bfseries Convex case: return} $\vtheta_{m-1}^S$
		\STATE {\bfseries Non-Convex case: return} $\vtheta_t^s$ with $(s,t)$ picked uniformly at random from $[0,S)\times[0,m)$
	\end{algorithmic}
\end{algorithm}


\section{Algorithm}
In on-line policy gradient problems, stochastic gradient ascent (SGA) is not really a choice, but is dictated by the necessity of interacting with an unknown environment. We would like to apply SVRG to the policy gradient framework in order to limit the variance introduced by sampling trajectories, which would ultimately lead to faster convergence. In this scenario, the function $f(\vtheta)$ to optimize is $J(\vtheta)$ and the (implicit) dataset $\mathcal{D}$ is the set of all possible trajectories $\mathcal{T}$. Compared to the typical finite sum optimization scenario, we have the following additional challenges:
\begin{itemize}
	\item The objective $J(\vtheta)$ is typically non-convex;
	\item The dataset $\mathcal{T}$ can be infinite;
	\item The value of the parameter $\vtheta$ influences the sampling of trajectories;
\end{itemize}
To deal with non-convexity, we require $J(\vtheta)$ to be L-smooth, which is a reasonable assumption for common policy classes such as truncated Gaussian and Softmax [appendix?]. Because of the infinite dataset, we can only rely on an estimate of the full gradient. Finally, we employ importance weighting \cite{rubinstein1981simulation} \cite{precup2000eligibility} to guarantee proper sampling of trajectories. We focus on the REINFORCE gradient estimator for simplicity, but the extension to G(PO)MDP is straightforward. The proposed SVRG-like gradient estimate, which we call Stochastic Variance Reduced Policy Gradient (SVRPG), is the following:
\begin{align*}
	\blacktriangledown J(\vtheta) &= \frac{1}{N}\sum_{j=0}^{N-1}\nabla\log\pi(\tau_j \mid \tilde{\vtheta})\Reward(\tau_j) \\
		&+ \nabla\log\pi(\tau_i \mid \vtheta)\Reward(\tau_i) 
		- \omega(\tau_i)\nabla\log\pi(\tau_i \mid \tilde{\vtheta})\Reward(\tau_i),
\end{align*}
where trajectories $\tau_j$ for $j=1,\dots,N$ are sampled from a snapshot policy $\pi_{\tilde{\vtheta}}$, $\tau_i$ is sampled from the current policy $\pol$, and $\omega(\tau) = \frac{\pi(\tau\mid\tilde{\vtheta})}{\pi(\tau\mid\vtheta)}$ is an importance weight from $\pol$ to the snapshot policy $\pi_{\tilde{\vtheta}}$. We immediately have the following (proof in Appendix \ref{app:proofs}):

\begin{restatable}{lemma}{unbias}\label{lemma:unbias}
The SVRPG gradient estimator is unbiased:
\[
\mathop{\mathbb{E}}
\left[\blacktriangledown J(\vtheta)\right] = \gradJ{\vtheta}.
\]
\end{restatable}

[May also add Lemma about convergence of the variance to zero, since it provides an intuitive justification of some design choices]
The resulting SVRPG policy optimization method is detailed in Algorithm \ref{alg:svrpg}.
To obtain a method that can be used in practice, a number of additional details must be specified, namely the choice of epoch size $m$, the number $N$ of trajectories used to estimate the full gradient and the step size $\alpha$. Adaptive selection of these meta-parameters is also possible. However, the next section will focus on providing converge guarantees for the plain SVRPG algorithm.

\begin{algorithm}[tb]
	\caption{SVRPG}
	\label{alg:svrpg}
	\begin{algorithmic}
		\STATE {\bfseries Input:} number of epochs $S$, epoch size $m$, step size $\alpha$, initial parameter $\vtheta_{m-1}^0$, batch size $N$
		\FOR{$s=0$ {\bfseries to} $S-1$}
		\STATE Sample N trajectories $\tau_j$ from $\pi(\cdot\mid\tilde{\vtheta}^{s})$
		\STATE $\gradApp{\tilde{\vtheta}^{s}}{N} = \frac{1}{N}\sum_{j=0}^{N-1}\score{}{\tau_j\mid\tilde{\vtheta}^{s}}\Reward(\tau_j)$
		\STATE $\vtheta_0^{s+1} = \tilde{\vtheta}^s = \vtheta_{m-1}^s$
		\FOR{$t=0$ {\bfseries to} $m-1$}
		\STATE Sample trajectory $\tau_i$ from 				$\pi(\cdot\mid\vtheta_t^{s+1})$
		\STATE 
		\begin{align*}
		\blacktriangledown J(\vtheta_t^{s+1}) = 
		&\gradApp{\tilde{\vtheta}^s}{N}
		+ \score{}{\tau_i\mid\vtheta_t^{s+1}}\Reward(\tau_i) \\
		& - \omega(\tau_i)\score{}{\tau_i \mid \tilde{\vtheta}^{s}}\Reward(\tau_i)
		\end{align*}
		\STATE $\vtheta_{t+1}^{s+1} = \vtheta_t^{s+1} + \alpha\blacktriangledown J(\vtheta_t^{s+1})$
		\ENDFOR
		\ENDFOR
		\STATE {\bfseries return} $\vtheta_t^s$ with $(s,t)$ picked uniformly at random from $[0,S)\times[0,m)$
	\end{algorithmic}
\end{algorithm}

\section{Convergence Guarantees}

\begin{restatable}[Convergence of the SVRPG algorithm]{theorem}{convergence}\label{theo:convergence}
Provided $J(\vtheta)$ is L-smooth with Lipschitz constant $L$ and step sizes are sufficiently small, the parameter vector $\vtheta_A$ returned by Algorithm \ref{alg:svrpg} after $T=m\times S$ iterations has, for some constants $\psi>0, D>0$ and $c_{t}>0$ for $t=1,\dots,m$, the following property:
\[
	\EVV[]
	{\norm[]{\nabla J(\vtheta_A)}^2} \leq
		\frac{J(\vtheta^*)-J(\vtheta_0)}{\psi T} +
		\frac{Z}{N},
\]
where
\begin{align*}
	Z &= D\max_{s}\EVV[]{\norm[]{\gradApp{\tilde{\vtheta^s}}{1}}^2} \\
	&+ \max_{s,t}\left|\Tr\Cov\left[
		\gradApp{\tilde{\vtheta^s}}{1},
		c_{t+1}\vtheta_t^{s+1} + \alpha_t\nabla J(\vtheta_t^{s+1})\right]\right|
\end{align*}
\end{restatable}

\section{Practical Variants}

\section{Related Work}

\section{Experiments}
We design our experiments to investigate if:
\begin{enumerate}
\item Algorithm reaches high value of the reward function faster then GPOMDP.
\item Averagely SVRPG maintains the reward function more stable then GPOMDP.
\end{enumerate}

In order to analyze these topics, we evaluate the behavior of rewards function mediated by 10 runs for each experiment.

We decide to use the following models of Rllab: \emph{Cart-Pole Balancing} ,\emph{Mujoco Swimmer},\emph{Mujoco Half Cheetah}.


In all the experiments we face with GPOMDP standard algorithm. GPOMDP uses a number of trajectories per batch that is equal to the number of trajectories used in SVRPG mini-batch size.
The neural network and the learning rate are the same for GPOMDP and SVRPG.
In SVRPG we implemented two different Adam parameters: the first one for Snapshot and the second one for sub-iterations.
The stopping condition depends on the maximum number of sub-iterations and on the step size of sub-iteration: in particular we stop if the ratio of step size to number of trajectories used results smaller in sub-iteration than in snapshot.
We fixed the maximum number of trajectories that both the algorithms can use. 

The neural networks is composed by one layer with 8 neurons for Cart-Pole environment and by two layers with 32x32 neurons for Swimmer and for Half Cheetah environments. In each experiment, the length of episodes is 500 steps.

\section{Discussion}

\bibliography{svrpg}
\bibliographystyle{icml2018}

\newpage
\mbox{}
\newpage
\onecolumn
\appendix

\section{Proofs}\label{app:proofs}

\subsection*{Definitions}
We give some additional definitions which will be useful in the proofs.
\begin{definition}
For a random variable $X$:
\begin{align*}
	&\mathbb{E}_{t\mid s}\left[X\right] \coloneqq 
		\mathop{\mathbb{E}}_{\substack{\tau_j\sim\pi(\cdot\mid\tilde{\vtheta}^s) \\ \tau_i\sim\pi(\cdot\mid\vtheta^{s+1}_i)\text{ for $i=0,\dots,t-1$}}}{\left[X \mid \tilde{\vtheta^s}\right]} \\
	&\coloneqq \EVV[\tau_j\sim\pi(\cdot\mid\tilde{\vtheta}^s)]{
			\EVV[\tau_0\sim\pi(\cdot\mid\vtheta_0^{s+1})]
				{\dots
					\EVV[\tau_t\sim\pi(\cdot\mid\vtheta_t^{s+1})]
						{X\mid\vtheta_t^{s+1}}
				 \dots
			\mid\vtheta_0^{s+1}}
		\mid\tilde{\vtheta}^s},
\end{align*}
where the sequence $\tilde{\vtheta}^s,\vtheta_0^{s+1},\dots,\vtheta_t^{s+1}$ is defined in Algorithm \ref{alg:svrpg}. 
\end{definition}
Intuitively, the $\Ets{\cdot}$ operator computes the expected value with respect to the sampling of trajectories from the snapshot $\tilde{\vtheta}^s$ up to the $t$-th iteration included. Note that the order in which expected values are taken is important since each $\vtheta_{t}^{s+1}$ is function of previously sampled trajectories and is used to sample new ones.

\begin{definition}
The full gradient estimation error is:
\[
	e_s \coloneqq \gradApp{\tilde{\vtheta}^s}{N} - \gradJ{\tilde{\vtheta}^s} 
\]
\end{definition}

\begin{definition}\label{def:ideal}
The ideal SVRPG gradient estimate is:
\begin{align*}
	\overline{\blacktriangledown}J(\vtheta_t^{s+1}) &\coloneqq 
	\gradJ{\tilde{\vtheta}^s}
	+ \nabla\log\pi(\tau_i \mid \vtheta_t^{s+1})\Reward(\tau_i) 
	- \omega(\tau_i)\nabla\log\pi(\tau_i \mid \tilde{\vtheta}^s)\Reward(\tau_i) \\
	&= \gradBlack{\vtheta_t^{s+1}} - \gradApp{\tilde{\vtheta}^s}{N} + \gradJ{\tilde{\vtheta}^s} \\
	&= \gradBlack{\vtheta_t^{s+1}} - e_s
\end{align*}
\end{definition}


\subsection*{Lemmas}
Before addressing the main convergence theorem, we prove some useful lemmas.

\unbias*
\begin{proof}
...
\end{proof}

%Lemma 2
\begin{restatable}[]{lemma}{auxtwo}\label{lemma:aux2}
The expected squared norm of the SVRPG gradient can be bounded as follows:
\[
\Ets{\gradBlack{\vtheta_t^{s+1}}} \leq
\Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
+L^2\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}}
\nonumber 
+dG^2R^2\left(M_2-1\right)
\]
\end{restatable}
\begin{proof}
	
	\begin{align}
	\Ets{\gradBlack{\vtheta_t^{s+1}}} 
	&= \Ets{\norm[]{\gradApp{\tilde{\vtheta}^s}{N}
			+ \score{\vtheta_t^{s+1}}{\tau_i} -
			\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}}^2} \nonumber\\
	&= \Ets{\norm[]{\gradApp{\tilde{\vtheta}^s}{N}
			+ \score{\vtheta_t^{s+1}}{\tau_i} -
			\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}-\gradJ{\vtheta_t^{s+1}} + \gradJ{\tilde{\vtheta}^s}
			+\gradJ{\vtheta_t^{s+1}} - \gradJ{\tilde{\vtheta}^s}}^2} \nonumber\\
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2}
	+\Ets[]{\norm[]{\gradApp{\tilde{\vtheta}^s}{N} - \Ets[]{\gradApp{\tilde{\vtheta}^s}{N}}}^2} \nonumber\\
	&\qquad+ 
	\Ets{\norm[]{\score{\vtheta_t^{s+1}}{\tau_i} -
			\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}- \Ets{\score{\vtheta_t^{s+1}}{\tau_i} -
				\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}}}^2} \nonumber\\
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}}
	\\
	&\qquad+ 
	\Ets{\norm[]{\score{\vtheta_t^{s+1}}{\tau_i} -
			\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}- \Ets{\score{\vtheta_t^{s+1}}{\tau_i} -
				\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}}}^2} \nonumber\\
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} \nonumber\\
	&\qquad+\Ets{\norm[]{\score{\vtheta_t^{s+1}}{\tau_i} -
			\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}}^2} \\
	&= \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}}
	\nonumber\\
	&\qquad+\Ets{\norm[]{\score{\vtheta_t^{s+1}}{\tau_i}
			-\score{\tilde{\vtheta}^s}{\tau_i}
			+\score{\tilde{\vtheta}^s}{\tau_i} 
			-\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}}^2} \nonumber\\
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}}
	\nonumber\\
	&\qquad+\Ets{\norm[]{\score{\vtheta_t^{s+1}}{\tau_i}
			-\score{\tilde{\vtheta}^s}{\tau_i}}^2} \nonumber
	+\Ets{\norm[]{\score{\tilde{\vtheta}^s}{\tau_i} 
			-\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}}^2} \nonumber\\
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}}
	\nonumber\\
	&\qquad+L^2\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
	+\Ets{\norm[]{\score{\tilde{\vtheta}^s}{\tau_i} 
			-\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}}^2} \\
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}}
	\nonumber\\
	&\qquad+L^2\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
	+dG^2R^2\Ets{(\omega(\tau_i)-1)^2} \\
	&= \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}}
	\nonumber\\
	&\qquad+L^2\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
	+dG^2R^2\left(\Ets{\omega^2(\tau_i)}-1\right) \nonumber\\
	&= \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}}
	\nonumber\\
	&\qquad+L^2\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
	+dG^2R^2\left(M_2-1\right),
\end{align}
where (1) is from the definition of $\gradApp{\vtheta}{N}$, (2) is from $\EVV[]{\norm[]{X-\EVV[]{X}}^2}\leq\EVV[]{\norm[]{X}^2}$ for any stochastic variable $X$, (3) is from L-smoothness, 
(4) is from [todo], and (5) is from [todo].
\end{proof}

%Lemma 0
\begin{restatable}[]{lemma}{auxzero}\label{lemma:aux0}
The expected scalar product between $\gradBlack{\vtheta_t^{s+1}}$ and any function $f(\vtheta_t^{s+1})$ which is deterministic for a fixed $\vtheta_t^{s+1}$ can be bounded as follows:
\begin{align*}
\Ets[t]{\dotprod{\gradBlack{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} &\geq
\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} \\
&\qquad-
\frac{1}{2N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} -\frac{1}{2}\Ets[t-1]{\norm[]{f(\vtheta_t^{s+1})}^2}
\end{align*}
\end{restatable}
\begin{proof}
\begin{align}
	\Ets{\dotprod{\gradBlack{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}}
	&=
	\Ets{\dotprod{\gradIdeal{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} +
	\Ets[t-1]{\dotprod{e_s}{f(\vtheta_t^{s+1})}} \\
	&=
	\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} +
	\Ets[t-1]{\dotprod{e_s}{f(\vtheta_t^{s+1})}} \\
	&=
	\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} \nonumber\\
	&\qquad+
	\dotprod{\Ets{e_s}}{\Ets{f(\vtheta_t^{s+1})}}
	+\Tr\Covts[t-1]{\gradApp{\tilde{\vtheta}^s}{N}}{f(\vtheta_t^{s+1})}  \nonumber\\
	&= 
	\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} \nonumber\\
	&\qquad+
	\Tr\Covts[t-1]{\gradApp{\tilde{\vtheta}^s}{N}}{f(\vtheta_t^{s+1})} \\
	&\geq  
	\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} \nonumber\\
	&\qquad-
	\Tr\left|\Covts[t-1]{\gradApp{\tilde{\vtheta}^s}{N}}{f(\vtheta_t^{s+1})}\right| \nonumber \\
	%
	&\geq
	\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} \nonumber\\
	&\qquad-\Tr\left(\sqrt{\Varts[]{\gradApp{\tilde{\vtheta}^s}{N}}}\circ\sqrt{\Varts[t-1]{f(\vtheta_t^{s+1})}}\right) \nonumber\\
	%
	&\geq	
	\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} \nonumber\\
	&\qquad-
	\frac{\alpha_t}{2}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{N}} -\frac{\alpha_t}{2}\Tr\Varts[t-1]{f(\vtheta_t^{s+1})}\\
	%
	&=
	\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} \nonumber\\
	&\qquad-
	\frac{1}{2N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} -\frac{1}{2}\Tr\Varts[t-1]{f(\vtheta_t^{s+1})} \\
	%
	&\geq
	\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} \nonumber\\
	&\qquad-
	\frac{1}{2N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} -\frac{1}{2}\Ets[t-1]{\norm[]{f(\vtheta_t^{s+1})}^2},\nonumber
\end{align}
where (6) is from Definition \ref{def:ideal}; (7) is from the fact that $\gradIdeal{\vtheta_t^{s+1}}$ is both unbiased and independent from $f(\vtheta_t^{s+1})$ \wrt the sampling at time $t$ alone, which is not true for $\gradBlack{\vtheta_t^{s+1}}$; (8) is from $\Ets{e_s}=0$; (9) is from Young's inequality; and (10) is from the definition of $\gradApp{\vtheta}{N}$.
\end{proof}

%Lemma 1
\begin{restatable}[]{lemma}{auxone}\label{lemma:aux1}
The expected squared norm of the true gradient $\gradJ{\vtheta_t^{s+1}}$ can be bounded as follows:
\[
	\Ets{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} \leq
	\frac{R_{t+1}^{s+1} - R_t^{s+1} + Z_t}{\Psi_t},
\]
	where
\[
	R_t^{s+1}\coloneqq \Ets{J(\vtheta_t^{s+1}) + c_t\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
\]
\end{restatable}
\begin{proof}
	We have:
	\begin{align}
	\Ets{J(\vtheta_{t+1}^{s+1})} 
	&\geq \Ets{J(\vtheta_t^{s+1})+\dotprod{\gradJ{\vtheta_t^{s+1}}}{\vtheta_{t+1}^{s+1}-\vtheta_t^{s+1}} - \frac{L}{2}\norm[]{\vtheta_{t+1}^{s+1}-\vtheta_t^{s+1}}^2} \\
	&= \Ets{J(\vtheta_t^{s+1})+\alpha_t\dotprod{\gradJ{\vtheta_t^{s+1}}}{\gradBlack{\vtheta_t^{s+1}}} - \frac{\alpha_t^2L}{2}\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2} \\
	&\geq
	\Ets{J(\vtheta_t^{s+1})+\alpha_t\norm[]{\gradJ{\vtheta_t^{s+1}}^2} - \frac{\alpha_t^2L}{2}\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2} \nonumber\\
	&\qquad-
	\frac{\alpha_t}{2N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} -\frac{\alpha_tL^2}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1} - \tilde{\vtheta}^s}^2},
\end{align}
where (11) is from the L-smoothness of $J(\vtheta)$, (12) is from the SVRPG update, and (13) is from Lemma \ref{lemma:aux0}.

Next we have:
\begin{align}
\Ets{\norm[]{\vtheta_{t+1}^{s+1}-\tilde{\vtheta}^s}^2} 
&= \Ets{\norm[]{\vtheta_{t+1}^{s+1}- \vtheta_t^{s+1} + \vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \nonumber\\
&=\Ets{\norm[]{\vtheta_{t+1}^{s+1}-\vtheta_{t}^{s+1}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2+2\dotprod{\vtheta_{t+1}^{s+1}-\vtheta_{t}^{s+1}}{\vtheta_t^{s+1}-\tilde{\vtheta}^s}} \nonumber \\
&= \Ets{\alpha_t^2\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2+2\alpha_t\dotprod{\gradBlack{\vtheta_t^{s+1}}}{\vtheta_t^{s+1}-\tilde{\vtheta}^s}} \\
&= \Ets{\alpha_t^2\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2+2\alpha_t\dotprod{\gradJ{\vtheta_t^{s+1}}}{\vtheta_t^{s+1}-\tilde{\vtheta}^s}} \nonumber\\ 
&\qquad-
\frac{\alpha_t}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} -\frac{1}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \\
%
&= \Ets{\alpha_t^2\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
-2\alpha_t\Ets{\left|\dotprod{\gradJ{\vtheta_t^{s+1}}}{\vtheta_t^{s+1}-\tilde{\vtheta}^s}\right|} \nonumber\\ 
&\qquad-
\frac{\alpha_t}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} -\frac{1}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \nonumber\\
%
&\geq \Ets{\alpha_t^2\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
-2\alpha_t\Ets{\norm[]{\gradJ{\vtheta_t^{s+1}}}\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}} \nonumber\\ 
&\qquad-
\frac{\alpha_t}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} -\frac{1}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \nonumber\\
%
&\geq \Ets{\alpha_t^2\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
-2\alpha_t\Ets{\frac{1}{2\beta}\norm[]{\gradJ{\vtheta_t^{s+1}}}^2+\frac{\beta}{2}\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \nonumber\\ 
&\qquad-
\frac{\alpha_t}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} -\frac{1}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2},
\end{align}
where (14) is from the SVRPG update, (15) is from Lemma \ref{lemma:aux0}, and (16) is from Young's inequality in the 'Peter-Paul' variant.

...[next: put (16),(13) and Lemma A.1 together to finish proof]
\end{proof}


\subsection*{Main theorem}

\convergence*
\begin{proof}
We prove the theorem for the following values of the constants:
\begin{align*}
& \psi \coloneqq \min_t\Psi_t, \\
& D \coloneqq \max_t d_t \\
& c_m \coloneqq 0 \\
& c_t \coloneqq c_{t+1}(1+\alpha_t\beta+3\alpha_t^2L^2), \\
\end{align*}
where:
\begin{align*}
	& \beta > 0 \\
	& \Psi_t \coloneqq (\alpha_t - \frac{c_{t+1}\alpha_t}{\beta} - \frac{3}{2}\alpha_t^2L - 3c_{t+1}\alpha_t^2) \\
	& d_t \coloneqq 3(\alpha_t\frac{L}{2} + c_{t+1}\alpha_t^2), \\
\end{align*}
and $\beta,\alpha_t$ are chosen so that $\psi_t > 0$ for $t=1,\dots,m$.

...
\end{proof}

\section{Experimental Details}\label{app:proofs}
Environments of our experiments are:
\begin{enumerate}
	\item \emph{Cart-Pole Balancing} : 4-dimensional state space: cart position x, pole angle $\theta$, the cart velocity $\dot{x}$ , and the pole velocity $\dot{\theta}$; 1-dimensional action space: the horizontal force applied to the cart body. Reward function  is defined as $r(s, a) := 10 - (1 - cos(\theta)) - 10^{-5}\norm[] a^2$. Episodes terminates when $|x|>2.4$ or $|\theta|>0.2$ or the number of time step T is major than 100.
	\item \emph{Mujoco Swimmer}: 12-dimensional state space: 3 links velocities ($v_x$ and $v_y$ of center of masses) and 2 actuated joints angles. 2-dimensional action space: the two momentums applied on actuated joints.  Reward function is defined as $r(s, a) := v_x - 10^{-4}\norm[2] a^2$. Episodes terminates when the number of time step T is major than 500.
	\item \emph{Mujoco Half Cheetah}: 20-dimensional state space: 9 links and 6 actuated joints angles. 6-dimensional action space: the 6 momentums applied on actuated joints.  Reward function is defined as $r(s, a) := v_x - 0.05\norm[2] a^2$. Episodes terminates when the number of time step T is major than 500.
\end{enumerate}

All the experiments' parameters are reported in the following table:

\centering
\begin{tabular}{| l | c  c  c |}
	\hline	
	& Cart-Pole & Swimmer & Half Cheetah \\
	\hline
	State space dimension  & 4 & 8 & 20 \\
	Control space dimension & 1 & 2 & 6 \\
	NN Hidden layers & 8 & 32x32 & 32x32 \\
	Learning rate & $10^{-2}$ & $10^{-3}$ & $10^{-3}$ \\
	Snapshot trajectories & 100 & 100 & 100 \\
	Sub-iterations trajectories/GPOMDP batch & 10 & 10 & 10 \\
	Max sub-iterations & 50 & 20 & 20 \\
	Trajectories lengths& 500 & 500 & 500 \\
	Discount $(\gamma)$& 0.99 & 0.995 & 0.99 \\
	Total trajectories& 10000 & 10000 & 10000 \\
	\hline  
\end{tabular}


\end{document}
