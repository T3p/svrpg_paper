\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

%%%USEFUL PACKAGES%%%
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{amsmath,amssymb}
\usepackage{mathtools}
\usepackage{pifont}
\usepackage[makeroom]{cancel}

%%Theorems
\usepackage{amsthm}
\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}
\newtheorem{case}{Case}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{conj}{Conjecture}
\newtheorem{example}{Example}
%Restatable
\usepackage{thmtools, thm-restate}
\declaretheorem[numberwithin=section]{thm}
\declaretheorem[sibling=thm]{lemma}
\declaretheorem[sibling=thm]{corollary}
\declaretheorem[sibling=thm]{assumption}
\declaretheorem[sibling=thm]{theorem}
%e.g. ...
\usepackage{xspace}
\DeclareRobustCommand{\eg}{e.g.,\@\xspace}
\DeclareRobustCommand{\ie}{i.e.,\@\xspace}
\DeclareRobustCommand{\wrt}{w.r.t.\@\xspace}
% graphics
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tikz,pgfplots}
\pgfplotsset{compat=newest}
%% TODOs
% \setlength{\marginparwidth}{2cm}
\usepackage[colorinlistoftodos, textwidth=20mm]{todonotes}
\definecolor{citrine}{rgb}{0.89, 0.82, 0.04}
\definecolor{blued}{RGB}{70,197,221}
%%todo by Marcello
\newcommand{\todomarc}[1]{\todo[color=green, inline]{\small #1}}
\newcommand{\todomarcout}[1]{\todo[color=green]{\scriptsize #1}}
%%todo by Matteo Papini
\newcommand{\todomat}[1]{\todo[color=citrine, inline]{\small #1}}
\newcommand{\todomatout}[1]{\todo[color=citrine]{\scriptsize #1}}
%%todo by Matteo Pirotta
\newcommand{\todopir}[1]{\todo[color=blued, inline]{\small #1}}
\newcommand{\todopirout}[1]{\todo[color=blued]{\scriptsize #1}}
%%todo by Damiano
\newcommand{\tododam}[1]{\todo[color=orange, inline]{\small #1}}
\newcommand{\tododamout}[1]{\todo[color=orange]{\scriptsize #1}}
%%todo by Giuseppe
\newcommand{\todobep}[1]{\todo[color=purple, inline]{\small #1}}
\newcommand{\todobepout}[1]{\todo[color=purple]{\scriptsize #1}}
%%%%%%

%%%CUSTOM COMMANDS%%%
%Math
\newcommand{\realspace}{\mathbb R}      % realspace
\newcommand{\transpose}[1]{{#1}^\texttt{T}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\EV}{\mathbb{E}}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator*{\Cov}{\mathbb{C}ov}
\DeclareMathOperator*{\Var}{\mathbb{V}ar}
\newcommand{\EVV}[2][\ppvect \in \ppspace]{\EV_{#1}\left[{#2}\right]}
\newcommand{\norm}[2][\infty]{\left\|#2\right\|_{#1}}
\newcommand{\Dij}[2]{\frac{\partial^{2}{#1}}{\partial{#2}_i\partial{#2}_j}}
\newcommand{\de}{\,\mathrm{d}}
\newcommand{\dotprod}[2]{\left\langle#1,#2\right\rangle}
\newcommand{\dnabla}{\nabla\!\!\!\!\nabla}
%RL
\newcommand{\vtheta}{\boldsymbol{\theta}}
\newcommand{\Aspace}{\mathcal{A}}
\newcommand{\Sspace}{\mathcal{S}}
\newcommand{\Tspace}{\mathcal{T}}
\newcommand{\Transition}{\mathcal{P}}
\newcommand{\Reward}{\mathcal{R}}
\newcommand{\stationary}{d_{\rho}^{\pi_{\vtheta}}(s)}
\newcommand{\policy}{\pi_{\vtheta}(a \vert s)}
\newcommand{\pol}{\pi_{\vtheta}}
\newcommand{\trajdistr}{\pi_{\vtheta}(\tau)}
\newcommand{\score}[2]{\nabla\log\pi_{#1}(#2)}
\newcommand{\Qfun}{Q^{\pi_{\vtheta}}(s,a)}
\newcommand{\Vfun}{V^{\pi_{\vtheta}}(s)}
\newcommand{\vTheta}{\boldsymbol{\Theta}}
\newcommand{\vphi}{\boldsymbol{\phi}}
\newcommand{\gradJ}[1]{\nabla J(#1)}
\newcommand{\gradApp}[2]{\widehat{\nabla}_{#2}J(#1)}
\newcommand{\eqdef}{\mathrel{\mathop:}=}
\newcommand{\Dataset}{\mathcal{D}}
%Specific
\newcommand{\Ets}[2][t]{\mathbb{E}_{#1\vert s}\left[#2\right]}
\newcommand{\Covts}[3][t]{{\mathbb{C}\text{ov}}_{#1\vert s}\left(#2,#3\right)}
\newcommand{\Varts}[2][t]{{\mathbb{V}\text{ar}}_{#1\vert s}\left[#2\right]}
\newcommand{\gradBlack}[1]{\blacktriangledown J(#1)}
\newcommand{\gradIdeal}[1]{\dnabla J(#1)}
\newcommand{\VARRF}{V}
\newcommand{\GRADLOG}{G}
\newcommand{\VARIS}{W}
\newcommand{\HESSLOG}{F}
%%%%%%

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{SVRPG}

\begin{document}

\twocolumn[
\icmltitle{Stochastic Variance Reduced Policy Gradient}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2018
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Damiano Binaghi}{equal,polimi}
\icmlauthor{Giuseppe Canonaco}{equal,polimi}
\icmlauthor{Matteo Papini}{polimi}
\icmlauthor{Matteo Pirotta}{inria}
\icmlauthor{Marcello Restelli}{polimi}
\end{icmlauthorlist}

\icmlaffiliation{polimi}{Politecnico di Milano, Milano, Italy}
\icmlaffiliation{inria}{Inria, Lille, France}

\icmlcorrespondingauthor{Matteo Papini}{matteo.papini@polimi.it}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}



\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Why we need variance-reduced gradient in RL?
\end{abstract}

\section{Introduction}
%Similarities between RL and SL
On a very general level, machine learning addresses the problem of an agent that must select the right actions to solve a task. The approach of Reinforcement Learning~\citep{sutton1998reinforcement} is to learn the best actions by direct interaction with the environment and evaluation of the performance in the form of a reward signal. This makes RL fundamentally different from Supervised Learning (SL), where correct actions are explicitly prescribed by a human teacher (\eg for classification, in the form of class labels). However, the two approaches share many challenges and tools. The problem of estimating a model from samples, which is at the core of SL, is equally fundamental in RL, whether we choose to model the environment, a value function, or a policy of behaviour directly. Moreover, when the tasks are characterized by large or continuous state-action spaces, RL needs to borrow the powerful function approximators that are the main subject of study of SL, such as neural networks.
% SL needs SVRG so RL
In a typical SL setting, a performance function $J(\vtheta)$ has to be optimized \wrt to model parameters $\vtheta$. The set of data that are available for training is often a subset of all the data of interest, which may even be infinite, leading to optimization of finite sums to approximate expected performance over an unknown data distribution. When generalization to the complete dataset is not taken into consideration we talk about Empirical Risk Minization (ERM). Even in this case, stochastic optimization is often used for reasons of efficiency. The idea of stochastic gradient descent \cite{nesterov2013introductory} is to iteratively focus on a random subset of the available data to obtain an approximate improvement direction. At the level of the single iteration, this can be much less expensive than taking into account all the data. However, the sub-sampling of data samples is a source of variance that can potentially compromise convergence, so that per-iteration efficiency and convergence rate must be traded off with proper handling of meta-parameters.
Variance-reduced gradient algorithms such as SAG \cite{roux2012stochastic}, SVRG \cite{johnson2013accelerating} and SAGA \cite{defazio2014saga} offer better ways of addressing this trade-off, with significant results both in theory and practice. Although designed explicitly for ERM, this algorithms address a problem that affects more general machine learning problems. 
In RL, stochastic optimization is rarely a matter of choice, since data must be actively sampled by interacting with an initially unknown environment. In this scenario, containing the variance of the estimates is a problem that cannot be avoided, which makes variance-reduced algorithms very interesting.
% Potential advantages of SVRG
Among RL approaches, policy gradient is the one that bears the closest similarity to SL solutions. The fundamental principle of this methods is to optimize a parametric policy by means of stochastic gradient descent. Compared to other applications of SGD, the cost of collecting samples can be very high since it requires to interact with the environment. This makes SVRG-like methods potentially much more efficient than, \eg batch learning.
% Difficulties in RL \wrt SL
Unfortunately, RL has a series of difficulties that are not present in ERM. First, in SL the objective can often be designed to be strongly convex. This is not possible in RL, so we have to deal with non-convexities. Then, as mentioned, the dataset is not initially available and may even be infinite, which  makes approximations unavoidable. This also rules out SAG and SAGA because of their storage requirements, which leaves SVRG as the most promising choice. Finally, the distribution used to sample data is not under direct control of the algorithm designer, but is a function of policy parameters that changes over time as the policy is optimized, which is a form of nonstationarity.

% Related work

%Paper structure
After providing a background on policy gradient and SVRG in Section \ref{sec:pre}, in Section \ref{sec:alg} we address the aforementioned difficulties to design SVRPG, a variant of SVRG for the policy gradient framework. In Section \ref{sec:conv} we provide convergence guarantees for our algorithm which also offers insights on the challenges of variance reduction in RL. In Section \ref{sec:stopping} we suggests how to set the hyperparameters of SVRPG, while in Section \ref{sec:prac} we discuss some practical variants of the algorithm. Finally, in Section \ref{sec:exp} we empirically evaluate the performance of our method on popular continuous RL benchmark tasks.

\section{Preliminaries}\label{sec:pre}
In this section we provide the essential background on policy gradient methods for Reinforcement Learning and stochastic variance-reduced gradient methods for finite-sum optimization.
%In the next section we will show how these methods can be combined to solve Reinforcement Learning tasks.

\subsection{Policy Gradient}\label{subsec:PolicyGradient}
A Reinforcement Learning task~\citep{sutton1998reinforcement} can be modelled with a discrete-time continuous Markov Decision Process (MDP) $M = \{\Sspace,\Aspace,\Transition,\Reward,\gamma,\rho\}$, where $\Sspace$ is a continuous state space; $\Aspace$ is a continuous action space; $\Transition$ is a Markovian transition model, where $\Transition(s'|s,a)$ defines the transition density from state $s$ to $s'$ under action $a$; $\Reward$ is the reward function, where $\Reward(s,a) \in [-R,R]$ is the expected reward for state-action pair $(s,a)$;
% and $R$ is the maximum absolute-value reward;
$\gamma\in[0,1)$ is the discount factor; and $\rho$ is the initial state distribution.
The agent's behaviour is modelled as a policy $\pi$, where $\pi(\cdot|s)$ is the density distribution over $\Aspace$ in state $s$.
% We consider episodic tasks, \ie tasks composed of episodes of length $H$, also called time horizon.
% In this setting, a trajectory $\tau$ is the sequence of states and actions $(s_0,a_0,s_1,a_1,\dots,s_{H-1},a_{H-1})$ observed by following some stationary policy in a given episode, where $s_0$ is always sampled from the initial state distribution $\rho$.
We consider episodic MDPs with effective horizon $H$.\footnote{The episode duration is a random variable but the optimal policy is able to reach the target state (\ie absorbing state) in at most $H$ steps. This has not to be confused with a finite horizon problem where the optimal policy is non-stationary.} In this setting, we can limit our attention to trajectories of length $H$. A trajectory $\tau$ is a sequence of states and actions $(s_0,a_0,s_1,a_1,\dots,s_{H-1},a_{H-1})$ observed by following a stationary policy, where $s_0 \sim \rho$.
With abuse of notation, we denote with $\pi(\tau)$ the density distribution induced by policy $\pi$ on the set $\Tspace$ of all possible trajectories, and with $\Reward(\tau)$ the total discounted reward provided by trajectory $\tau$:
%
$\Reward(\tau) = \sum_{t=0}^{H-1}\gamma^t\Reward(s_t,a_t).$
%
Policies can be ranked based on their expected total reward: $J(\pi) = \EVV[\tau \sim \pi]{\Reward(\tau)|M}$.
Solving an MDP $M$ means finding $\pi^* \in \argmax_{\pi} \{J(\pi)\}$.

Policy gradient methods restrict the search for the performance-maximizing policy over a class of parametrized policies $\Pi_{\vtheta}=\{\pol: \vtheta \in \realspace^m\}$, with the only constraint that $\pol$ is differentiable \wrt $\vtheta$. We denote the performance of a parametric policy with $J(\vtheta)$ for brevity, while in some occasions we replace $\pol(\tau)$ with $\pi(\tau|\vtheta)$ for the sake of readability.\todopirout{See if it used}
The search for a locally optimal policy is performed by means of gradient ascent, where the policy gradient
%of $J(\vtheta)$ \wrt the policy parameters 
is \cite{sutton2000policy, Peters2008reinf}:
\begin{align} \label{E:policygradient}
        \gradJ{\vtheta} = \EVV[\tau \sim \pol]{\score{\vtheta}{\tau}\Reward(\tau)}
        %\int_{\Tspace}\pol(\tau)\score{\vtheta}{\tau}\Reward(\tau)\de \tau.
\end{align}
Notice that the distribution defining the gradient is induced by the current policy. This aspect introduces a nonstationarity in the sampling process. Since the underlying distribution changes over time, it is necessary to resample at each update or use weighting techniques such as importance sampling.
Here, we consider the \emph{online learning scenario}, where trajectories are sampled by interacting with the environment at each policy change. 
In this setting, stochastic gradient ascent is typically employed.
At each iteration $k >0$, a batch $\mathcal{D}_N^k = \{\tau_i\}_{i=0}^N$ of $N>0$ trajectories is collected using policy $\pi_{\vtheta_k}$.
The policy is then updated as $\vtheta_{k+1}  = \vtheta_k + \alpha\gradApp{\vtheta_k}{N}$, where $\alpha$ is a step size and $\gradApp{\vtheta}{N}$ is an estimate of~\eqref{E:policygradient} using $\mathcal{D}_N^k$, \ie
\begin{align} \label{E:policygradient.estimate}
        \gradApp{\vtheta}{N} = \frac{1}{N}\sum_{n=1}^{N} \nabla f(\tau_i), \quad \tau_i \in \mathcal{D}_N^k.
\end{align}
The most widespread gradient estimators are REINFORCE~\citep{williams1992simple} and G(PO)MDP~\citep{baxter2001infinite}.
We refer the reader to App.~\ref{A:gradient_estimators} for a formal definition.

The main limitation of plain policy gradient has been the high variance of these estimators.
The naive approach of increasing the batch size is not an option in RL due to the high cost of collecting samples by interacting with the environment.
For this reason, literature has focused on the introduction of baselines aiming to reduce the variance~\citep[\eg][]{Peters2008reinf,Thomas2017actionbaseline,wu2018variance}.\footnote{A baseline is a function $f(s,a)$ such that $\EVV[a \sim \pol]{f(s,a)} = 0$, for any $s\in\Sspace$.}
There has been a surge of recent interest in variance reduction techniques for gradient optimization in supervised learning.
Altough, these techniques has been mainly derived for finite-sum problems, we will show in Sec.~\ref{sec:algo} how can be used in RL.
The next section has the aim to describe variance reduction techniques for finite-sum problems. In particular, we will present SVRG algorithm that is at the core of this work.


\subsection{Stochastic Variance-Reduced Gradient}
Finite-sum optimization is the problem of maximizing an objective function $f(\vtheta)$ which can be decomposed into the sum or average of a finite number of functions $g_i(\cdot|\vtheta)$:
\begin{align*}
        \max_{\vtheta} \left\{ f(\vtheta) = \frac{1}{N}\sum_{i=1}^{N}g_i(\vtheta)\right\}.
\end{align*}
This kind of optimization is very common in machine learning, where each $g_i$ may correspond to a data sample $x_i$ from a dataset $\mathcal{D}_N$ of size $N$ (\ie $g_i(\vtheta) = g(x_i|\vtheta)$). 
%% In this case, we adopt the following, more meaningful notation:
%% \begin{align*}
%% \max_{\vtheta} f(\vtheta) = \frac{1}{N}\sum_{i=1}^{N}g(x_i \vert \vtheta).
%% \end{align*}
A common requirement is that $g$ must be smooth and {\color{red} concave} in $\vtheta$.\footnote{Note that we are considering a maximization problem instead of the classical minimization one.} Under this hypothesis, full gradient ascent \cite{cauchy1847methode} with a constant step size achieves an Incremental First-order Oracle (IFO) complexity~\citep{agarwal2014lower} of $O(\nicefrac{N}{\epsilon})$, \ie it requires $k = O(\nicefrac{N}{\epsilon})$ gradient computations to achieve $\norm[]{\nabla_{\vtheta}f(\vtheta_k)}^2\leq\epsilon$.\todopirout{Is it $\ell_2$-norm?} 
This can be too expensive for large values of $N$. Stochastic Gradient (SG) ascent~\citep[\eg][]{robbins1951stochastic,bottou2004large} overcomes this problem by sampling a single sample $x_i$ per iteration, but a vanishing step size is required to control the variance introduced by sampling. The resulting IFO complexity is $O(\nicefrac{1}{\epsilon^2})$ in expectation, which does not depend on $N$, but has a worse dependency on $\epsilon$. 
\todopir{What is the reference?}
Starting from SAG, a series of variations to SG have been proposed to achieve a better trade-off between convergence speed and cost per iteration: \eg SAG~\citep{roux2012stochastic}, SVRG~\cite{johnson2013accelerating}, SAGA~\cite{defazio2014saga} and Finito~\cite{defazio2014finito}, 
The common idea is to reuse past gradient computations to reduce the variance of the current estimate.
In particular, Stochastic Variance-Reduced Gradient (SVRG) is often preferred to other similar methods for its limited storage requirements, which can become a problem when deep and/or wide neural networks are employed.  

The idea of SVRG is to alternate full and stochastic gradient updates. 
\todopir{here would be nice to have a notation for the single batch}
Each $m = O(N)$ iterations, a snapshot $\tilde{\vtheta}$ of the current parameter is saved together with its full gradient $\nabla f(\tilde{\vtheta})$.
Otherwise, the parameter is updated with a corrected stochastic gradient estimate of the form:
\begin{align*}
\blacktriangledown f(\vtheta) = 
	\nabla f(\tilde{\vtheta}) + 
	\nabla g(x_i | \vtheta) -
	\nabla g(x_i | \tilde{\vtheta}),
\end{align*} 
where $x_i$ is sampled uniformly at random from $\mathcal{D}_N$. The corrected gradient $\blacktriangledown f(\vtheta)$ is an unbiased estimate of $\nabla f(\vtheta)$, and it is able to control the variance introduced by sampling even with a fixed step size, without resorting to plain full gradient. The resulting IFO complexity is $O(N+\nicefrac{\sqrt{N}}{\epsilon})$ \cite{reddi2016stochastic}.

More recently, some extensions of variance reduction algorithms to the non-convex objectives have been proposed~\citep[\eg][]{reddi2016stochastic,allen2016variance,reddi2016stochastic,reddi2016fast}. In this scenario, $f$ is typically required to be L-smooth, \ie $\norm[]{\nabla f(\vtheta') - \nabla f(\vtheta)} \leq L\norm[]{\vtheta'-\vtheta}$ for each $\vtheta,\vtheta'\in\realspace^n$ and for some Lipschitz constant $L$. Under this hypothesis, the IFO complexity of full gradient  and of stochastic gradient ascent are the same as in the convex case~\citep{nesterov2013introductory,ghadimi2013stochastic}. Also in this case, SVRG yields an advantageous trade-off, with an IFO complexity of $O(N + \nicefrac{N^{\nicefrac{2}{3}}}{N})$~\citep{reddi2016stochastic}. The only additional requirement is to select $\vtheta^*$ uniformly at random among all the $\vtheta_k$ instead of simply setting it to the final value. Pseudocode of SVRG for both the convex and the non-convex case is provided in Alg.~\ref{alg:svrg}. 


\begin{algorithm}[tb]
	\caption{SVRG}
	\label{alg:svrg}
	\begin{algorithmic}
		\STATE {\bfseries Input:} number of epochs $S$, epoch size $m$, step size $\alpha$, initial parameter $\vtheta_{m-1}^0$
		\FOR{$s=0$ {\bfseries to} $S-1$}
		\STATE $\vtheta_0^{s+1} = \tilde{\vtheta}^s = \vtheta_{m-1}^s$
		\FOR{$t=0$ {\bfseries to} $m-1$}
		\STATE Pick $i$ uniformly at random from $[1,N]$
		\STATE \begin{align*}
			\blacktriangledown f(\vtheta_t^{s+1}) = 
			&\nabla f(\tilde{\vtheta}^s) \\ + 
			&\nabla g(x_i\vert\vtheta_t^{s+1}) -
			\nabla g(x_i \vert \tilde{\vtheta}^{s+1})
		\end{align*}
		\STATE $\vtheta_{t+1}^{s+1} = \vtheta_t^{s+1} + \alpha\blacktriangledown f(\vtheta_t^{s+1})$
		\ENDFOR
		\ENDFOR
		\STATE {\bfseries Convex case: return} $\vtheta_{m-1}^S$
		\STATE {\bfseries Non-Convex case: return} $\vtheta_t^s$ with $(s,t)$ picked uniformly at random from $[0,S)\times[0,m)$
	\end{algorithmic}
\end{algorithm}


\section{Algorithm}\label{sec:alg}
In on-line policy gradient problems, stochastic gradient ascent (SGA) is not really a choice, but is dictated by the necessity of interacting with an unknown environment. We would like to apply SVRG to the policy gradient framework in order to limit the variance introduced by sampling trajectories, which would ultimately lead to faster convergence. In this scenario, the function $f(\vtheta)$ to optimize is $J(\vtheta)$ and the (implicit) dataset $\mathcal{D}$ is the set of all possible trajectories $\mathcal{T}$. Compared to the typical finite sum optimization scenario, we have the following additional challenges:
\begin{enumerate}
	\item \textit{Non-convexity}: the objective $J(\vtheta)$ is typically non-convex;
	\item \textit{Approximation}: the dataset $\mathcal{T}$ can be infinite, but we can only sample a finite number of trajectories;
	\item \textit{Non-stationarity}: the value of the parameter $\vtheta$ influences the sampling of trajectories;
\end{enumerate}
To deal with non-convexity, we require $J(\vtheta)$ to be L-smooth, which is a reasonable assumption for common policy classes such as truncated Gaussian and Softmax [appendix?]. Because of the infinite dataset, we can only rely on an estimate of the full gradient. Finally, we employ importance weighting \cite{rubinstein1981simulation} \cite{precup2000eligibility} to guarantee proper sampling of trajectories. 
We focus on the REINFORCE gradient estimator for simplicity, but the extension to G(PO)MDP is straightforward. The proposed SVRG-like gradient estimate, which we call Stochastic Variance Reduced Policy Gradient (SVRPG), is the following:
\begin{align*}
	\blacktriangledown J(\vtheta) &= \frac{1}{N}\sum_{j=0}^{N-1}\nabla\log\pi(\tau_j \vert \tilde{\vtheta})\Reward(\tau_j) \\
		&+ \nabla\log\pi(\tau_i \vert \vtheta)\Reward(\tau_i) 
		- \omega(\tau_i)\nabla\log\pi(\tau_i \vert \tilde{\vtheta})\Reward(\tau_i),
\end{align*}
where trajectories $\tau_j$ for $j=1,\dots,N$ are sampled from a snapshot policy $\pi_{\tilde{\vtheta}}$, $\tau_i$ is sampled from the current policy $\pol$, and $\omega(\tau) = \frac{\pi(\tau\vert\tilde{\vtheta})}{\pi(\tau\vert\vtheta)}$ is an importance weight from $\pol$ to the snapshot policy $\pi_{\tilde{\vtheta}}$. 
Our update is still fundamentally on-policy, since the weighting concerns only the correction term. However, this partial "off-policyness" is enough to introduce variance. To mitigate it, we use mini-batches of trajectories of size B to compute the second and the third term, as follows:
\begin{align*}
\blacktriangledown J(\vtheta) &= \frac{1}{N}\sum_{j=0}^{N-1}\nabla\log\pi(\tau_j \vert \tilde{\vtheta})\Reward(\tau_j) \\
&+ \frac{1}{B}\left[\sum_{i=0}^{B-1}
\nabla\log\pi(\tau_i \vert \vtheta)\Reward(\tau_i) 
- \omega(\tau_i)\nabla\log\pi(\tau_i \vert \tilde{\vtheta})\Reward(\tau_i)\right].
\end{align*}
The use of mini-batches is a common practice in SVRG since it can yield a performance improvement even in the supervised case \cite{konevcny2016mini} \cite{harikandeh2015stopwasting}. It is easy to show that the SVRPG gradient estimator has the following, desirable properties (detailed proofs in Appendix \ref{app:proofs}):
\begin{restatable}{lemma}{unbias}\label{lemma:unbias}
The SVRPG estimator is unbiased:
\[
\mathop{\mathbb{E}}
\left[\blacktriangledown J(\vtheta)\right] = \gradJ{\vtheta}.
\]
\end{restatable}
\begin{restatable}{lemma}{zerovar}\label{lemma:zerovar}
At convergence, the variance of the SVRPG estimator is equal to the variance of the REINFORCE estimator, regardless of the mini-batch size $B$:
\[
	\mathbb{V}ar\left[\gradBlack{\vtheta^*}\right] = 
	\mathbb{V}ar\left[\gradApp{\vtheta^*}{N}\right].
\]
\end{restatable}
In particular, the latter suggests that an SVRG-like algorithm using $\gradBlack{\vtheta}$ can actually achieve faster convergence, by performing much more parameter updates with the same data without introducing additional variance, at least asymptotically.
The resulting SVRPG policy optimization method is detailed in Algorithm \ref{alg:svrpg}.
To obtain a method that can be used in practice, a number of additional details must be specified, namely the choice of epoch size $m$, the number $N$ of trajectories used to estimate the full gradient, the mini-batch size $B$, and the step size $\alpha$. Adaptive selection of these meta-parameters is also possible, and is explored in Section \ref{sec:stopping}. However, the next section will focus on providing general converge guarantees for Algorithm \ref{alg:svrpg}.

\begin{algorithm}[tb]
	\caption{SVRPG}
	\label{alg:svrpg}
	\begin{algorithmic}
		\STATE {\bfseries Input:} number of epochs $S$, epoch size $m$, step size $\alpha$, initial parameter $\vtheta_{m-1}^0$, batch size $N$, mini-batch size B
		\FOR{$s=0$ {\bfseries to} $S-1$}
		\STATE Sample N trajectories $\tau_j$ from $\pi(\cdot\vert\tilde{\vtheta}^{s})$
		\STATE $\gradApp{\tilde{\vtheta}^{s}}{N} = \frac{1}{N}\sum_{j=0}^{N-1}\score{}{\tau_j\vert\tilde{\vtheta}^{s}}\Reward(\tau_j)$
		\STATE $\vtheta_0^{s+1} = \tilde{\vtheta}^s = \vtheta_{m-1}^s$
		\FOR{$t=0$ {\bfseries to} $m-1$}
		\STATE Sample $B$ trajectories $\tau_i$ from 				$\pi(\cdot\vert\vtheta_t^{s+1})$
		\STATE 
		\begin{align*}
		\blacktriangledown J(\vtheta_t^{s+1}) = 
		&\gradApp{\tilde{\vtheta}^s}{N} \\
		&+\frac{1}{B}\sum_{i=0}^{B-1}\left[ 
		\score{}{\tau_i\vert\vtheta_t^{s+1}}\Reward(\tau_i)\right. \\
		&\left. - \omega(\tau_i)\score{}{\tau_i \vert \tilde{\vtheta}^{s}}\Reward(\tau_i)\right]
		\end{align*}
		\STATE $\vtheta_{t+1}^{s+1} = \vtheta_t^{s+1} + \alpha\blacktriangledown J(\vtheta_t^{s+1})$
		\ENDFOR
		\ENDFOR
		\STATE {\bfseries return} $\vtheta_t^s$ with $(s,t)$ picked uniformly at random from $[0,S)\times[0,m)$
	\end{algorithmic}
\end{algorithm}

\section{Convergence Guarantees}\label{sec:conv}
In this section we adapt existing convergence guarantees for nonconvex SVRG \cite{reddi2016stochastic} \cite{allen2016variance} to the Policy Gradient framework.
Each of the three challenges presented at the beginning of Section \ref{sec:algo} can potentially prevent convergence, so we need additional assumptions:
\begin{enumerate}
\item \textit{Non-convexity}:
	as already mentioned, a common assumption for nonconvex optimization is the L-smoothness of the objective. We actually require the following, which is stronger:
	\begin{assumption}\label{ass:bounded_score}
		For each trajectory $\tau$, value of $\vtheta$ and dimensions $i,j$ there are positive constants $G,F<\infty$ such that:
		\begin{align*}
		&\left|\nabla_{\theta_i}\log\pi_{\vtheta}(\tau)\right| \leq \GRADLOG \\
		&\left|\frac{\partial^2}{\partial\theta_i\partial\theta_j}\log\pi_{\vtheta}(\tau)\right| \leq \HESSLOG
		\end{align*}
	\end{assumption}
	This property is satisfied by the commonly used truncated Gaussian policy, and is a sufficient condition for the L-smoothness of the objective (see Lemma \ref{lemma:lsmooth} in the appendix).
\item \textit{Approximation}:
since we can only compute an estimate of the full gradient, we require its variance to be bounded:
	\begin{assumption}\label{ass:REINFORCE}
		There is a constant $V<\infty$ such that, for each policy $\pol$:
		\[
		\Tr\mathbb{V}ar\left(\gradApp{\vtheta}{1}\right) \leq \VARRF.
		\]
	\end{assumption}
This assumption is satisfied by the REINFORCE and G(PO)MDP estimators and Gaussian policies \cite{zhao2011analysis} \cite{pirotta2013adaptive}. The problem of full gradient approximation for SVRG has been addressed in the supervised learning framework \cite{harikandeh2015stopwasting} with results similar to ours.
\item \textit{Non-stationarity}:
the variance introduced by importance sampling must be bounded:
	\begin{assumption}\label{ass:M2}
		There is a constant $W<\infty$ such that, for each pair of policies encountered in Algorithm \ref{alg:svrpg} and for each possible trajectory,
		\[
		\mathbb{V}ar\left(\omega(\tau)\right) \leq \VARIS.
		\]
	\end{assumption}
Differently from the previous two, this assumption must be enforced by a proper handling of the epoch size $m$.
\end{enumerate}

We can now state the following key result:
\begin{restatable}[Convergence of the SVRPG algorithm]{theorem}{convergence}\label{theo:convergence}
Under Assumptions \ref{ass:bounded_score}, \ref{ass:REINFORCE} and \ref{ass:M2}, the parameter vector $\vtheta_A$ returned by Algorithm \ref{alg:svrpg} after $T=m\times S$ iterations has, for some positive constants $\psi,\zeta, \xi$ and for proper choice of the step size $\alpha$ and the epoch size $m$, the following property:
\begin{align*}
	&\EVV[]
	{\norm[]{\nabla J(\vtheta_A)}^2} 
		\leq
		\frac{J(\vtheta^*)-J(\vtheta_0)}{\psi T} +
		\frac{\zeta}{N}
		+\frac{\xi}{B}
\end{align*}
\end{restatable}
Specific constants and meta-parameter constraints are provided in the proof of the theorem in Appendix \ref{app:proofs}. In Section \ref{sec:stopping}, instead, we propose a joint selection of step size $\alpha$ and epoch size $m$ which is more suited for applications. 
The $\mathcal{O}(\nicefrac{1}{T})$ term is coherent with results on non-convex SVRG, \eg \cite{reddi2016stochastic}; the $\mathcal{O}(\nicefrac{1}{N})$ term is due to full gradient approximation and is analogous to the one in \cite{harikandeh2015stopwasting}; the $\mathcal{O}(\nicefrac{1}{B})$ term is due to importance weighting. To achieve true convergence, both the batch size $N$ and the mini-batch size $B$ should increase over time. In practice, it is enough to choose $N$ and $B$ large enough to make the second and the third term negligible, \ie to mitigate the variance introduced by full gradient approximation and importance sampling respectively. Since the optimal configuration may be task-dependent, we leave this choice to the empirical analysis of Section \ref{sec:exp}. 

\section{Hyperparameters}\label{sec:stopping}
In section \ref{sec:conv}, we introduced the practical need to properly choose $\alpha$ and $m$, which are, respectively, the step size and the number of sub-iterations to perform after a snapshot. The first one is crucial to balance the variance introduced by estimations within sub-iterations with respect to the variance introduced by estimations associated to the snapshots. The second one is essetial to control the variance introduced by the importance weights, mainly because the farther you go from the snapshot policy the higher the variance of the importance weights will be.    

A common way to reduce the effect of variance is to use Adaptive Moment Estimation (Adam) \cite{kingma2014adam}, a method that computes adaptive learning rates for each parameter. Adam helps to stabilize the gradient estimate by computing adaptive learning rates for each parameter based on the history of the gradients' variance.

Due to this feature we can use two different adam in order to properly leverage the structure of the proposed SVRPG update. The first adam will be associated to the snapshot updates and the second one will be associated to the sub-iteration updates. By doing so we are decoupling the contribution to the variance introduced by the snapshots from that one introduced by the sub-iterations. These two variance contributions lie, respectively, on two different order of magnitude because the number of trajectories used to perform the estimates for the snapshot is greater than that used for a sub-iteration.Therefore, using two adam optimizers allows us to properly adapt the learning rate in each situation. For the above mentioned reasons let us denote by $\alpha_{snap}$ the learning rate associated to the last snapshot and by  $\alpha_{sub-iter}$ the learning rate associated to the current sub-iteration.

We have already said that as the number of sub-iterations $m$ increases the variance of the importance weights increases too. Therefore, if $m$ is not properly set the SVRPG algorithm fails in its primary aim of stabilizing the gradient estimate.

We have seen, sperimentally, that a fixed epoch size is not the optimal choice, but an adaptive one would better exploit the total amount of data at hand. Hence, we permorm a new snapshot anytime the weighted learning rate proposed by the adam optimizer in a sub-iteration is smaller than the weighted one proposed at the immediately previous snapshot. This stopping condition is subject to a constraint on the maximum number of sub-iterations performable, which is problem dependent.
More precisely we stop performing sub-iterations when:

\[\frac{\alpha_{snap}}{N}>\frac{\alpha_{sub-iter}}{B}\]

These two weights ${\bf{\it N}}$ and ${\bf{\it B}}$ represent, respectively, the number of trajectories used for a snapshot and the number of trajectories used for an update of the parameters in a sub-iteration. Therefore, in this stopping condition, we use them to take into account the trajectories efficiency. The less trajectories we use to perform an update, the better it is from a data usage point of view. Hence we want to facilitate sub-iterations with respect to snapshots.

Another insight about this adaptive stopping condition lies within the value of the two different learning rate proposed by adam. Whenever the learning rate of a sub-iteration becomes too small with respect to that one proposed at the previous snapshot we are experiencing high variance. Hence it could be better to perform a new snapshot so to mitigate this issue.


\section{Practical Variants}
\todomat{Missing opening to section}
We introduce a term with importance weights to guarantee proper sampling of trajectories.
This term's variance is subjected to the variance of importance weights. 
There are some methods to reduce this variance.

As reported in Section \ref{subsec:PolicyGradient} we use G(PO)MDP gradient estimator~\cite{baxter2001infinite}, which gives us the same guarantees we have for the Reinforce Estimator.
One variant in off-policy G(PO)MDP is to use the weighted per-decision importance sampling estimator \cite{precup2000eligibility}, allowing a reduction of the gradient estimator variance. The off-policy gradient estimator becomes:
\begin{align*}
\gradApp{\vtheta}{N} = \frac{1}{\Omega}\sum_{n=1}^{N}\sum_{h=0}^{H-1}\left(\sum_{k=0}^{h}\score{\vtheta}{a_k^n\vert s_k^n}\right)\left(\gamma^h r_h^n \omega(z_{0:h})\right).
\end{align*} 
where:
\begin{align*}
\omega(z_{0:h}) = \prod_{k=0}^{h} \frac{\pi_{\vtheta}({a_k^n\vert s_k^n})
 }{\pi_{\vtheta^{'}}({a_k^n\vert s_k^n})}
\end{align*}
\begin{align*}
\Omega = \sum_{n=1}^{N}\sum_{h=0}^{H-1}\gamma^h\prod_{k=0}^{h} \frac{\pi_{\vtheta}({a_k^n\vert s_k^n})
}{\pi_{\vtheta^{'}}({a_k^n\vert s_k^n})}
\end{align*}
\todomat{why should we use it? What advantages do we expect? At what cost? Pointer to experiments}
\section{Related Work}

\section{Experiments}\label{sec:exp}
We have designed our experiments to investigate whether:
\begin{enumerate}
\item The algorithm reaches high value of the reward function faster than GPOMDP.
\item Averagely SVRPG keep the reward function more stable than GPOMDP.
\end{enumerate}

In order to pursue the above mentioned objectives, we evaluate the behavior of the reward function averaged on 10 runs for each experiment.

We decide to use the following models of Rllab: \emph{Cart-Pole Balancing}, \emph{Mujoco Swimmer}, \emph{Mujoco Half Cheetah}.


In all the experiments we compete with GPOMDP standard algorithm. GPOMDP uses a number of trajectories per batch that is equal to the number of trajectories used in SVRPG mini-batch size.
The neural network and the learning rate are the same for GPOMDP and SVRPG.
In SVRPG we used two different Adam parameters: the first one for Snapshot and the second one for sub-iterations.
We used the stopping condition defined in Section \ref{sec:stopping}.
We fixed the maximum number of trajectories that both the algorithms can use as shown in appendices. 

The neural networks is composed of one layer with 8 neurons for Cart-Pole environment and by two layers with 32x32 neurons for Swimmer and for Half Cheetah environments. In each experiment, the length of episodes is 500 steps.

\section{Discussion}

\bibliography{svrpg}
\bibliographystyle{icml2018}

\newpage
\mbox{}
\newpage
\onecolumn
\appendix

\section{Policy Gradient Estimators} \label{A:gradient_estimators}


The REINFORCE gradient estimator~\citep{williams1992simple} provides a simple, unbiased way of estimating the gradient:
\begin{align*}
\gradApp{\vtheta}{N} = \frac{1}{N}\sum_{n=1}^{N}\left(\sum_{h=0}^{H-1}\score{\vtheta}{a_h^n\vert s_h^n}\right)\left(\sum_{h=0}^{H-1}\gamma^h r_h^n - b\right),
\end{align*}
where subscripts denote the time step, superscripts denote the trajectory, $r_h$ is the reward actually collected at time $h$ and b can be any baseline, provided it is constant \wrt to actions.
\todopir{Actually it can be state-action dependant~\citep[\eg][]{Thomas2017actionbaseline,wu2018variance}.}
The G(PO)MDP gradient estimator~\cite{baxter2001infinite} is a refinement of REINFORCE which is subject to less variance while preserving the unbiasedness:
\begin{align*}
\gradApp{\vtheta}{N} = \frac{1}{N}\sum_{n=1}^{N}\sum_{h=0}^{H-1}\left(\sum_{k=0}^{h}\score{\vtheta}{a_h^n\vert s_h^n}\right)\left(\gamma^h r_h^n - b\right).
\end{align*}



\section{Proofs}\label{app:proofs}

\subsection*{Definitions}
We give some additional definitions which will be useful in the proofs.
\begin{definition}
For a random variable $X$:
\begin{align*}
	&\mathbb{E}_{t\vert s}\left[X\right] \coloneqq 
		\mathop{\mathbb{E}}_{\substack{\tau_j\sim\pi(\cdot\vert\tilde{\vtheta}^s)\forall j \in {\it {\bf N}} \\ \tau_{i,h}\sim\pi(\cdot\vert\vtheta^{s+1}_i) \forall h \in {\it {\bf B}}, \text{ for $i=0,\dots,t-1$}}}{\left[X \vert \tilde{\vtheta^s}\right]} \\
	&\coloneqq \EVV[\tau_j\sim\pi(\cdot\vert\tilde{\vtheta}^s)\forall j \in {\it {\bf N}}]{
			\EVV[\tau_{0,h}\sim\pi(\cdot\vert\vtheta_0^{s+1}) \forall h \in {\it {\bf B}}]
				{\dots
					\EVV[\tau_{t,h}\sim\pi(\cdot\vert\vtheta_t^{s+1})\forall h \in {\it {\bf B}}]
						{X\vert\vtheta_t^{s+1}}
				 \dots
			\vert\vtheta_0^{s+1}}
		\vert\tilde{\vtheta}^s},
\end{align*}
where the sequence $\tilde{\vtheta}^s,\vtheta_0^{s+1},\dots,\vtheta_t^{s+1}$ is defined in Algorithm \ref{alg:svrpg}, ${\it {\bf N}}$ is the amount of trajectories sampled for the snapshot and ${\it {\bf B}}$ is the amount of trajectories sampled for a sub iteration. 
\end{definition}
Intuitively, the $\Ets{\cdot}$ operator computes the expected value with respect to the sampling of trajectories from the snapshot $\tilde{\vtheta}^s$ up to the $t$-th iteration included. Note that the order in which expected values are taken is important since each $\vtheta_{t}^{s+1}$ is function of previously sampled trajectories and is used to sample new ones.

\begin{definition}
The full gradient estimation error is:
\[
	e_s \coloneqq \gradApp{\tilde{\vtheta}^s}{N} - \gradJ{\tilde{\vtheta}^s} 
\]
\end{definition}

\begin{definition}\label{def:ideal}
The ideal SVRPG gradient estimate is:
\begin{align*}
	\gradIdeal{\vtheta_t^{s+1}} &\coloneqq 
	\gradJ{\tilde{\vtheta}^s}
	+ \nabla\log\pi(\tau_i \vert \vtheta_t^{s+1})\Reward(\tau_i) 
	- \omega(\tau_i)\nabla\log\pi(\tau_i \vert \tilde{\vtheta}^s)\Reward(\tau_i) \\
	&= \gradBlack{\vtheta_t^{s+1}} - \gradApp{\tilde{\vtheta}^s}{N} + \gradJ{\tilde{\vtheta}^s} \\
	&= \gradBlack{\vtheta_t^{s+1}} - e_s
\end{align*}
\end{definition}


\subsection*{Basic Lemmas}
We prove two basic properties of the SVRPG update.

\unbias*
\begin{proof}
\begin{align*}
\EVV[]{\gradBlack{\vtheta}} &= \EVV[]{\gradApp{\tilde{\vtheta}}{N}}  + \EVV[]{\gradApp{\vtheta}{B}} - \EVV[]{\frac{1}{B}\sum_{i=0}^{B-1}\omega(\tau_i)\score{\tilde{\vtheta}}{\tau_i}R(\tau_i)} \\
&= \gradJ{\tilde{\vtheta}} + \gradJ{\vtheta} - \gradJ{\tilde{\vtheta}} = \gradJ{\vtheta}.
\end{align*}
Note that the importance weight is necessary to guarantee unbiasedness, since the $\tau_i$ are sampled from $\pi_{\vtheta}$.
\end{proof}

\zerovar*
\begin{proof}
As $\vtheta\to\vtheta^*$, also $\tilde{\vtheta}\to\vtheta^*$. Hence, by continuity of $J(\vtheta)$:
\begin{align*}
\Var\left[\gradBlack{\vtheta}\right] &\to \Var\left[\gradApp{\vtheta^*}{N}\right] + \frac{1}{B}\Var\left[\score{\vtheta^*}{\tau}R(\tau) - \cancel{\omega(\tau)}\score{\vtheta^*}{\tau}R(\tau)\right] \\
&= \Var\left[\gradApp{\vtheta^*}{N}\right].
\end{align*}
Note that it is important that the trajectories used in the second and the third term are the same for the variance to vanish.
\end{proof}

\subsection*{Ancillary Lemmas}
Before addressing the main convergence theorem, we prove some useful lemmas.

 
\begin{restatable}[]{lemma}{L-smoothness}\label{lemma:lsmooth}
	Under Assumption \ref{ass:bounded_score}, $J(\vtheta)$ is L-smooth for some positive Lipschitz constant $L$.
\end{restatable}
\begin{proof}
By definition of $J(\vtheta)$:
\begin{align}
\Dij{J(\vtheta)}{\theta} 
&= \int_{\Tspace}\Dij{}{\theta}\pol(\tau)\Reward(\tau)\de \tau
\nonumber\\ 
&= \int_{\Tspace}\pol(\tau)\score{\theta}{\tau}\score{\theta}{\tau}^T\Reward(\tau)\de \tau + \int_{\Tspace}\pol(\tau)\Dij{}{\theta}\log\pol(\tau)\Reward(\tau)\de \tau \nonumber\\
&\leq \sup_{\tau} \left\{\left|\Reward(\tau)\right|\right\} \left(\GRADLOG^2+\HESSLOG\right) \label{eq:0}\\
&= \frac{R}{1-\gamma}\left(\GRADLOG^2+\HESSLOG\right),\nonumber
\end{align}
where \ref{eq:0} is from Assumption \ref{ass:bounded_score}.
Since the Hessian is bounded, $J(\vtheta)$ is Lipschitz-smooth.
\end{proof}

%Lemma 2
\begin{restatable}[]{lemma}{auxtwo}\label{lemma:aux2}
Under Assumption \ref{ass:bounded_score}
, the expected squared norm of the SVRPG gradient can be bounded as follows:
\[
\Ets{\gradBlack{\vtheta_t^{s+1}}} \leq
\Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
+L^2\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}}
\nonumber 
+\frac{G^2R^2W}{(1-\gamma)^2B}
\]
\end{restatable}
\begin{proof}
	\begin{align}
	\Ets{\gradBlack{\vtheta_t^{s+1}}} 
	&= \Ets{\norm[]{\gradApp{\tilde{\vtheta}^s}{N}
			+\frac{1}{B}\sum_{i=0}^{B-1} \score{\vtheta_t^{s+1}}{\tau_i}R(\tau_i) 
			-\frac{1}{B}\sum_{i=0}^{B-1}
			\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}R(\tau_i)}^2} \nonumber\\
	%
	&= \mathbb{E}_{t\vert s}\left[\left\|\gradApp{\tilde{\vtheta}^s}{N}
			+\frac{1}{B}\sum_{i=0}^{B-1}\left( 
			\score{\vtheta_t^{s+1}}{\tau_i}R(\tau_i) -
			\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}R(\tau_i)\right)
			\right.\right.\nonumber\\&\qquad\left.\left.
			-\gradJ{\vtheta_t^{s+1}} + \gradJ{\tilde{\vtheta}^s}
			+\gradJ{\vtheta_t^{s+1}} - \gradJ{\tilde{\vtheta}^s}\right\|^2\right] \nonumber\\
	%
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2}
	+\Ets[]{\norm[]{\gradApp{\tilde{\vtheta}^s}{N} - \Ets[]{\gradApp{\tilde{\vtheta}^s}{N}}}^2} \nonumber\\
	&\qquad+ 
	\mathbb{E}_{t\vert s}\left[\left\|
		\frac{1}{B}\sum_{i=0}^{B-1}\left(
		\score{\vtheta_t^{s+1}}{\tau_i}R(\tau_i) -
			\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}R(\tau_i)\right)
		\right.\right.\nonumber\\&\qquad\left.\left.
		- \Ets{
			\frac{1}{B}\sum_{i=0}^{B-1}\left(
			\score{\vtheta_t^{s+1}}{\tau_i}R(\tau_i) -
				\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}R(\tau_i)\right)}\right\|^2\right] 
	\nonumber\\
	%
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}}
	\nonumber\\
	&\qquad+ 
		\mathbb{E}_{t\vert s}\left[\left\|
		\frac{1}{B}\sum_{i=0}^{B-1}\left(
		\score{\vtheta_t^{s+1}}{\tau_i}R(\tau_i) -
		\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}R(\tau_i)\right)
		\right.\right.\nonumber\\&\qquad\left.\left.
		- \Ets{
			\frac{1}{B}\sum_{i=0}^{B-1}\left(
			\score{\vtheta_t^{s+1}}{\tau_i}R(\tau_i) -
			\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}R(\tau_i)\right)}\right\|^2\right] 
		\label{eq:1}\\%
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} \nonumber\\
	&\qquad+\Ets{\norm[]{
			\frac{1}{B}\sum_{i=0}^{B-1}\left(
			\score{\vtheta_t^{s+1}}{\tau_i}R(\tau_i) -
			\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}R(\tau_i)\right)}^2} \label{eq:2}\\
	%
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} \nonumber\\
	&\qquad+
			\frac{1}{B^2}\sum_{i=0}^{B-1}
			\Ets{\norm[]{
			\score{\vtheta_t^{s+1}}{\tau_i}R(\tau_i) -
			\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}R(\tau_i)}^2} \nonumber\\
	%
	&= \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}}
	\nonumber\\
	&\qquad+
			\frac{1}{B^2}\sum_{i=0}^{B-1}
			\Ets{\norm[]{\score{\vtheta_t^{s+1}}{\tau_i}R(\tau_i)
			-\score{\tilde{\vtheta}^s}{\tau_i}R(\tau_i)
			+\score{\tilde{\vtheta}^s}{\tau_i}R(\tau_i) 
			-\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}R(\tau_i)}^2} \nonumber\\
	%
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}}
	\nonumber\\
	&\qquad+
			\frac{1}{B^2}\sum_{i=0}^{B-1}
			\Ets{\norm[]{\score{\vtheta_t^{s+1}}{\tau_i}R(\tau_i)
			-\score{\tilde{\vtheta}^s}{\tau_i}R(\tau_i)}^2} \nonumber\\
	&\qquad
			+\frac{1}{B^2}\sum_{i=0}^{B-1}
			\Ets{\norm[]{\score{\tilde{\vtheta}^s}{\tau_i}R(\tau_i) 
			-\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}R(\tau_i)}^2} \nonumber\\
	%
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}}
	+\frac{L^2}{B}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}\nonumber\\
	&\qquad+
		\frac{1}{B^2}\sum_{i=0}^{B-1}
		\Ets{\norm[]{\score{\tilde{\vtheta}^s}{\tau_i}R(\tau_i) 
			-\omega(\tau_i)\score{\tilde{\vtheta}^s}{\tau_i}R(\tau_i)}^2} \label{eq:3}
% ...
\end{align}
\begin{align}
% ...
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}}
	\nonumber\\
	&\qquad+\frac{L^2}{B}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
	+\GRADLOG^2\frac{R^2}{(1-\gamma)^2}\frac{1}{B^2}\sum_{i=0}^{B-1}\Ets{(\omega(\tau_i)-1)^2} \label{eq:4}\\
	%
	&= \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}}
	\nonumber\\
	&\qquad+\frac{L^2}{B}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
	+\GRADLOG^2\frac{R^2}{(1-\gamma)^2b^2}\sum_{i=0}^{B-1}\Varts{\omega(\tau_i)} \nonumber\\
	%
	&\leq \Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}}
	\nonumber\\
	&\qquad+\frac{L^2}{B}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
	+\GRADLOG^2\frac{R^2\VARRF}{(1-\gamma)^2B}, \label{eq:5}
\end{align}
where (\ref{eq:1}) is from the definition of $\gradApp{\vtheta}{N}$, (\ref{eq:2}) is from $\EVV[]{\norm[]{X-\EVV[]{X}}^2}\leq\EVV[]{\norm[]{X}^2}$ for any stochastic variable $X$, (\ref{eq:3}) is from L-smoothness, 
(\ref{eq:4}) is from Assumption \ref{ass:bounded_score}, and (\ref{eq:5}) is from Assumption \ref{ass:M2}.
\end{proof}

%Lemma 0
\begin{restatable}[]{lemma}{auxzero}\label{lemma:aux0}
Under Assumption \ref{ass:bounded_score}, for any function $f(\vtheta_t^{s+1})$ which is deterministic for a fixed $\vtheta_t^{s+1}$:
\begin{align*}
\left|\Ets[t]{\dotprod{\gradBlack{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}}
-\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}}
\right|
\leq
\frac{1}{2N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} +\frac{1}{2}\Ets[t-1]{\norm[]{f(\vtheta_t^{s+1})}^2}
\end{align*}
\end{restatable}
\begin{proof}
\begin{align}
	\Ets{\dotprod{\gradBlack{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}}
	&=
	\Ets{\dotprod{\gradIdeal{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} +
	\Ets[t-1]{\dotprod{e_s}{f(\vtheta_t^{s+1})}} \label{eq:6}\\
	&=
	\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} +
	\Ets[t-1]{\dotprod{e_s}{f(\vtheta_t^{s+1})}} \label{eq:7}\\
	&=
	\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} \nonumber\\
	&\qquad+
	\dotprod{\Ets{e_s}}{\Ets{f(\vtheta_t^{s+1})}}
	+\Tr\Covts[t-1]{\gradApp{\tilde{\vtheta}^s}{N}}{f(\vtheta_t^{s+1})}  \nonumber\\
	&= 
	\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}} \nonumber\\
	&\qquad+
	\Tr\Covts[t-1]{\gradApp{\tilde{\vtheta}^s}{N}}{f(\vtheta_t^{s+1})} \label{eq:8}
\end{align}
Hence:
\begin{align}
	\left|\Ets{\dotprod{\gradBlack{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}}
	\right.&-\left.\Ets{\dotprod{\gradJ{\vtheta_t^{s+1}}}{f(\vtheta_t^{s+1})}}\right| 
	=
	\left|\Tr\Covts[t-1]{\gradApp{\tilde{\vtheta}^s}{N}}{f(\vtheta_t^{s+1})}\right|  
	\nonumber\\
	&\leq
	\Tr\left|\Covts[t-1]{\gradApp{\tilde{\vtheta}^s}{N}}{f(\vtheta_t^{s+1})}\right| \nonumber \\
	%
	&\leq
	\Tr\left(\sqrt{\Varts[]{\gradApp{\tilde{\vtheta}^s}{N}}}\circ\sqrt{\Varts[t-1]{f(\vtheta_t^{s+1})}}\right) \nonumber\\
	%
	&\leq	
	\frac{\alpha_t}{2}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{N}} +\frac{\alpha_t}{2}\Tr\Varts[t-1]{f(\vtheta_t^{s+1})}\label{eq:9}\\
	%
	&=
	\frac{1}{2N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} +\frac{1}{2}\Tr\Varts[t-1]{f(\vtheta_t^{s+1})} \label{eq:10}\\
	%
	&\leq
	\frac{1}{2N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} +\frac{1}{2}\Ets[t-1]{\norm[]{f(\vtheta_t^{s+1})}^2},
	\nonumber
\end{align}
where (\ref{eq:6}) is from Definition \ref{def:ideal}; (\ref{eq:7}) is from the fact that $\gradIdeal{\vtheta_t^{s+1}}$ is both unbiased and independent from $f(\vtheta_t^{s+1})$ \wrt the sampling at time $t$ alone, which is not true for $\gradBlack{\vtheta_t^{s+1}}$; (\ref{eq:8}) is from $\Ets{e_s}=0$; (\ref{eq:9}) is from Young's inequality; and (\ref{eq:10}) is from the definition of $\gradApp{\vtheta}{N}$.
\end{proof}

%Lemma 1
\begin{restatable}[]{lemma}{auxone}\label{lemma:aux1}
Under Assumptions \ref{ass:bounded_score} ans \ref{ass:M2}, the expected squared norm of the true gradient $\gradJ{\vtheta_t^{s+1}}$, for appropriate choices of $\alpha_t\geq0$ and $\beta_t>0$, can be bounded as follows:
\[
	\Ets{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} \leq
	\frac{R_{t+1}^{s+1} - R_t^{s+1}}{\Psi_t} + \frac{d_tV}{N\Psi_t}
	+\frac{f_tW}{b\Psi_t},
\]
	where
\begin{align*}
	&R_t^{s+1}\coloneqq \Ets{J(\vtheta_t^{s+1}) - c_t\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}, \\
	&c_{m-1} = 0, \\
	&c_t = c_{t+1}\left(\frac{3}{2}+\alpha_t\beta_t+\alpha_t^2\right) + \alpha_t\frac{L^2}{b}+\frac{\alpha_t^2L^3}{2}, \\
	&\Psi_t = \alpha_t\left(1-\frac{c_{t+1}}{\beta_t}-\frac{\alpha_tL}{2}-\alpha_tc_{t+1}\right), \\
	&d_t = \frac{\alpha_t}{2}\left(1+2c_{t+1}+\alpha_t^2L+2\alpha_t^2c_{t+1}\right), \\
	&f_t = \alpha_t^2\frac{G^2R^2(L+2c_{t+1})}{2(1-\gamma)^2}.
\end{align*}
In particular, the following constraints on $\alpha_t$ and $\beta_t$ are sufficient:
\begin{align*}
&0\leq\alpha_t < \frac{2(1-\nicefrac{c_{t+1}}{\beta_t})}{L+2c_{t+1}} \\
&\beta_t > c_{t+1}.
\end{align*}
\end{restatable}
\begin{proof}
	We have:
	\begin{align}
	\Ets{J(\vtheta_{t+1}^{s+1})} 
	&\geq \Ets{J(\vtheta_t^{s+1})+\dotprod{\gradJ{\vtheta_t^{s+1}}}{\vtheta_{t+1}^{s+1}-\vtheta_t^{s+1}} - \frac{L}{2}\norm[]{\vtheta_{t+1}^{s+1}-\vtheta_t^{s+1}}^2} \label{eq:11}\\
	&= \Ets{J(\vtheta_t^{s+1})+\alpha_t\dotprod{\gradJ{\vtheta_t^{s+1}}}{\gradBlack{\vtheta_t^{s+1}}} - \frac{\alpha_t^2L}{2}\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2} \label{eq:12}\\
	&\geq
	\Ets{J(\vtheta_t^{s+1})+\alpha_t\norm[]{\gradJ{\vtheta_t^{s+1}}}^2 - \frac{\alpha_t^2L}{2}\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2} \nonumber\\
	&\qquad-
	\frac{\alpha_t}{2N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} -\frac{\alpha_tL^2}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1} - \tilde{\vtheta}^s}^2}, \label{eq:13}
\end{align}
where (\ref{eq:11}) is from the L-smoothness of $J(\vtheta)$, (\ref{eq:12}) is from the SVRPG update, and (\ref{eq:13}) is from Lemma \ref{lemma:aux0}.

Next we have:
\begin{align}
\Ets{\norm[]{\vtheta_{t+1}^{s+1}-\tilde{\vtheta}^s}^2} 
&= \Ets{\norm[]{\vtheta_{t+1}^{s+1}- \vtheta_t^{s+1} + \vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \nonumber\\
&=\Ets{\norm[]{\vtheta_{t+1}^{s+1}-\vtheta_{t}^{s+1}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2+2\dotprod{\vtheta_{t+1}^{s+1}-\vtheta_{t}^{s+1}}{\vtheta_t^{s+1}-\tilde{\vtheta}^s}} \nonumber \\
&= \Ets{\alpha_t^2\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2+2\alpha_t\dotprod{\gradBlack{\vtheta_t^{s+1}}}{\vtheta_t^{s+1}-\tilde{\vtheta}^s}} \label{eq:14}\\
&\leq \Ets{\alpha_t^2\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2+2\alpha_t\dotprod{\gradJ{\vtheta_t^{s+1}}}{\vtheta_t^{s+1}-\tilde{\vtheta}^s}} \nonumber\\ 
&\qquad+
\frac{\alpha_t}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} +\frac{1}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \label{eq:15}\\
%
&\leq \Ets{\alpha_t^2\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
+2\alpha_t\Ets{\left|\dotprod{\gradJ{\vtheta_t^{s+1}}}{\vtheta_t^{s+1}-\tilde{\vtheta}^s}\right|} \nonumber\\ 
&\qquad+
\frac{\alpha_t}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} +\frac{1}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \nonumber\\
%
&\leq \Ets{\alpha_t^2\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
+2\alpha_t\Ets{\norm[]{\gradJ{\vtheta_t^{s+1}}}\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}} \nonumber\\ 
&\qquad+
\frac{\alpha_t}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} +\frac{1}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \nonumber\\
%
&\leq \Ets{\alpha_t^2\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
+2\alpha_t\Ets{\frac{1}{2\beta}\norm[]{\gradJ{\vtheta_t^{s+1}}}^2+\frac{\beta}{2}\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \nonumber\\ 
&\qquad
\frac{\alpha_t}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} +\frac{1}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}, \label{eq:16}
\end{align}
where (\ref{eq:14}) is from the SVRPG update, (\ref{eq:15}) is from Lemma \ref{lemma:aux0}, and (\ref{eq:16}) is from Young's inequality in the 'Peter-Paul' variant.
Finally:
\begin{align}
	R_{t+1}^{s+1} &= \Ets{J(\vtheta_{t+1}^{s+1}) - c_{t+1}\norm[]{\vtheta_{t+1}^{s+1}-\tilde{\vtheta}^s}^2} \nonumber\\
	%
	&\geq	\Ets{J(\vtheta_t^{s+1})+\alpha_t\norm[]{\gradJ{\vtheta_t^{s+1}}}^2 - \frac{\alpha_t^2L}{2}\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2} \nonumber\\
	&\qquad-
	\frac{\alpha_t}{2N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} -\frac{\alpha_tL^2}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1} - \tilde{\vtheta}^s}^2}
	-c_{t+1}\Ets{\norm[]{\vtheta_{t+1}^{s+1}-\tilde{\vtheta}^s}^2} \label{eq:17}\\
	%
	&\geq \Ets{J(\vtheta_t^{s+1})+\alpha_t\norm[]{\gradJ{\vtheta_t^{s+1}}}^2 - \frac{\alpha_t^2L}{2}\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2} 
	-\frac{\alpha_t}{2N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} \nonumber\\
	&\qquad -\frac{\alpha_tL^2}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1} - \tilde{\vtheta}^s}^2}
	-c_{t+1}\Ets{\norm[]{\vtheta_{t+1}^{s+1}-\tilde{\vtheta}^s}^2} \nonumber\\
	%
	&\geq \Ets{J(\vtheta_t^{s+1})+\alpha_t\norm[]{\gradJ{\vtheta_t^{s+1}}}^2 - \frac{\alpha_t^2L}{2}\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2} 
	-\frac{\alpha_t}{2N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} \nonumber\\
	&\qquad -\frac{\alpha_tL^2}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1} - \tilde{\vtheta}^s}^2} -c_{t+1}\Ets{\alpha_t^2\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2+\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
	\nonumber\\
	&\qquad-2c_{t+1}\alpha_t\Ets{\frac{1}{2\beta}\norm[]{\gradJ{\vtheta_t^{s+1}}}^2+\frac{\beta}{2}\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \nonumber\\ 
	&\qquad
	-c_{t+1}\frac{\alpha_t}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} -c_{t+1}\frac{1}{2}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2} \label{eq:18}\\
	%
	&= \Ets{J(\vtheta_t^{s+1})} - \left(c_{t+1}\left(\frac{3}{2}+\alpha_t\beta\right)+\alpha_tL^2\right)\Ets{\norm[]{\vtheta_{t}^{s+1}-\tilde{\vtheta}}^2} \nonumber\\
	&\qquad-\alpha_t^2\left(\frac{L}{2}+c_{t+1}\right)\Ets{\norm[]{\gradBlack{\vtheta_t^{s+1}}}^2}
	+\alpha_t\left(1-\frac{c_{t+1}}{\beta}\right)\Ets{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} \nonumber\\
	&\qquad-\frac{\alpha_t}{2N}\left(1+2c_{t+1}\right)\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} \nonumber\\
	%
	&\geq  \Ets{J(\vtheta_t^{s+1})} - \left(c_{t+1}\left(\frac{3}{2}+\alpha_t\beta\right)+\alpha_tL^2\right)\Ets{\norm[]{\vtheta_{t}^{s+1}-\tilde{\vtheta}}^2} \nonumber\\
	&\qquad
	-\alpha_t^2\left(\frac{L}{2}+c_{t+1}\right)\left(\Ets[t-1]{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} 
	+\frac{1}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}}
	\right.\nonumber\\
	&\left.\qquad+\frac{L^2}{b}\Ets[t-1]{\norm[]{\vtheta_t^{s+1}-\tilde{\vtheta}^s}^2}
	+G^2\frac{R^2}{(1-\gamma)^2b}W\right)
	+\alpha_t\left(1-\frac{c_{t+1}}{\beta}\right)\Ets{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} \nonumber\\
	&\qquad-\frac{\alpha_t}{2N}\left(1+2c_{t+1}\right)\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} \label{eq:19}\\
	%
	& = \Ets{J(\vtheta_t^{s+1}) - \left(c_{t+1}\left(\frac{3}{2}+\alpha_t\beta+\alpha_t^2\right)+\alpha_t\frac{L^2}{b}+\frac{\alpha_t^2L^3}{2}\right)\norm[]{\vtheta_{t}^{s+1}-\tilde{\vtheta}}^2} \nonumber\\
	&\qquad
	+\alpha_t\left(1-\frac{c_{t+1}}{\beta}-\frac{\alpha_tL}{2}-\alpha_tc_{t+1}\right)\Ets{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2} \nonumber\\
	&\qquad-\frac{\alpha_t}{2N}\left(1+2c_{t+1}+\alpha_t^2L+2\alpha_t^2c_{t+1}\right)\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}} \nonumber\\
	&\qquad-\alpha_t^2\frac{(L+2c_{t+1})G^2R^2\VARIS}{2(1-\gamma)^2b} \nonumber\\
	%
	&= R_t^{s+1}
	+\Psi_t\Ets{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2}
	-\frac{d_t}{N}\Tr\Varts[]{\gradApp{\tilde{\vtheta}^s}{1}}
	-\frac{f_t}{b}\VARIS,\nonumber\\
	&\geq R_t^{s+1}
	+\Psi_t\Ets{\norm[]{\gradJ{\vtheta_t^{s+1}}}^2}
	-\frac{d_t}{N}\VARRF
	-\frac{f_t}{b}\VARIS, \label{eq:20}
\end{align}
where (\ref{eq:17}) is from (\ref{eq:13}), (\ref{eq:18}) is from (\ref{eq:16}), (\ref{eq:19}) is from Lemma \ref{lemma:aux0}, and (\ref{eq:20}) is from Assumption \ref{ass:REINFORCE}.
To complete the proof, besides rearranging terms, we have to ensure that $\Psi_t>0$ for each $t$. This gives the constraints on $\alpha_t$ and $\beta_t$.
\end{proof}


\subsection*{Main theorem}
We finally provide the proof of the convergence theorem:

\convergence*
\begin{proof}
We prove the theorem for the following values of the constants:
\begin{align*}
& \psi \coloneqq \min_t\{\Psi_t\}, \\
& \zeta \coloneqq \frac{\max_t\{d_t\}V}{\psi} \\
& \xi \coloneqq \frac{\max_t\{f_t\}W}{\psi}.
\end{align*}
The values of all other constants and the constraints on the step size are provided in Lemma \ref{lemma:aux1}.

Starting from Lemma \ref{lemma:aux1}, summing over iterations of an epoch $s$ we get:
\begin{align}
\sum_{t=0}^{m-1}\Ets{\norm[]{\gradJ{(\vtheta_t^{s+1})}^2}}&\leq
 \frac{\sum_{t=0}^{m-1}\left(R_{t+1}^{s+1} - R_t^{s+1}\right)}{\psi} + \frac{m\zeta}{N} + \frac{m\xi}{B} \nonumber\\
 %
 &= \frac{\sum_{t=0}^{m-1}\left(\Ets[t+1]{J(\vtheta_{t+1}^{s+1}) - c_{t+1}\norm[]{\vtheta_{t+1}^{s+1}-\tilde{\vtheta}^s}^2} - \Ets[t]{J(\vtheta_{t}^{s+1}) - c_{t}\norm[]{\vtheta_{t}^{s+1}-\tilde{\vtheta}^s}^2}\right)}{\psi} \nonumber\\
 &\qquad+ \frac{m\zeta}{N} + \frac{m\xi}{B} \nonumber\\
 %
 &= \frac{\Ets[m]{\sum_{t=0}^{m-1}\left(J(\vtheta_{t+1}^{s+1}) - c_{t+1}\norm[]{\vtheta_{t+1}^{s+1}-\tilde{\vtheta}^s}^2 - J(\vtheta_{t}^{s+1}) - c_{t}\norm[]{\vtheta_{t}^{s+1}-\tilde{\vtheta}^s}^2\right)}}{\psi} \nonumber\\
 &\qquad+ \frac{m\zeta}{N} + \frac{m\xi}{B} \nonumber\\
 %
 &= \frac{\Ets[m]{J(\vtheta_{m}^{s+1}) - c_{m}\norm[]{\vtheta_{m}^{s+1}-\tilde{\vtheta}^s}^2 - J(\vtheta_{0}^{s+1}) - c_{0}\norm[]{\vtheta_{0}^{s+1}-\tilde{\vtheta}^s}^2}}{\psi} \nonumber\\
 &\qquad+ \frac{m\zeta}{N} + \frac{m\xi}{B} \label{eq:21}\\
 %
 &= \frac{\Ets[m]{J(\tilde{\vtheta}^{s+1}) - J(\tilde{\vtheta}^{s})}}{\psi} + \frac{m\zeta}{N} + \frac{m\xi}{B}, \label{eq:22}
 \end{align}
where (\ref{eq:21}) is obtained by telescoping the sum and (\ref{eq:22}) from the fact that $c_m=0$ and $\vtheta_0^{s+1}=\tilde{\vtheta}^s$.
Next, summing over epochs:
\begin{align}
\sum_{s=0}^{S-1}\sum_{t=0}^{m-1}\Ets{\norm[]{\gradJ{(\vtheta_t^{s+1})}^2}}&\leq
\frac{\sum_{s=0}^{S-1}\Ets[m]{J(\tilde{\vtheta}^{s+1}) - J(\tilde{\vtheta}^{s})}}{\psi} + \frac{T\zeta}{N} + \frac{T\xi}{B} \nonumber\\
%
&\leq
\frac{\EVV[]{J(\tilde{\vtheta}^{S}) - J(\tilde{\vtheta}^{0})}}{\psi} + \frac{T\zeta}{N} + \frac{T\xi}{B}
 \label{eq:23}\\
%
&\leq
\frac{J(\vtheta^*) - J(\vtheta^0)}{\psi} + \frac{T\zeta}{N} + \frac{T\xi}{B}, \label{eq:24}
\end{align}
where the expectation in (\ref{eq:23}) is \wrt all the trajectories sampled in a run of Algorithm \ref{alg:svrpg} and (\ref{eq:24}) is from the definition of $\vtheta^*$.
 
Finally, we consider the expectation \wrt all sources of randomness, including the uniform sampling of the output parameter:
\begin{align}
\EVV[]{\norm[]{\gradJ{(\vtheta_t^{s+1})}}^2} 
&=\frac{1}{T}\sum_{s=0}^{S-1}\sum_{t=0}^{m-1}\Ets{\norm[]{\gradJ{(\vtheta_t^{s+1})}}^2} \nonumber\\
&\leq
\frac{J(\vtheta^*) - J(\vtheta^0)}{\psi T} + \frac{\zeta}{N} + \frac{\xi}{B} \nonumber.
\end{align}
\end{proof}

\section{Applicability to Gaussian Policies}\label{app:pol}
\todomat{...}

\section{Experimental Details}\label{app:exp}
Setting specifications for all the used environments:
\begin{enumerate}
	\item \emph{Cart-Pole Balancing} : 4-dimensional state space: cart position x, pole angle $\theta$, cart velocity $\dot{x}$ and pole velocity $\dot{\theta}$; 1-dimensional action space: the horizontal force applied to the cart body. Reward function  is defined as $r(s, a) := 10 - (1 - cos(\theta)) - 10^{-5}\norm[] a^2$. The episodes terminates when $|x|>2.4$ or $|\theta|>0.2$ or the number of time steps T is greater than 100.
	\item \emph{Mujoco Swimmer}: 12-dimensional state space: 3 links velocities ($v_x$ and $v_y$ of center of masses) and 2 actuated joints angles. 2-dimensional action space: the two momentums applied on actuated joints.  The reward function is defined as $r(s, a) := v_x - 10^{-4}\norm[2] a^2$. The episodes terminates when the number of time steps T is greater than 500.
	\item \emph{Mujoco Half Cheetah}: 20-dimensional state space: 9 links and 6 actuated joints angles. 6-dimensional action space: the 6 momentums applied on actuated joints.  The reward function is defined as $r(s, a) := v_x - 0.05\norm[2] a^2$. The episodes terminates when the number of time steps T is greater than 500.
\end{enumerate}

All the experiments' parameters are reported in the following table:

\centering
\begin{tabular}{| l | c  c  c |}
	\hline	
	& Cart-Pole & Swimmer & Half Cheetah \\
	\hline
	State space dimension  & 4 & 8 & 20 \\
	Control space dimension & 1 & 2 & 6 \\
	NN Hidden layers & 8 & 32x32 & 32x32 \\
	Learning rate & $10^{-2}$ & $10^{-3}$ & $10^{-3}$ \\
	Snapshot trajectories & 100 & 100 & 100 \\
	Sub-iterations trajectories/GPOMDP batch & 10 & 10 & 10 \\
	Max sub-iterations & 50 & 20 & 20 \\
	Trajectories lengths& 500 & 500 & 500 \\
	Discount $(\gamma)$& 0.99 & 0.995 & 0.99 \\
	Total trajectories& 10000 & 10000 & 10000 \\
	\hline  
\end{tabular}


\end{document}
