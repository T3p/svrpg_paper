\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

%%%USEFUL PACKAGES%%%
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{amsmath,amssymb}
\usepackage{mathtools}
%%Theorems
\usepackage{amsthm}
\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}
\newtheorem{case}{Case}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{conj}{Conjecture}
\newtheorem{example}{Example}
%Restatable
\usepackage{thmtools, thm-restate}
\declaretheorem[numberwithin=section]{thm}
\declaretheorem[sibling=thm]{lemma}
\declaretheorem[sibling=thm]{corollary}
\declaretheorem[sibling=thm]{assumption}
\declaretheorem[sibling=thm]{theorem}
%e.g. ...
\usepackage{xspace}
\DeclareRobustCommand{\eg}{e.g.,\@\xspace}
\DeclareRobustCommand{\ie}{i.e.,\@\xspace}
\DeclareRobustCommand{\wrt}{w.r.t.\@\xspace}
%%%%%%

%%%CUSTOM COMMANDS%%%
%Math
\newcommand{\realspace}{\mathbb R}      % realspace
\newcommand{\transpose}[1]{{#1}^\texttt{T}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\EV}{\mathbb{E}}
\newcommand{\EVV}[2][\ppvect \in \ppspace]{\EV_{#1}\left[{#2}\right]}
\newcommand{\norm}[2][\infty]{\left\|#2\right\|_{#1}}
\newcommand{\Dij}[2]{\frac{\partial^{2}{#1}}{\partial{#2}_i\partial{#2}_j}}
\newcommand{\de}{\,\mathrm{d}}
%RL
\newcommand{\vtheta}{\boldsymbol{\theta}}
\newcommand{\Aspace}{\mathcal{A}}
\newcommand{\Sspace}{\mathcal{S}}
\newcommand{\Tspace}{\mathcal{T}}
\newcommand{\Transition}{\mathcal{P}}
\newcommand{\Reward}{\mathcal{R}}
\newcommand{\stationary}{d_{\rho}^{\pi_{\vtheta}}(s)}
\newcommand{\policy}{\pi_{\vtheta}(a \mid s)}
\newcommand{\pol}{\pi_{\vtheta}}
\newcommand{\trajdistr}{\pi_{\vtheta}(\tau)}
\newcommand{\score}[2]{\nabla_{#1}\log\pi_{#1}(#2)}
\newcommand{\Qfun}{Q^{\pi_{\vtheta}}(s,a)}
\newcommand{\Vfun}{V^{\pi_{\vtheta}}(s)}
\newcommand{\vTheta}{\boldsymbol{\Theta}}
\newcommand{\vphi}{\boldsymbol{\phi}}
\newcommand{\gradJ}[1]{\nabla_{#1}J(\vtheta)}
\newcommand{\gradApp}[2]{\hat{\nabla}_{#2}J(#1)}
\newcommand{\eqdef}{\mathrel{\mathop:}=}
\newcommand{\Dataset}{\mathcal{D}}
%%%%%%

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{SVRPG}

\begin{document}

\twocolumn[
\icmltitle{Stochastic Variance Reduced Policy Gradient}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2018
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Damiano Binaghi}{equal,polimi}
\icmlauthor{Giuseppe Canonaco}{equal,polimi}
\icmlauthor{Matteo Papini}{polimi}
\icmlauthor{Matteo Pirotta}{inria}
\icmlauthor{Marcello Restelli}{polimi}
\end{icmlauthorlist}

\icmlaffiliation{polimi}{Politecnico di Milano, Milano, Italy}
\icmlaffiliation{inria}{Inria, Lille, France}

\icmlcorrespondingauthor{Matteo Papini}{matteo.papini@polimi.it}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

\end{abstract}

\section{Introduction}

\section{Preliminaries}
In this section we provide essential background on policy gradient methods for Reinforcement Learning and stochastic variance-reduced gradient methods for finite sum optimization. In the next section we will show how these methods can be combined to solve Reinforcement Learning tasks.

\subsection{Policy Gradient}
A Reinforcement Learning task \cite{sutton2011reinforcement} can be modeled with a discrete-time continuous Markov Decision Process (MDP) $\langle\Sspace,\Aspace,\Transition,\Reward,\gamma,\rho\rangle$, where $\Sspace$ is a continuous state space; $\Aspace$ is a continuous action space; $\Transition$ is a Markovian transition model, where $\Transition(s'\mid s,a)$ defines the transition density from state $s$ to $s'$ under action $a$; $\Reward$ is the reward function, where $\Reward(s,a) \in [-R,R]$ is the expected reward for state-action pair $(s,a)$ and $R$ is the maximum absolute-value reward; $\gamma\in[0,1)$ is the discount factor; and $\rho$ is the initial state distribution.
The agent's behavior is modeled as a policy $\pi$, where $\pi(a\mid s)$ is the density distribution over $\Aspace$ of the action to take in state $s$.
We consider episodic tasks, \ie, tasks composed of episodes of length $H$, also called time horizon. In this setting, a trajectory $\tau$ is the sequence of states and actions $(s_0,a_0,s_1,a_1,\dots,s_{H-1},a_{H-1})$ observed by following some stationary policy in a given episode, where $s_0$ is always sampled from the initial state distribution $\rho$. With abuse of notation, we denote with $\pi(\tau)$ the density distribution induced by policy $\pi$ on the set $\Tspace$ of all possible trajectories, and with $\Reward(\tau)$ the total discounted reward provided by trajectory $\tau$:
\begin{align*}
\Reward(\tau) = \sum_{t=0}^{H-1}\gamma^t\Reward(s_t,a_t).
\end{align*}
Performance $J$ allows to rank policies in terms of expected reward:
\begin{align*}
	J(\pi) = \int_{\Tspace}\pi(\tau)\Reward(\tau)\de \tau.
\end{align*}
Solving an MDP means finding a policy $\pi^*$ maximizing $J$.
\par
Policy gradient methods restrict the search for the performance-maximizing policy over a class of parametrized policies $\Pi_{\vtheta}=\{\pol: \vtheta \in \realspace^m\}$, with the only constraint that $\pol$ is differentiable in the $m$-dimensional parameter vector $\vtheta$. We denote the performance of a parametric policy with $J(\vtheta)$ for brevity, while in some occasions we denote $\pol(\tau)$ with $\pi(\tau\mid\vtheta)$ for the sake of readability. The search for a locally optimal policy is performed by means of gradient ascent, where the gradient of $J(\vtheta)$ \wrt the policy parameters is \cite{sutton2000policy}:
\begin{align*}
\gradJ{} = \int_{\Tspace}\pol(\tau)\score{}{\tau}\Reward(\tau)\de \tau.
\end{align*}
We consider the on-line learning problem, where trajectories must be sampled directly by interacting with the environment. In this setting, stochastic gradient descent is typically employed: after sampling a trajectory $\tau$ or a batch of $N$ trajectories $\Dataset_{N}$,
policy parameters are updated as $\vtheta\gets\vtheta + \alpha\gradApp{\vtheta}{N}$, where $\alpha$ is a step size and $\gradApp{\vtheta}{N}$ is an estimate of the true gradient. The REINFORCE gradient estimator \cite{williams1992simple} provides a simple, unbiased way of estimating the gradient:
\begin{align*}
\gradApp{\vtheta}{N} = \frac{1}{N}\sum_{n=1}^{N}\left(\sum_{h=0}^{H-1}\score{}{a_h^n\mid s_h^n}\right)\left(\sum_{h=0}^{H-1}\gamma^h r_h^n - b\right),
\end{align*}
where subscripts denote the time step, superscripts denote the trajectory, $r_h$ is the reward actually collected at time $h$ and b can be any baseline, provided it is constant \wrt to actions.
The G(PO)MDP gradient estimator \cite{baxter2001infinite} is a refinement of REINFORCE which is subject to less variance while preserving the unbiasedness:
\begin{align*}
\gradApp{\vtheta}{N} = \frac{1}{N}\sum_{n=1}^{N}\sum_{h=0}^{H-1}\left(\sum_{k=0}^{h}\score{\vtheta}{a_h^n\mid s_h^n}\right)\left(\gamma^h r_h^n - b\right).
\end{align*}

\subsection{Stochastic Variance-Reduced Gradient}
Finite sum optimization is the problem of maximizing an objective function $f(\vtheta)$ which can be decomposed into the sum or average of a finite number of functions $g_i(\vtheta)$:
\begin{align*}
\max_{\vtheta} f(\vtheta) = \frac{1}{N}\sum_{i=1}^{N}g_i(\vtheta).
\end{align*}
This kind of optimization is very common in machine learning, where each $g_i$ may correspond to a data sample $x_i$ from a dataset $\mathcal{D}_N$ of size $N$. In this case, we adopt the following, more meaningful notation:
\begin{align*}
\max_{\vtheta} f(\vtheta) = \frac{1}{N}\sum_{i=1}^{N}g(x_i \mid \vtheta).
\end{align*}
A common requirement is that $g$ must be smooth and convex in $\vtheta$. Under this hypothesis, full gradient ascent \cite{cauchy1847methode} with a constant step size achieves an IFO complexity \cite{agarwal2014lower} of $O(\nicefrac{N}{\epsilon})$, \ie it requires $k = O(\nicefrac{N}{\epsilon})$ gradient computations to achieve $\norm[]{\nabla_{\vtheta}f(\vtheta_k)}^2\leq\epsilon$. This can be too expensive for large values of $N$. Stochastic gradient ascent \cite{robbins1951stochastic} \cite{bottou2004large} overcomes this problem by sampling a single $x_i$ per iteration, but a vanishing step size is required to control the variance introduced by sampling. The resulting IFO complexity is $O(\nicefrac{1}{\epsilon^2})$ in expectation, which does not depend on $N$, but has a worse dependency on $\epsilon$. 
Starting from SAG \cite{roux2012stochastic}, a series of variations to stochastic gradient ascent have been proposed to achieve a better trade-off between convergence speed and cost per iteration. This family of algorithms includes SAG, SVRG \cite{johnson2013accelerating}, SAGA \cite{defazio2014saga} and Finito \cite{defazio2014finito}, the common intuition being the reuse of previous gradient computations to reduce the variance of the current one. In particular, Stochastic Variance Reduced Gradient (SVRG) is often preferred to other similar methods for its limited storage requirements, which can become a problem when neural networks with several parameters are employed.  

The idea of SVRG is to alternate full and stochastic gradient updates. Each $m = O(N)$ iterations, a snapshot $\tilde{\vtheta}$ of the current parameter is saved together with its full gradient $\nabla f(\tilde{\vtheta})$.
Otherwise, the parameter is updated with a corrected stochastic gradient estimate of the form:
\begin{align*}
\blacktriangledown f(\vtheta) = 
	\nabla f(\tilde{\vtheta}) + 
	\nabla g(x_i\mid\vtheta) -
	\nabla g(x_i \mid \tilde{\vtheta}),
\end{align*} 
where $x_i$ is sampled uniformly at random from $\mathcal{D}_N$. The corrected gradient $\blacktriangledown f(\vtheta)$ is an unbiased estimate of $\nabla f(\vtheta)$, and it is able to control the variance introduced by sampling even with a fixed step size, without resorting to plain full gradient. The resulting IFO complexity is $O(N+\nicefrac{\sqrt{N}}{\epsilon})$ \cite{reddi2016stochastic}.

More recently, some extensions of variance reduction algorithms to the non-convex objectives have been proposed \cite{reddi2016stochastic} \cite{allen2016variance} \cite{reddi2016stochastic} \cite{reddi2016fast}. In this scenario, $f$ is typically required to be L-smooth, \ie $\norm[]{\nabla f(\vtheta') - \nabla f(\vtheta)} \leq L\norm[]{\vtheta'-\vtheta}$ for each $\vtheta,\vtheta'\in\realspace^n$ and for some Lipschitz constant $L$. Under this hypothesis, the IFO complexity of full gradient  and of stochastic gradient ascent are the same as in the convex case \cite{nesterov2013introductory} \cite{ghadimi2013stochastic}. Also in this case, SVRG yields an advantageous trade-off, with an IFO complexity of $O(N + \nicefrac{N^{\nicefrac{2}{3}}}{N})$ \cite{reddi2016stochastic}. The only additional requirement is to select $\vtheta^*$ uniformly at random among all the $\vtheta_k$ instead of simply setting it to the final value. Pseudocode of SVRG for both the convex and the non-convex case is provided in Algorithm \ref{alg:svrg}. 


\begin{algorithm}[tb]
	\caption{SVRG}
	\label{alg:svrg}
	\begin{algorithmic}
		\STATE {\bfseries Input:} number of epochs $S$, epoch size $m$, step size $\alpha$, initial parameter $\vtheta_{m-1}^0$
		\FOR{$s=0$ {\bfseries to} $S-1$}
		\STATE $\vtheta_0^{s+1} = \tilde{\vtheta}^s = \vtheta_{m-1}^s$
		\FOR{$t=0$ {\bfseries to} $m-1$}
		\STATE Pick $i$ uniformly at random from $[1,N]$
		\STATE \begin{align*}
			\blacktriangledown f(\vtheta_t^{s+1}) = 
			&\nabla f(\tilde{\vtheta}^s) \\ + 
			&\nabla g(x_i\mid\vtheta_t^{s+1}) -
			\nabla g(x_i \mid \tilde{\vtheta}^{s+1})
		\end{align*}
		\STATE $\vtheta_{t+1}^{s+1} = \vtheta_t^{s+1} + \alpha\blacktriangledown f(\vtheta_t^{s+1})$
		\ENDFOR
		\ENDFOR
		\STATE {\bfseries Convex case: return} $\vtheta_{m-1}^S$
		\STATE {\bfseries Non-Convex case: return} $\vtheta_t^s$ with $(s,t)$ picked uniformly at random from $[0,S)\times[0,m)$
	\end{algorithmic}
\end{algorithm}


\section{Algorithm}
In on-line policy gradient problems, stochastic gradient ascent (SGA) is not really a choice, but is dictated by the necessity of interacting with an unknown environment. We would like to apply SVRG to the policy gradient framework in order to limit the variance introduced by sampling trajectories, which would ultimately lead to faster convergence. In this scenario, the function $f(\vtheta)$ to optimize is $J(\vtheta)$ and the (implicit) dataset $\mathcal{D}$ is the set of all possible trajectories $\mathcal{T}$. Compared to the typical finite sum optimization scenario, we have the following additional challenges:
\begin{itemize}
	\item The objective $J(\vtheta)$ is typically non-convex;
	\item The dataset $\mathcal{T}$ can be infinite;
	\item The value of the parameter $\vtheta$ influences the sampling of trajectories;
\end{itemize}
To deal with non-convexity, we require $J(\vtheta)$ to be L-smooth, which is a reasonable assumption for common policy classes such as truncated Gaussian and Softmax [appendix?]. Because of the infinite dataset, we can only rely on an estimate of the full gradient. Finally, we employ importance weighting \cite{rubinstein1981simulation} \cite{precup2000eligibility} to guarantee proper sampling of trajectories. We focus on the REINFORCE gradient estimator for simplicity, but the extension to G(PO)MDP is straightforward. The proposed SVRG-like gradient estimate, which we call Stochastic Variance Reduced Policy Gradient (SVRPG), is the following:
\begin{align*}
	\blacktriangledown J(\vtheta) &= \frac{1}{N}\sum_{j=0}^{N-1}\nabla\log\pi(\tau_j \mid \tilde{\vtheta})\Reward(\tau_j) \\
		&+ \nabla\log\pi(\tau_i \mid \vtheta)\Reward(\tau_i) 
		- \omega(\tau_i)\nabla\log\pi(\tau_i \mid \tilde{\vtheta})\Reward(\tau_i),
\end{align*}
where trajectories $\tau_j$ for $j=1,\dots,N$ are sampled from the snapshot policy $\pi_{\tilde{\vtheta}}$, $\tau_i$ is sampled from the current policy $\pol$, and $\omega(\tau) = \frac{\pi(\tau\mid\tilde{\vtheta})}{\pi(\tau\mid\vtheta)}$ is an importance weight from $\pol$ to the snapshot policy $\pi_{\tilde{\vtheta}}$. We immediately have the following (proof in Appendix \ref{app:proofs}):

\begin{restatable}{lemma}{unbias}\label{lemma:unbias}
The SVRPG gradient estimator is unbiased:
\[
\mathop{E}_{\tau\sim\pi(\cdot\mid\tilde{\vtheta})}\left[\blacktriangledown J(\vtheta)\right] = \gradJ{}.
\]
\end{restatable}

[May also add Lemma about convergence of the variance to zero, since it provides an intuitive justification of some design choices]
The resulting SVRPG policy optimization method is detailed in Algorithm \ref{alg:svrpg}.
To obtain a method that can be used in practice, a number of additional details must be specified, namely the choice of epoch size $m$, the number $N$ of trajectories used to estimate the full gradient and the step size $\alpha$. Adaptive selection of these meta-parameters is also possible. However, the next section will focus on providing converge guarantees for the plain SVRPG algorithm.

\begin{algorithm}[tb]
	\caption{SVRPG}
	\label{alg:svrpg}
	\begin{algorithmic}
		\STATE {\bfseries Input:} number of epochs $S$, epoch size $m$, step size $\alpha$, initial parameter $\vtheta_{m-1}^0$, batch size $N$
		\FOR{$s=0$ {\bfseries to} $S-1$}
		\STATE Sample N trajectories $\tau_j$ from $\pi(\cdot\mid\tilde{\vtheta}^{s})$
		\STATE $\gradApp{\tilde{\vtheta}^{s}}{N} = \frac{1}{N}\sum_{j=0}^{N-1}\score{}{\tau_j\mid\tilde{\vtheta}^{s}}\Reward(\tau_j)$
		\STATE $\vtheta_0^{s+1} = \tilde{\vtheta}^s = \vtheta_{m-1}^s$
		\FOR{$t=0$ {\bfseries to} $m-1$}
		\STATE Sample trajectory $\tau_i$ from 				$\pi(\cdot\mid\vtheta_t^{s+1})$
		\STATE 
		\begin{align*}
		\blacktriangledown J(\vtheta_t^{s+1}) = 
		&\gradApp{\tilde{\vtheta}^s}{N}
		+ \score{}{\tau_i\mid\vtheta_t^{s+1}}\Reward(\tau_i) \\
		& - \omega(\tau_i)\score{}{\tau_i \mid \tilde{\vtheta}^{s}}\Reward(\tau_i)
		\end{align*}
		\STATE $\vtheta_{t+1}^{s+1} = \vtheta_t^{s+1} + \alpha\blacktriangledown J(\vtheta_t^{s+1})$
		\ENDFOR
		\ENDFOR
		\STATE {\bfseries return} $\vtheta_t^s$ with $(s,t)$ picked uniformly at random from $[0,S)\times[0,m)$
	\end{algorithmic}
\end{algorithm}

\section{Convergence Guarantees}

\section{Practical Variants}

\section{Related Work}

\section{Experiments}

\section{Discussion}

\bibliography{svrpg}
\bibliographystyle{icml2018}


\appendix

\section{Proofs}\label{app:proofs}

\unbias*
\begin{proof}
	...
\end{proof}


\end{document}
